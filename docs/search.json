[
  {
    "objectID": "nopara.html#introducción-a-los-contrastes-no-paramétricos",
    "href": "nopara.html#introducción-a-los-contrastes-no-paramétricos",
    "title": "4  Contraste no paramétrico",
    "section": "4.1 Introducción a los contrastes no paramétricos",
    "text": "4.1 Introducción a los contrastes no paramétricos\nEn el campo de la inferencia estadística, los contrastes no paramétricos se utilizan para comparar grupos o evaluar hipótesis cuando no se cumplen los supuestos necesarios para aplicar métodos paramétricos tradicionales visto en el Capítulo 3. Estos métodos no dependen de la suposición de que los datos sigan una distribución específica, como la Normal, lo que los hace especialmente útiles en situaciones donde los datos presentan sesgos, distribuciones desconocidas o tamaños de muestra pequeños.\nLos contrastes no paramétricos ofrecen una alternativa robusta y flexible para analizar datos en diversas circunstancias. Entre sus ventajas se incluyen la capacidad de manejar datos ordinales que no se ajustan a una escala continua y la resistencia a la influencia de valores atípicos. Esto permite obtener resultados fiables y válidos sin necesidad de transformaciones complicadas de los datos.\nAlgunos de los contrastes no paramétricos más conocidos incluyen la prueba de Mann-Whitney U, utilizada para comparar medianas entre dos grupos independientes, y la prueba de Wilcoxon para muestras relacionadas, que evalúa diferencias en medianas para datos apareados. Otros ejemplos son la prueba de Kruskal-Wallis, que extiende el análisis a más de dos grupos independientes, y la prueba de Friedman, que se aplica a diseños con medidas repetidas."
  },
  {
    "objectID": "nopara.html#prueba-chi-cuadrado-de-independencia",
    "href": "nopara.html#prueba-chi-cuadrado-de-independencia",
    "title": "4  Contraste no paramétrico",
    "section": "4.2 Prueba Chi-cuadrado de Independencia",
    "text": "4.2 Prueba Chi-cuadrado de Independencia\nLa prueba chi-cuadrado de independencia se utiliza para determinar si hay una asociación significativa entre dos variables categóricas \\(X\\) con categorías \\(X_1,X_2,\\ldots,X_r\\) e \\(Y\\) con categorías \\(Y_1,Y2,\\ldots,Y_c\\). Esta prueba compara las frecuencias observadas en una tabla de contingencia con las frecuencias esperadas bajo la hipótesis de independencia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(Y_1\\)\n\\(\\ldots\\)\n\\(Y_j\\)\n\\(\\ldots\\)\n\\(Y_c\\)\n\n\n\n\n\n\\(X_1\\)\n\\(n_{1,1}\\)\n\\(\\ldots\\)\n\\(n_{1,j}\\)\n\\(\\ldots\\)\n\\(n_{1,c}\\)\n\\(n_{1.}\\)\n\n\n\\(\\ldots\\)\n\n\n\n\n\n\\(\\ldots\\)\n\n\n\\(X_i\\)\n\\(n_{i,1}\\)\n\\(\\ldots\\)\n\\(n_{i,j}\\)\n\\(\\ldots\\)\n\\(n_{i,c}\\)\n\\(n_{i.}\\)\n\n\n\\(\\ldots\\)\n\n\n\n\n\n\\(\\ldots\\)\n\n\n\\(X_r\\)\n\\(n_{r,1}\\)\n\\(\\ldots\\)\n\\(n_{r,j}\\)\n\\(\\ldots\\)\n\\(n_{r,c}\\)\n\\(n_{r.}\\)\n\n\n\n\\(n_{.1}\\)\n\n\\(n_{.j}\\)\n\n\\(n_{.c}\\)\n\\(n_{..}\\)\n\n\n\nLa hipótesis nula \\(H_0\\) de esta prueba es que no hay asociación entre las variables, esto es que las variables implicadas son independientes:\n\nHipótesis nula \\(H_0\\): No hay asociación entre las variables (son independientes).\nHipótesis alternativa \\(H_1\\): Hay una asociación entre las variables (son dependientes).\n\nLas frecuencias esperadas se calculan como sigue: \\[\n   E_{ij} = \\frac{(n_{i.} \\times n_{.j})}{N}\n   \\] donde \\(E_{ij}\\) es la frecuencia esperada en la celda \\((i,j)\\), \\(n_{i.}\\) es el total de la fila (\\(i\\)), (\\(n_{.j}\\)) es el total de la columna (\\(j\\)), y (\\(n_{..}\\)) es el total general.\nAhora, comparamos las frecuencias esperadas con las frecuencias observadas, definiendo con ellos el estadístico chi-cuadrado: \\[\n\\chi^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\ndonde (\\(O_{ij}\\)) es la frecuencia observada en la celda \\((i,j)\\).\nBajo la hipótesis nula, el estadístico de prueba sigue una distribución chi-cuadrado con \\((r-1)(c-1)\\) grados de libertad, donde (\\(r\\)) es el número de filas y (\\(c\\)) es el número de columnas. Podemos calcular el \\(p-valor\\) como: \\[\np-valor=P(Chi^2_{(r-1)(c-1)}\\geq\\chi^2)\n\\]\nComo ocurría en los contrastes de hipótesis paramétricos, comparamos el \\(p-valor\\) p con el nivel de significancia (\\(\\alpha\\)), generalmente \\(0.05\\). Si (\\(p-valor &lt; \\alpha\\)), se rechaza la hipótesis nula.\n\n\n\n\n\n\nEjemplo Práctico. Prueba Chi-cuadrado de independencia\n\n\n\n\n\nSupongamos que un investigador desea determinar si hay una asociación entre el tipo de dispositivo usado (Laptop, Tablet, Smartphone) y la satisfacción del usuario (Satisfecho, No Satisfecho).\nDeterminamos las hipótesis del problema:\n\nHipótesis nula (\\(H_0\\)): No hay asociación entre el tipo de dispositivo y la satisfacción del usuario (son independientes).\nHipótesis alternativa (\\(H_1\\)): Hay una asociación entre el tipo de dispositivo y la satisfacción del usuario (son dependientes).\n\nRecolectamos datos de una muestra de \\(150\\) usuarios y construimos la siguiente tabla de contingencia:\n\n\n\n\nSatisfecho\nNo Satisfecho\nTotal\n\n\n\n\nLaptop\n30\n10\n40\n\n\nTablet\n20\n20\n40\n\n\nSmartphone\n50\n20\n70\n\n\nTotal\n100\n50\n150\n\n\n\nVamos a utilizar R para realizar la prueba chi-cuadrado de independencia.\n\n# Crear la tabla de contingencia\ntabla &lt;- matrix(c(30, 10, 20, 20, 50, 20), nrow = 3, byrow = TRUE)\nrownames(tabla) &lt;- c(\"Laptop\", \"Tablet\", \"Smartphone\")\ncolnames(tabla) &lt;- c(\"Satisfecho\", \"No Satisfecho\")\ntabla &lt;- as.table(tabla)\n\n# Mostrar la tabla de contingencia\nprint(tabla)\n\n           Satisfecho No Satisfecho\nLaptop             30            10\nTablet             20            20\nSmartphone         50            20\n\n# Realizar la prueba chi-cuadrado\nchi2_test &lt;- chisq.test(tabla)\n\n# Mostrar los resultados\nprint(chi2_test)\n\n\n    Pearson's Chi-squared test\n\ndata:  tabla\nX-squared = 6.9643, df = 2, p-value = 0.03074\n\n\nEl resultado de chisq.test en R incluye varios componentes clave:\n\nEstadístico Chi-cuadrado (\\(chi^2\\)): Este es el valor calculado del estadístico chi-cuadrado.\nGrados de libertad (\\(df\\)): Los grados de libertad de la prueba.\nValor p (\\(p-value\\)): Este es el \\(p-valor\\) asociado con el estadístico chi-cuadrado calculado.\n\nEn este ejemplo, el valor p es 0.031, que es menor que \\(0.05\\). Por lo tanto, rechazamos la hipótesis nula y concluimos que hay una asociación significativa entre el tipo de dispositivo y la satisfacción del usuario.\n\n\n\n\n\n\n\n\n\nEjemplo Práctico. Prueba Chi-cuadrado en Aprendizaje Automático\n\n\n\n\n\nVamos a realizar un ejemplo de la prueba Chi-cuadrado de independencia utilizando la matriz de confusión de un modelo de aprendizaje automático. Tal y como hemos visto, la prueba Chi-cuadrado de independencia se utiliza para determinar si hay una asociación significativa entre dos variables categóricas.\nSupongamos que tenemos un modelo de clasificación binaria que predice si un correo electrónico es spam o no. Construiremos estos modelos en la asignatura de Aprendizaje Automático I del Grado en Ciencia e Ingeniería de Datos. Tras entrenar y evaluar el modelo, obtenemos la siguiente matriz de confusión:\n\n\n\n\nPredicción: No Spam\nPredicción: Spam\n\n\n\n\nActual: No Spam\n50\n10\n\n\nActual: Spam\n5\n35\n\n\n\nEsta tabla de doble entrada cruza la predicción del modelo de aprendiaje automático con el verdadero valor en la muestra sobre la que dicho modelo ha sido entrenado. El objetivo de todo modelo de aprendizaje automático es conseguir una matriz de confusión diagonal. En ese caso, no hay errores en la predicción. Todo son éxitos y el modelo es perfecto.\nVamos a realizar la prueba Chi-cuadrado de independencia para ver si hay una asociación significativa entre las predicciones del modelo y las etiquetas reales. Si el modelo es útil, dicha asociación ha de existir. Si el modelo no es útil, entonces la prueba debería de decirnos que no podemos rechazar la hipótesis nula de independencia entre las dos variables (Predicción y Actual).\n\n# Cargar el paquete necesario\nlibrary(MASS)\n\n# Crear la matriz de confusión\nmatriz_confusion &lt;- matrix(c(50, 10, 5, 35), nrow = 2, byrow = TRUE)\ncolnames(matriz_confusion) &lt;- c(\"Predicción: No Spam\", \"Predicción: Spam\")\nrownames(matriz_confusion) &lt;- c(\"Actual: No Spam\", \"Actual: Spam\")\n\n# Mostrar la matriz de confusión\nprint(matriz_confusion)\n\n                Predicción: No Spam Predicción: Spam\nActual: No Spam                  50               10\nActual: Spam                      5               35\n\n# Realizar la prueba Chi-cuadrado de independencia\nprueba_chi_cuadrado &lt;- chisq.test(matriz_confusion)\n\n# Mostrar los resultados de la prueba\nprint(prueba_chi_cuadrado)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  matriz_confusion\nX-squared = 45.833, df = 1, p-value = 1.288e-11\n\n\nEl resultado de chisq.test() proporciona el valor de Chi-cuadrado, los grados de libertad y el valor p. En este caso, dado que el \\(p-valor\\) de la prueba es menor que el nivel de significancia (\\(0.05\\)), podemos rechazar la hipótesis nula de independencia y concluir que hay una asociación significativa entre las predicciones del modelo y las etiquetas reales. En otras palabras, el modelo de aprendizaje máquina es útil para predecir si un correo electrónico es (o no) spam."
  },
  {
    "objectID": "nopara.html#prueba-de-chi-cuadrado-de-bondad-de-ajuste",
    "href": "nopara.html#prueba-de-chi-cuadrado-de-bondad-de-ajuste",
    "title": "4  Contraste no paramétrico",
    "section": "4.3 Prueba de Chi-cuadrado de Bondad de Ajuste",
    "text": "4.3 Prueba de Chi-cuadrado de Bondad de Ajuste\nEsta prueba se utiliza para determinar si una distribución de frecuencias observadas sigue una distribución teórica esperada.\n\nHipótesis nula (\\(H_0\\)): Las frecuencias observadas siguen la distribución esperada.\nHipótesis alternativa (\\(H_1\\)): Las frecuencias observadas no siguen la distribución esperada.\n\nEl estadístico del contraste, como en el caso anterior se calcula como sigue: \\[ \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\\] donde (\\(O_i\\)) son las frecuencias observadas y (\\(E_i\\)) son las frecuencias esperadas según la distribución teórica.\nUtilizando la distribución chi-cuadrado con (\\(k-1\\)) grados de libertad, donde (\\(k\\)) es el número de categorías, podemos calcular el \\(p-valor\\). Comparamos el \\(p-valor\\) con el nivel de significancia. Si (\\(p-valor &lt; \\alpha\\)), se rechaza la hipótesis nula.\n\n\n\n\n\n\nEjemplo Práctico. Prueba Chi-cuadrado de bondad de ajuste\n\n\n\n\n\nSupongamos que un investigador quiere determinar si los resultados de un dado son uniformemente distribuidos. El dado se lanza \\(60\\) veces y los resultados son los siguientes:\n\n1: 8 veces\n2: 10 veces\n3: 9 veces\n4: 11 veces\n5: 12 veces\n6: 10 veces\n\nQueremos comprobar si estos resultados siguen una distribución uniforme, es decir, cada número tiene la misma probabilidad de 1/6.\nFormulamos las hipótesis:\n\nHipótesis nula (H0): Los resultados del dado siguen una distribución uniforme.\nHipótesis alternativa (H1): Los resultados del dado no siguen una distribución uniforme.\n\nA continuación se recogen los datos:\n\n# Resultados observados\nobserved &lt;- c(8, 10, 9, 11, 12, 10)\n\n# Frecuencias esperadas si el dado es justo (distribución uniforme)\nexpected &lt;- rep(60 / 6, 6)\n\nY se realiza la prueba\n\n# Realizar la prueba chi-cuadrado\nchi2_test &lt;- chisq.test(observed, p = rep(1/6, 6))\n\n# Mostrar los resultados\nprint(chi2_test)\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 1, df = 5, p-value = 0.9626\n\n\nEl resultado de chisq.test en R incluye varios componentes clave:\n\nEstadístico Chi-cuadrado (\\(\\chi^2\\))): Este es el valor calculado del estadístico chi-cuadrado.\nGrados de libertad (\\(df\\)): Los grados de libertad de la prueba, que es \\(n - 1\\) donde \\(n\\) es el número de categorías.\nValor p (p-value): Este es el \\(p-valor\\) asociado con el estadístico chi-cuadrado calculado.\n\nEn este ejemplo, el \\(p-valor\\) es 0.963, que es mucho mayor que \\(0.05\\). Por lo tanto, no rechazamos la hipótesis nula y concluimos que no hay suficiente evidencia para decir que los resultados del dado no siguen una distribución uniforme."
  },
  {
    "objectID": "nopara.html#pruebas-de-de-homogeneidad.",
    "href": "nopara.html#pruebas-de-de-homogeneidad.",
    "title": "4  Contraste no paramétrico",
    "section": "4.4 Pruebas de de homogeneidad.",
    "text": "4.4 Pruebas de de homogeneidad.\nLa prueba no paramétrica de homogeneidad se utiliza para determinar si dos o más muestras independientes provienen de la misma distribución o de distribuciones similares. Estas pruebas son útiles cuando no se cumplen los supuestos necesarios para las pruebas paramétricas, como la normalidad de los datos. Ejemplo de pruebas no paramétricas de Homogeneidad son:\n\nPrueba de Kolmogorov-Smirnov para dos muestras: Compara dos muestras para verificar si provienen de la misma distribución.\nPrueba de Mann-Whitney U (o Wilcoxon Rank-Sum Test): Compara dos muestras independientes para determinar si tienen la misma distribución.\nPrueba de Kruskal-Wallis: Extiende la prueba de Mann-Whitney U a más de dos muestras independientes.\n\n\n4.4.1 Prueba de Kolmogorov-Smirnov para dos muestras\nLa Prueba de Kolmogorov-Smirnov (K-S) para dos muestras es una prueba no paramétrica utilizada para determinar si dos muestras independientes provienen de la misma distribución. A diferencia de otras pruebas que se centran en comparar medias o varianzas, la prueba K-S compara las distribuciones acumuladas de dos muestras.\nLas hipótesis de la prueba son:\n\nHipótesis nula (\\(H_0\\)): Las dos muestras provienen de la misma distribución.\nHipótesis alternativa (\\(H_1\\)): Las dos muestras provienen de distribuciones diferentes.\n\nPara cada muestra, se construyen las funciones de distribución empírica (EDF). La función de distribución empírica \\(F_n(x)\\) es una función escalonada que aumenta en \\(\\frac{1}{n}\\) en cada punto de datos, donde \\(n\\) es el tamaño de la muestra. Para más detalles desplegar aquí:\n\n\n\n\n\n\nFunción de Distribución Empírica\n\n\n\n\n\nLa función de distribución empírica (EDF, por sus siglas en inglés) es una función de distribución de probabilidad utilizada para estimar la distribución subyacente de un conjunto de datos observados. Es una herramienta no paramétrica que proporciona una estimación de la función de distribución acumulada de una muestra de datos.\nDada una muestra de datos \\((X_1, X_2, \\ldots, X_n)\\), la función de distribución empírica \\(F_n(x)\\) se define como: \\[\nF_n(x) = \\frac{1}{n} \\sum_{i=1}^{n} I(X_i \\leq x)\n\\] donde \\(I(X_i \\leq x)\\) es una función indicadora que toma el valor \\(1\\) si \\(X_i \\leq x\\) y \\(0\\) en caso contrario.\nEn otras palabras, \\(F_n(x)\\) es la proporción de valores en la muestra que son menores o iguales a \\(x\\).\nPropiedades de la Función de Distribución Empírica\n\nEscalonada: La EDF es una función escalonada que incrementa en pasos de \\(1/n\\) en cada punto de datos.\nNo decreciente: La EDF nunca disminuye a medida que \\(x\\) aumenta.\nLímites: La EDF varía entre \\(0\\) y \\(1\\). Específicamente, \\(F_n(x) = 0\\) para \\(x\\) menor que el valor mínimo de la muestra y \\(F_n(x) = 1\\) para \\(x\\) mayor que el valor máximo de la muestra.\n\n\n\n\nCalculamos el Estadístico \\(D\\) de la prueba K-S es la máxima diferencia absoluta entre las dos funciones de distribución empírica: \\[\n     D = \\sup_x |F_{n_1}(x) - F_{n_2}(x)|\n\\]\nDonde, \\(F_{n_1}(x)\\) y \\(F_{n_2}(x)\\) son las funciones de distribución empírica de las dos muestras.\nEl \\(p-valor\\) se calcula para determinar la significancia de la diferencia observada. Se utiliza la distribución asintótica del estadístico D para calcular el valor p. El valor p se determina utilizando la distribución del estadístico \\(D\\) bajo la hipótesis nula de que ambas muestras provienen de la misma distribución. El cálculo exacto del \\(p-valor\\) para la prueba de K-S no es trivial y generalmente se realiza mediante métodos numéricos o tablas pre-calculadas. Sin embargo, se puede aproximar utilizando la distribución asintótica del estadístico \\(D\\).\nPara muestras grandes, el valor p se puede aproximar usando la fórmula: \\[\np \\approx Q_{KS}(\\sqrt{n} D)\n\\] donde:\n\n\\(n = \\frac{n_1 \\cdot n_2}{n_1 + n_2}\\) es el número efectivo de muestras.\n\\(D\\) es el valor del estadístico K-S.\n\\(Q_{KS}\\) es una función que representa la cola superior de la distribución de Kolmogorov-Smirnov.\n\nLa función \\(Q_{KS}\\) para grandes valores de \\(n\\) se puede aproximar usando la siguiente fórmula: \\[\nQ_{KS}(\\lambda) = 2 \\sum_{k=1}^{\\infty} (-1)^{k-1} e^{-2k^2 \\lambda^2}\n\\]\ndonde \\(\\lambda = \\sqrt{n} D\\).\nPara valores prácticos, esta sumatoria converge rápidamente y a menudo solo se necesita calcular unos pocos términos.\nComo en otros contrastes, si el \\(p-valor\\) calculado es menor que un grado de significancia estadística previamente fijado \\(\\alpha\\), entonces rechazamos la hipótesis nula en favor de la alternativa.\n\n\n\n\n\n\nEjemplo Práctico. Prueba K-S\n\n\n\n\n\nSupongamos que queremos comparar dos muestras para determinar si provienen de la misma distribución.\n\nMuestra 1: 1, 2, 3, 4, 5\nMuestra 2: 2, 3, 4, 5, 6\n\nFormulamos las hipótesis\n\nH0: Las dos muestras provienen de la misma distribución.\nH1: Las dos muestras provienen de distribuciones diferentes.\n\nCalculamos las funciones de distribución empírica \\(F_{n_1}(x)\\) y \\(F_{n_2}(x)\\) para las dos muestras.\n\n\n\n\\(x\\)\n\\(F_{n_1}(x)\\)\n\\(F_{n_2}(x)\\)\n\n\n\n\n1\n0.2\n0.0\n\n\n2\n0.4\n0.2\n\n\n3\n0.6\n0.4\n\n\n4\n0.8\n0.6\n\n\n5\n1.0\n0.8\n\n\n6\n1.0\n1.0\n\n\n\nCalculamos la máxima diferencia absoluta entre \\(F_{n_1}(x)\\) y \\(F_{n_2}(x)\\): \\[\nD = \\max(|0.2 - 0.0|, |0.4 - 0.2|, |0.6 - 0.4|, |0.8 - 0.6|, |1.0 - 0.8|, |1.0 - 1.0|) = 0.2\n\\]\nEl valor p se determina utilizando la distribución asintótica del estadístico \\(D\\). Esto generalmente se hace usando tablas de referencia o software estadístico.\n\n4.4.2 Implementación en R\nPodemos realizar esta prueba en R utilizando la función ks.test:\n\n# Datos de las dos muestras\nmuestra1 &lt;- c(1, 2, 3, 4, 5)\nmuestra2 &lt;- c(2, 3, 4, 5, 6)\n\n# Realizar la prueba de Kolmogorov-Smirnov\nks_test &lt;- ks.test(muestra1, muestra2)\n\n# Mostrar los resultados\nprint(ks_test)\n\n\n    Exact two-sample Kolmogorov-Smirnov test\n\ndata:  muestra1 and muestra2\nD = 0.2, p-value = 1\nalternative hypothesis: two-sided\n\n\nEl resultado de ks.test incluye varios componentes clave:\n\nEstadístico D: Este es el valor calculado del estadístico K-S.\nValor p (p-value): Este es el valor p asociado con el estadístico calculado.\nDescripción de las muestras: Indica las muestras comparadas.\n\nEn este ejemplo, el valor p es 1, que es mayor que \\(0.05\\). Por lo tanto, no rechazamos la hipótesis nula y concluimos que no hay suficiente evidencia para decir que las dos muestras provienen de distribuciones diferentes.\n\n\n\n\n\n\n4.4.3 Prueba de Mann-Whitney U\nLa prueba de Mann-Whitney U, también conocida como prueba de Wilcoxon para muestras independientes, es una prueba no paramétrica que se utiliza para comparar dos muestras independientes para determinar si provienen de la misma distribución. Es una alternativa a la prueba t de Student cuando no se cumplen los supuestos de normalidad. En lugar de trabajar con los valores originales, la prueba utiliza los rangos de los datos.\nLas hipótesis que vamos a contrastar son:\n\nHipótesis Nula (\\(H_0\\)): Las dos muestras provienen de la misma distribución.\nHipótesis Alternativa (\\(H_1\\)): Las dos muestras no provienen de la misma distribución.\n\nEn primer lugar se combinan los datos de ambas muestras y se ordenan los valores de menor a mayor. A continuación se asignan rangos a estos valores, manejando adecuadamente los empates (otorgando a los valores iguales el rango promedio).\nSe calcula el estadístico del contraste tal y como sigue: \\[  U_1 = n_1n_2+ \\frac{n_1 (n_1 + 1)}{2} -R_1\n\\] \\[\nU_2 =U_1 = n_1n_2+ \\frac{n_2 (n_2 + 1)}{2} -R_2\n\\] donde:\n\n\\(n_1\\) y \\(n_2\\) son los tamaños de las dos muestras.\n\\(R_1\\) y \\(R_2\\) son las sumas de los rangos de las muestras \\(1\\) y \\(2\\), respectivamente.\n\nEl estadístico \\(U\\) final es: \\[\nU=min(U_1,U_2)\n\\]\nEl \\(p-valor\\) se determina comparando el estadístico \\(U\\) con una distribución de referencia para U (tabla de Mann-Whitney) o mediante aproximación normal para grandes tamaños de muestra. En ese caso: \\[\nZ=\\frac{U-E(U)}{\\sqrt{Var(U)}} \\approx N(0,1)\n\\] siendo \\[\nE(U)=n_1n_2+\\frac{n_1(n_1+1)}{2}-E(R_1)\n\\] \\[\nE(U)= n_1n_2+\\frac{n_1(n_1+1)}{2}-\\frac{n_1(n_1+n_2+1)}{2}=\\frac{n_1n_2}{2}\n\\] y \\[\nVar(U)=Var(R_1)=\\frac{n_1n_2(n_1+n_2+1)}{12}\n\\] Y podemos calcular el \\(p-valor\\) como hemos hecho en métodos anteriores: \\[\np-valor=P(Z&gt;z)\n\\] Siendo \\(z\\) el valor del estadístico calculado en las muestras.\nComparamos el \\(p-valor\\) p con el nivel de significancia (\\(\\alpha\\)), generalmente \\(0.05\\). Si (\\(p-valor &lt; \\alpha\\)), se rechaza la hipótesis nula.\n\n\n\n\n\n\nEjemplo Práctico. Prueba Mann-Whitney\n\n\n\n\n\nSupongamos que queremos comparar los tiempos de entrega (en días) de dos proveedores distintos:\n\nProveedor A: \\(2, 3, 5, 6, 8\\)\nProveedor B: \\(1, 4, 4, 7, 9\\)\n\nEn primer lugar combinar y ordenamos las muestra\nValores combinados y ordenados: \\(1, 2, 3, 4, 4, 5, 6, 7, 8, 9\\)\nAsignación de rangos:\n\n1: rango 1\n2: rango 2\n3: rango 3\n4: rango 4.5 (promedio de rangos 4 y 5)\n4: rango 4.5\n5: rango 6\n6: rango 7\n7: rango 8\n8: rango 9\n9: rango 10\n\nRangos para cada muestra:\n\nProveedor A: \\(2, 3, 6, 7, 9\\) (sumados dan \\(R_1 = 27\\))\nProveedor B: \\(1, 4.5, 4.5, 8, 10\\) (sumados dan \\(R_2 = 28\\))\n\nPasamos a calcular el estadístico \\(U\\)\nTamaños de muestra:\n\n\\(n_1 = 5\\)\n\\(n_2 = 5\\)\n\nCálculo de \\(U_1\\) y \\(U_2\\):\n\\[\nU_1 = n_1n_2+ \\frac{n_1 (n_1 + 1)}{2} -R_1  =5\\cdot 5+ \\frac{5 \\cdot 6}{2} -27 = 23\n\\]\n\\[\nU_2 =n_1n_2+ \\frac{n_2 (n_2 + 1)}{2} -R_2  =5\\cdot 5+  \\frac{5 \\cdot 6}{2}-28 = 12\n\\]\nEl estadístico U es el menor de \\(U_1\\) y \\(U_2\\): \\(U = 12\\).\nPara determinar el valor p, utilizamos una tabla de referencia para U o una aproximación normal si las muestras son grandes. En este caso, consultamos una tabla para \\(n_1 = 5\\) y \\(n_2 = 5\\). Si no se dispone de la tabla, se puede utilizar software estadístico para calcular el \\(p-valor\\) como sigue:\n\n# Datos de ejemplo\nproveedorA &lt;- c(2, 3, 5, 6, 8)\nproveedorB &lt;- c(1, 4, 4, 7, 9)\n\n# Realizar la prueba de Mann-Whitney U\ntest &lt;- wilcox.test(proveedorA, proveedorB, alternative = \"two.sided\")\n\nWarning in wilcox.test.default(proveedorA, proveedorB, alternative =\n\"two.sided\"): cannot compute exact p-value with ties\n\n# Mostrar los resultados\nprint(test)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  proveedorA and proveedorB\nW = 12, p-value = 1\nalternative hypothesis: true location shift is not equal to 0\n\n\nLos resultados muestra:\n\nEstadístico W: El valor calculado del estadístico U.\nValor p (p-value): El valor p asociado con el estadístico.\nHipótesis alternativa: La prueba es de dos lados, lo que significa que estamos probando si las distribuciones son diferentes en cualquier dirección.\n\nEn este ejemplo, el \\(p-valor\\) es 1, que es mayor que \\(0.05\\), lo que indica que no hay suficiente evidencia para rechazar la hipótesis nula. Por lo tanto, no podemos concluir que las dos muestras provienen de diferentes distribuciones.\n\n\n\n\n\n4.4.4 Prueba de Kruskal-Wallis\nLa prueba de Kruskal-Wallis es una prueba no paramétrica utilizada para comparar tres o más muestras independientes para determinar si provienen de la misma distribución. Es una extensión de la prueba de Mann-Whitney U a más de dos grupos y una alternativa robusta a la ANOVA que veremos en el Capítulo 5 cuando no se cumplen los supuestos de normalidad y homogeneidad de varianzas.\nDado que es una prueba no paramétrica, no requiere que los datos provengan de una distribución normal. Al igual que la prueba de Mann-Whitney, la prueba de Kruskal-Wallis trabaja con los rangos de los datos en lugar de los valores originales.\nLas hipótesis son:\n\nHipótesis Nula (\\(H_0\\)): Todas las muestras provienen de la misma distribución.\nHipótesis Alternativa (\\(H_1\\)): Al menos una de las muestras proviene de una distribución diferente.\n\nA continuación, se combinan los datos de todas las muestras y se ordenan los valores de menor a mayor. Se asignan rangos a estos valores, manejando adecuadamente los empates (otorgando a los valores iguales el rango promedio).\nCalculamos el estadístico H utilizando la siguiente fórmula: \\[\n     H = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N+1)\n\\] donde:\n\n\\(N\\) es el tamaño total de la muestra (suma de los tamaños de las \\(k\\) muestras).\n\\(R_i\\) es la suma de los rangos de la \\(i\\)-ésima muestra.\n\\(n_i\\) es el tamaño de la \\(i\\)-ésima muestra.\n\\(k\\) es el número de muestras.\n\nPara tamaños de muestra relativamente grandes el estadístico \\(H\\) sigue una distriniución \\(\\chi^2\\) con \\(k-1\\) grados de libertdad. El \\(p-valor\\) se determina, por tanto como \\[\np-valor=P(H&gt;h)\n\\] donde h es el valor calculado para un caso particular.\n\n\n\n\n\n\nEjemplo Práctico. Prueba Kruskal-Wallis\n\n\n\n\n\nSupongamos que queremos comparar los tiempos de espera (en días) de tres proveedores diferentes:\n\nProveedor A: \\(2, 3, 5, 6, 8\\)\nProveedor B: \\(1, 4, 4, 7, 9\\)\nProveedor C: \\(3, 4, 6, 8, 10\\)\n\nValores combinados y ordenados: \\(1, 2, 3, 3, 4, 4, 4, 5, 6, 6, 7, 8, 8, 9, 10\\)\nAsignación de rangos:\n\n1: rango 1\n2: rango 2\n3: rango 3.5 (promedio de rangos 3 y 4)\n3: rango 3.5\n4: rango 6 (promedio de rangos 5, 6 y 7)\n4: rango 6\n4: rango 6\n5: rango 8\n6: rango 9.5 (promedio de rangos 9 y 10)\n6: rango 9.5\n7: rango 11\n8: rango 12.5 (promedio de rangos 12 y 13)\n8: rango 12.5\n9: rango 14\n10: rango 15\n\nRangos para cada muestra:\n\nProveedor A: \\(2, 3.5, 8, 9.5, 12.5\\) (sumados dan \\(R_1 = 35.5\\))\nProveedor B: \\(1, 6, 6, 11, 14\\) (sumados dan \\(R_2 = 38\\))\nProveedor C: \\(3.5, 6, 9.5, 12.5, 15\\) (sumados dan \\(R_3 = 46.5\\))\n\nCalculamos el Estadístico \\(H\\)\nTamaños de muestra:\n\n\\(n_1 = 5\\)\n\\(n_2 = 5\\)\n\\(n_3 = 5\\)\n\nTamaño total de la muestra: \\(N = 15\\)\n\\[\nH = \\frac{12}{N(N+1)} \\sum_{i=1}^{k} \\frac{R_i^2}{n_i} - 3(N+1)\n\\]\n\\[\nH = \\frac{12}{15 \\cdot 16} \\left( \\frac{35.5^2}{5} + \\frac{38^2}{5} + \\frac{46.5^2}{5} \\right) - 3 \\cdot 16 = 0.665\n\\]\nPara determinar el \\(p-valor\\), utilizamos una tabla de distribución chi-cuadrado con \\(k-1 = 3-1 = 2\\) grados de libertad.\n\n# Datos de ejemplo\nproveedorA &lt;- c(2, 3, 5, 6, 8)\nproveedorB &lt;- c(1, 4, 4, 7, 9)\nproveedorC &lt;- c(3, 4, 6, 8, 10)\n\n# Realizar la prueba de Kruskal-Wallis\ntest &lt;- kruskal.test(list(proveedorA, proveedorB, proveedorC))\n\n# Mostrar los resultados\nprint(test)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  list(proveedorA, proveedorB, proveedorC)\nKruskal-Wallis chi-squared = 0.67342, df = 2, p-value = 0.7141\n\n\nde donde obtenemos\n\nKruskal-Wallis chi-squared: El valor calculado del estadístico \\(H\\).\ndf: Los grados de libertad.\nValor p (p-value): El \\(p-valor\\) asociado con el estadístico \\(H\\).\n\nEn este ejemplo, el \\(p-valor\\) es 0.714, que es mayor que \\(0.05\\), lo que indica que no hay suficiente evidencia para rechazar la hipótesis nula. Por lo tanto, no podemos concluir que las tres muestras provienen de diferentes distribuciones; podrían provenir de la misma distribución."
  },
  {
    "objectID": "nopara.html#prueba-sobre-muestras-pareadas",
    "href": "nopara.html#prueba-sobre-muestras-pareadas",
    "title": "4  Contraste no paramétrico",
    "section": "4.5 Prueba sobre muestras pareadas",
    "text": "4.5 Prueba sobre muestras pareadas\n\n4.5.1 Prueba del signo\nLa prueba del signo es una prueba no paramétrica utilizada para comparar dos muestras relacionadas o emparejadas. Se emplea cuando se tienen dos conjuntos de datos dependientes y se desea determinar si hay una diferencia significativa en sus medianas. Es particularmente útil cuando los supuestos de normalidad necesarios para pruebas paramétricas como la prueba t para muestras pareadas no se cumplen.\nSupongamos que tenemos dos muestras relacionadas \\(X\\) e \\(Y\\) de tamaño \\(n\\):\nFormular las Hipótesis: - Hipótesis Nula (\\(H_0\\)): Las medianas de las dos muestras son iguales. - Hipótesis Alternativa (\\(H_1\\)): Las medianas de las dos muestras son diferentes.\nCalcular las Diferencias: Para cada par \\((X_i, Y_i)\\), calcular la diferencia \\(D_i = X_i - Y_i\\).\nContar los Signos: - Contar cuántas diferencias son positivas (\\(D_i &gt; 0\\)). - Contar cuántas diferencias son negativas (\\(D_i &lt; 0\\)). - Ignorar las diferencias que son cero (\\(D_i = 0\\)).\nEstadístico de Prueba:\n\nDenotar el número de diferencias positivas por \\(S_+\\).\nDenotar el número de diferencias negativas por \\(S_-\\).\n\nEl estadístico de la prueba del signo es el menor entre \\(S_+\\) y \\(S_-\\):\\[\n   s=min(S_+,S_-)\n   \\]\nDeterminar el Valor Crítico: Consultar una tabla de la distribución binomial o la tabla de la prueba del signo para obtener el valor crítico correspondiente al nivel de significancia \\(\\alpha\\) y el tamaño de la muestra efectiva \\(n\\) (número de pares no nulos). Podemos calcular el valor crítico como sigue: \\[\np-valor=P(S\\leq s)\n\\] donde \\(S\\) es binomial \\(n\\), \\(p=0.5\\).\nDecisión:\n\nRechazar \\(H_0\\) si el estadístico de la prueba es menor o igual al valor crítico.\nNo rechazar \\(H_0\\) si el estadístico de la prueba es mayor que el valor crítico.\n\n\n\n\n\n\n\nEjemplo Práctico. Prueba del signo\n\n\n\n\n\nSupongamos que un investigador quiere comparar los tiempos de reacción antes y después de un tratamiento en 10 sujetos. Los datos son los siguientes:\n\n\n\nSujeto\nAntes (X)\nDespués (Y)\n\n\n\n\n1\n15\n10\n\n\n2\n20\n18\n\n\n3\n25\n20\n\n\n4\n30\n30\n\n\n5\n18\n15\n\n\n6\n22\n19\n\n\n7\n26\n21\n\n\n8\n28\n26\n\n\n9\n24\n22\n\n\n10\n20\n20\n\n\n\nFormular las Hipótesis: - \\(H_0\\): No hay diferencia en los tiempos de reacción antes y después del tratamiento. - \\(H_1\\): Hay una diferencia en los tiempos de reacción antes y después del tratamiento.\nCalcular las Diferencias:\n\n\n\nSujeto\nDiferencia (D = X - Y)\nSigno\n\n\n\n\n1\n5\n+\n\n\n2\n2\n+\n\n\n3\n5\n+\n\n\n4\n0\n0\n\n\n5\n3\n+\n\n\n6\n3\n+\n\n\n7\n5\n+\n\n\n8\n2\n+\n\n\n9\n2\n+\n\n\n10\n0\n0\n\n\n\nContar los Signos: - \\(S_+ = 8\\) - \\(S_- = 0\\) - Pares nulos (\\(D = 0\\)): 2\nEstadístico de Prueba: El estadístico de la prueba del signo es el menor entre \\(S_+\\) y \\(S_-\\), que es 0 en este caso.\nDeterminar el Valor Crítico: Para un nivel de significancia \\(\\alpha = 0.05\\) y \\(n = 8\\) (solo los pares no nulos se consideran), consultamos la tabla de la prueba del signo y encontramos que el valor crítico es 1.\nDecisión: - Como el estadístico de la prueba (0) es menor que el valor crítico (1), rechazamos la hipótesis nula \\(H_0\\). Hay suficiente evidencia para concluir que hay una diferencia significativa en los tiempos de reacción antes y después del tratamiento.\n\n\n\n\n\n4.5.2 Prueba de rangos de signos de Wilcoxon\nPara comparar muestras pareadas la opción no paramétrica más común es la prueba de Wilcoxon de rangos de signos (Wilcoxon signed-rank test). La prueba de Wilcoxon de rangos de signos es una alternativa no paramétrica a la prueba t de muestras pareadas. Se utiliza cuando los datos no cumplen con los supuestos de normalidad necesarios para la prueba t. En lugar de comparar medias, esta prueba compara las medianas de las diferencias entre las dos muestras pareadas. Es una prueba ideal para muestras pequeñas y para datos ordinales o de razón/intervalo cuando la normalidad no puede ser asumida.\nProcedimiento:\n\nCalcular las diferencias entre cada par de observaciones.\nOrdenar las diferencias absolutas y asignarles rangos, ignorando las diferencias que sean cero.\nAsignar signos a los rangos de acuerdo con el signo de las diferencias originales.\nCalcular la suma de los rangos positivos y la suma de los rangos negativos.\nDeterminar el estadístico de prueba: El estadístico de Wilcoxon es el menor de las dos sumas de rangos.\nComparar el estadístico de prueba con los valores críticos de la tabla de Wilcoxon para determinar la significancia estadística.\n\nLa prueba de Wilcoxon de signos y rangos es robusta y fácil de implementar, lo que la convierte en una herramienta valiosa para el análisis de muestras pareadas en situaciones donde los supuestos de normalidad no se cumplen.\n\n\n\n\n\n\nEjemplo Práctico. Prueba sobre datos emparejados\n\n\n\n\n\nSupongamos que tenemos dos conjuntos de datos emparejados que representan las puntuaciones antes y después de una intervención:\n\n# Datos de ejemplo\nantes &lt;- c(10, 20, 30, 40, 50)\ndespues &lt;- c(12, 18, 33, 35, 55)\n\n# Realizar la prueba de Wilcoxon de signos y rangos\nresultado &lt;- wilcox.test(antes, despues, paired = TRUE)\n\n# Mostrar el resultado\nprint(resultado)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  antes and despues\nV = 6, p-value = 0.7855\nalternative hypothesis: true location shift is not equal to 0\n\n\nEl estadístico V es 6 y el \\(p-valor\\) es 0.7854947. En este caso, dado que el valor p es mayor que un nivel de significancia común (como \\(0.05\\)), no se rechaza la hipótesis nula de que no hay una diferencia significativa entre las puntuaciones antes y después de la intervención."
  },
  {
    "objectID": "intro.html#ciencia-de-datos",
    "href": "intro.html#ciencia-de-datos",
    "title": "1  Introducción",
    "section": "",
    "text": "Ciencia de datos\n\n\n\nLa ciencia de datos es un área interdisciplinar que abarca un conjunto de principios, problemas, definiciones, algoritmos y procesos cuyo objetivo es extraer conocimiento no obvio y útil a partir de un conjunto de datos.\n\n\n\n\n\n\n\n\n\nMatemáticas y Estadística\n\n\n\n\n\nConocimientos de Matemáticas, y más concretamente de Estadística, son necesarios para analizar correctamente los datos disponibles. Conceptos como intervalo de confianza, histograma de frecuencias, contraste de hipótesis, espacio de características, métrica, hiperplano separador, error de clasificación, p_valor, etc. han de formar parte del conocimiento de todo científico de datos. Un equipo de ciencia de datos ha de contar con uno o varios expertos en Matemáticas y Estadística. Un buen libro de referencia para dominar los conceptos fundamentales en el ámbito matemático y estadístico que son necesarios en ciencia de datos es (Hastie et al. 2009). Además disponéis de esta versión online (James et al. 2013), similar pero más enfocada al análisis de datos. ¿Lo conocías ya?\n\n\n\n\n\n\n\n\n\nCiencias de la Computación\n\n\n\n\n\nEstudio del diseño y la arquitectura de los ordenadores y su aplicación en el campo de la ciencia y la tecnología, incluyendo el hardware, el software y las redes de comunicación. Un experto en ciencias de la computación ha de dominar lenguajes de programación como Python, JavaScript, C++, así como los elementos fundamentales que hacen que estos lenguajes funcionen. Algunas referencias útiles para estudiar estos lenguajes son (Hao y Ho 2019), (Osmani 2012) y (Oualline 2003). De igual modo, el científico de datos, ha de conocer ámbitos como los diferentes sistemas operativos, redes, seguridad, algoritmos y arquitectura de ordenadores. Un equipo de ciencia de datos ha de contar con uno o varios expertos en Ciencias de la Computación.\n\n\n\n\n\n\n\n\n\nConocimiento del Dominio\n\n\n\n\n\nRepresenta el problema que deseamos estudiar, la organización que lo proporciona y su dominio de aplicación. Existen casos de éxito de la ciencia de datos en prácticamente todos los dominios de interés que podamos mencionar: medicina, ciudades inteligentes, energía, telecomunicaciones, finanzas, seguros, ganadería, agricultura, ciencias sociales, ciberseguridad, etc. Un equipo de ciencia de datos ha de contar con uno o varios expertos en el dominio de aplicación. Estos expertos han de implicarse, fuertemente, en el problema que se quiere resolver. En (Kelleher, Mac Namee, y D’arcy 2020) podéis encontrar ejemplos muy interesantes de cómo la ciencia de datos es aplicada en diferentes dominios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Fundamentos\n\n\n\n\n\n\n\n\n\n\n\n(b) Aplicaciones\n\n\n\n\n\n\n\nFigura 1.1: Ciencia de datos\n\n\n\n\n\n\n\n\n\n\nDefinir objetivos\n\n\n\n\n\nEntender el negocio es el primer paso en el proceso. Con ayuda de expertos en el dominio, se definen las preguntas a responder con el proyecto (definición de objetivos de la entidad responsable del proyecto). Una vez comprendido el negocio se designa una solución analítica para abordar el problema. En esta etapa las reuniones entre los matemáticos, informáticos y los expertos del dominio (habitualmente trabajadores con grandes conocimiento del problema que se trata de abordar) son frecuentes, necesarias y (casi) nunca, suficientes.\n\n\n\n\n\n\n\n\n\nObtener, preparar y gestionar los datos\n\n\n\n\n\nMediante técnicas informáticas, se recopilan y se preparan los datos para su posterior análisis. Podremos hablar de Big Data si los datos se caracterizan por su volumen, variedad o velocidad de procesamiento. Todo proceso de Ciencia de Datos es un proceso de aprendizaje en torno al dato. Las preguntas no surgen de los datos, pero se necesitan datos para responderlas. Es esta la etapa en la que trataremos una parte fundamental de todo el proceso: el análisis exploratorio de datos. Podrás estudiar más sobre cómo preparar los datos para etapas posteriores en el tema 3 de este libro.\n\n\n\n\n\n\n\n\n\nConstruir un modelo\n\n\n\n\n\nA través de métodos matemáticos y estadísticos se estudian y analizan los datos, se construyen algoritmos y se aplican modelos. Es la etapa asociada a los modelos de ML que trataremos en los temas 5,7 y 8.\n\n\n\n\n\n\n\n\n\nEvaluar y criticar el modelo\n\n\n\n\n\nSe definen medidas de rendimiento de los modelos que permitan su evaluación tanto por parte del desarrollador como por parte del cliente. Trataremos estas medidas en el tema 6 de nuestro curso.\n\n\n\n\n\n\n\n\n\nVisualización y presentación\n\n\n\n\n\nSe presentan los resultados buscando la comprensión por parte del cliente. No se trata únicamente de aplicar modelos complejos que nadie, más allá del desarrollador o experto matemático, pueda comprender. Muy al contrario, existe en la actualidad una corriente de investigación orientada a construir métodos capaces de ser explicables, junto con otras técnicas para convertir en entendibles los resultados obtenidos por los más complejos algoritmos matemáticos. Hablaremos de explicabilidad de modelos en el último tema del curso.\n\n\n\n\n\n\n\n\n\nDespliegue en producción\n\n\n\n\n\nLa solución final se convierte en un producto que podrá ser comercializado. Los modelos de ML se construyen para un propósito dentro de una organización. La integración de este modelo dentro del proceso de la organización debería de ser el último paso del proyecto de ciencia de datos.\n\n\n\n\n\n\n\n\n\n\n\nEstadística Descriptiva\n\n\n\n\n\nSe ocupa de resumir y describir las características de un conjunto de datos mediante herramientas gráficas y numéricas, como tablas, gráficos, medias, medianas, varianzas, etc. Su objetivo es proporcionar una visión clara y comprensible de la estructura y características de los datos.\n\n\n\n\n\n\n\n\n\nEstadística Inferencial\n\n\n\n\n\nUtiliza muestras de datos para hacer generalizaciones o inferencias sobre una población más amplia. Involucra el uso de métodos como la estimación de parámetros, pruebas de hipótesis y la construcción de intervalos de confianza. La inferencia estadística permite tomar decisiones y hacer predicciones basadas en datos muestreados.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#sec-inferencia",
    "href": "intro.html#sec-inferencia",
    "title": "1  Introducción",
    "section": "1.2 Inferencia Estadística",
    "text": "1.2 Inferencia Estadística\nLa inferencia estadística se refiere a los métodos y procesos utilizados para extraer conclusiones acerca de una población a partir de una muestra de datos. A diferencia de la mera descripción de datos, la inferencia permite ir más allá de lo observado y hacer generalizaciones, estimaciones y decisiones en presencia de incertidumbre. Esto es fundamental para cualquier análisis de datos que aspire a ser predictivo o que busque comprender fenómenos más amplios que los capturados por los datos disponibles.\nAlgunos ejemplos del uso de la inteferencia estadística son:\n\nProporción de votantes a un determinado partido\nProporción de elementos defectuosos en una partida de productos\nProporción de paquetes que llegan tarde\nSalario medio\nAltura media\nConcentración media de un componente\nDuración media de un viaje en tren\nEspera media entre dos trenes consecutivos\n\nEn el contexto de la Ciencia de Datos, la inferencia estadística permite abordar preguntas críticas como:\n\n¿Cuál es la efectividad de un nuevo medicamento?\n¿Qué factores influyen en la satisfacción del cliente?\n¿Cómo se puede predecir el comportamiento futuro de los mercados financieros?\n\nEstas preguntas no solo requieren una recopilación cuidadosa de datos, sino también un análisis riguroso que tenga en cuenta la variabilidad inherente y las posibles fuentes de error.\nLa relevancia de la inferencia estadística en la Ciencia de Datos se manifiesta en varias áreas clave. Despliega el panel para averiguarlas.\n\n\n\n\n\n\nÁreas clave\n\n\n\n\n\nEstas son algunas de las áreas clave donde la inferencia estadística cobra valor.\n\nModelado predictivo: La inferencia estadística es fundamental para construir modelos que pueden predecir resultados futuros basándose en datos históricos. Estos modelos se utilizan en una variedad de campos, desde el marketing hasta la medicina y las finanzas.\nAnálisis experimental: En muchos dominios, como la biomedicina y la psicología, los experimentos controlados son esenciales para determinar causalidad y no solo correlación. La inferencia estadística proporciona el marco para diseñar estos experimentos y analizar los resultados de manera adecuada.\nDecisiones basadas en datos: En el ámbito empresarial y gubernamental, las decisiones basadas en datos permiten optimizar procesos, asignar recursos de manera eficiente y mejorar los servicios. La inferencia estadística permite que estas decisiones sean informadas y respaldadas por evidencia cuantitativa.\nManejo de la incertidumbre: En cualquier análisis de datos, es crucial manejar y comunicar la incertidumbre. La inferencia estadística ofrece métodos para cuantificar esta incertidumbre y tomar decisiones informadas pese a la presencia de variabilidad y error.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#datos-y-variables",
    "href": "intro.html#datos-y-variables",
    "title": "1  Introducción",
    "section": "1.3 Datos y variables",
    "text": "1.3 Datos y variables\nLos datos son las observaciones o medidas que recopilamos del mundo que nos rodea. Estos pueden ser números, categorías o cualquier tipo de información cuantificable. Llamaremos elementos a los individuos, los sujetos, las observaciones sobre las que se recojen un conjunto de variables.\n\n1.3.1 Datos en R\nR incluye en sus librerías numerosos conjuntos de datos. Para acceder a ellos, basta con cargar la librería correspondiente.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nEs posible leer datos desde una cuenta de git. Y podemos tener una primera visión de los datos con sentencias comohead que nos enseña las primeras observaciones y sus variables.\n\nlibrary (readr)\n\nurlfile=\"https://raw.githubusercontent.com/IsaacMartindeDiego/IA/master/datasets/california_housing.csv\"\n\nmydata&lt;-read_csv(url(urlfile))\n\nhead(mydata)\n\n# A tibble: 6 × 10\n  longitude latitude housing_median_age total_rooms total_bedrooms population\n      &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;       &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1     -122.     37.9                 41         880            129        322\n2     -122.     37.9                 21        7099           1106       2401\n3     -122.     37.8                 52        1467            190        496\n4     -122.     37.8                 52        1274            235        558\n5     -122.     37.8                 52        1627            280        565\n6     -122.     37.8                 52         919            213        413\n# ℹ 4 more variables: households &lt;dbl&gt;, median_income &lt;dbl&gt;,\n#   median_house_value &lt;dbl&gt;, ocean_proximity &lt;chr&gt;\n\ndim(mydata)\n\n[1] 20640    10\n\nsummary(mydata)\n\n   longitude         latitude     housing_median_age  total_rooms   \n Min.   :-124.3   Min.   :32.54   Min.   : 1.00      Min.   :    2  \n 1st Qu.:-121.8   1st Qu.:33.93   1st Qu.:18.00      1st Qu.: 1448  \n Median :-118.5   Median :34.26   Median :29.00      Median : 2127  \n Mean   :-119.6   Mean   :35.63   Mean   :28.64      Mean   : 2636  \n 3rd Qu.:-118.0   3rd Qu.:37.71   3rd Qu.:37.00      3rd Qu.: 3148  \n Max.   :-114.3   Max.   :41.95   Max.   :52.00      Max.   :39320  \n                                                                    \n total_bedrooms     population      households     median_income    \n Min.   :   1.0   Min.   :    3   Min.   :   1.0   Min.   : 0.4999  \n 1st Qu.: 296.0   1st Qu.:  787   1st Qu.: 280.0   1st Qu.: 2.5634  \n Median : 435.0   Median : 1166   Median : 409.0   Median : 3.5348  \n Mean   : 537.9   Mean   : 1425   Mean   : 499.5   Mean   : 3.8707  \n 3rd Qu.: 647.0   3rd Qu.: 1725   3rd Qu.: 605.0   3rd Qu.: 4.7432  \n Max.   :6445.0   Max.   :35682   Max.   :6082.0   Max.   :15.0001  \n NA's   :207                                                        \n median_house_value ocean_proximity   \n Min.   : 14999     Length:20640      \n 1st Qu.:119600     Class :character  \n Median :179700     Mode  :character  \n Mean   :206856                       \n 3rd Qu.:264725                       \n Max.   :500001                       \n                                      \n\n\nLa estructura de datos más habitual para realizar análisis de datos es el data frame. ¿Has estudiado este concepto en cursos anteriores?. Los data frame son estructuras de datos de dos dimensiones (como una matriz) que pueden contener datos de diferentes tipos. Normalmente nos referimos a las filas de un data frame como observaciones o registros, mientras que las columnas reciben el nombre de campos, variables, o características.\n\nL3 &lt;- LETTERS[1:3]\nchar &lt;- sample(L3, 10, replace = TRUE)\ndatos &lt;- data.frame(x = 1, y = 1:10, char = char)\ndatos\n\n   x  y char\n1  1  1    C\n2  1  2    A\n3  1  3    A\n4  1  4    B\n5  1  5    A\n6  1  6    A\n7  1  7    C\n8  1  8    A\n9  1  9    B\n10 1 10    A\n\nis.data.frame(datos)\n\n[1] TRUE\n\n\nUn tibble, o tbl_df, es una actualización del concepto del data frame. Los tibbles son data.frames perezosos. Esto le obliga a enfrentarse a los problemas antes, lo que normalmente conduce a un código más limpio y expresivo. Los tibbles también tienen un método print() mejorado que facilita su uso con grandes conjuntos de datos que contienen objetos complejos.\n\nlibrary(tidyverse)\nas.tibble(iris)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# ℹ 140 more rows\n\n\n\n\n\n\n\n\nPráctica\n\n\n\nEs importante que realices algún ejercicio en R, leyendo datos de diferentes fuentes y familiarizándote con las diferentes estructuras de datos que R proporciona.\n\n\n\n\n1.3.2 Tipo de variables\nUna variable es una característica o atributo que se pueden observar en los elementos y que puede tomar diferentes valores. Llamaremos valores a los resultados que se pueden observar de la característica en el elemento. Las variables siguen una distribución de probabilidad. Las variables pueden ser de varios tipos:\n\nCualitativas: También conocidas como categóricas, estas variables describen atributos o cualidades y se dividen en nominales (sin orden específico, como colores) y ordinales (con un orden, como niveles de satisfacción).\nCuantitativas: Estas variables son numéricas y pueden ser discretas (valores contables, como el número de hijos) o continuas (valores dentro de un rango, como la altura o el peso).\nMarcas de tiempo o identificadores: Como por ejemplo la fecha y hora de una transacción o el código de un producto o el número de identidad.\n\nEn este tema vamos a trabajar con los datos de Bank Marketing del repositorio UCI. En primer lugar debemos comprender el problema. ¿Qué sabes del marketing bancario? En el caso que nos ocupa, los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una entidad bancaria portuguesa. El objetivo del problema en cuestión es predecir si el cliente suscribirá un depósito a plazo (variable objetivo). Abordaremos este problema en próximos cursos. En este curso nos conformamos con entender los datos y proponer ciertas hipótesis en base a ellos.\nLas variables que debemos estudiar son:\nVariables de entrada:\n# datos del cliente bancario:\n\nedad (variable numérica)\nempleo : tipo de empleo (variable categórica con las siguientes categorías: “admin.”, “desconocido”, “desempleado”, “directivo”, “empleada del hogar”, “empresario”, “estudiante”, “obrero”, “autónomo”, “jubilado”, “técnico”, “servicios”)\nestado civil : estado civil (variable categórica con categorías: “casado”, “divorciado”, “soltero”; nota: “divorciado” significa divorciado o viudo)\neducación (variable categórica con categorías: “desconocida”, “secundaria”, “primaria”, “terciaria”)\nimpago: ¿tiene un crédito impagado? (variable binaria con dos posibles valores: “sí”, “no”)\nsaldo: saldo medio anual, en euros (variable numérica)\nvivienda: ¿tiene préstamo para vivienda? (variable binaria: “sí”, “no”)\npréstamo: ¿tiene préstamo personal? (variable binaria: “sí”, “no”)\n# relacionado con el último contacto de la campaña actual:\ncontacto: tipo de comunicación del contacto (variable categórica: “desconocido”, “teléfono”, “móvil”)\ndía: día del mes del último contacto (variable numérica)\nmes: mes del año del último contacto (variable categórica: “ene”, “feb”, “mar”, …, “nov”, “dic”)\nduración: duración del último contacto, en segundos (variable numérica)\n\n# otros atributos\n\ncampaña: número de contactos realizados durante esta campaña y para este cliente (variable numérica, incluye el último contacto)\npdays: número de días transcurridos desde que el cliente fue contactado por última vez en una campaña anterior (variable numérica, -1 significa que el cliente no fue contactado previamente)\nprevious: número de contactos realizados antes de esta campaña y para este cliente (variable numérica)\npoutcome: resultado de la campaña de marketing anterior (variable categórica: “desconocido”, “otro”, “fracaso”, “éxito”)\n\n# Variable de salida (objetivo deseado):\n17 - y: ¿ha suscrito el cliente un depósito a plazo? (variable binaria: “sí”, “no”)\n\n\n\n\n\n\nPara recordar\n\n\n\nA veces (muchas veces) la descripción que encontramos en una primera etapa no coincide al completo con los datos que luego nos entrega el cliente.\nEn otras ocasiones no se dispone de la descripción de las variables. En ese caso, ¡hay que hacer lo imposible por conseguirla!\n\n\nLeemos los datos con R.\n\nlibrary(tidyverse)\nbank = read.csv('https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv')\ndim(bank)\n\n[1] 11162    17\n\nbank=as.tibble(bank)\nbank\n\n# A tibble: 11,162 × 17\n     age job       marital education default balance housing loan  contact   day\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n 1    59 admin.    married secondary no         2343 yes     no    unknown     5\n 2    56 admin.    married secondary no           45 no      no    unknown     5\n 3    41 technici… married secondary no         1270 yes     no    unknown     5\n 4    55 services  married secondary no         2476 yes     no    unknown     5\n 5    54 admin.    married tertiary  no          184 no      no    unknown     5\n 6    42 manageme… single  tertiary  no            0 yes     yes   unknown     5\n 7    56 manageme… married tertiary  no          830 yes     yes   unknown     6\n 8    60 retired   divorc… secondary no          545 yes     no    unknown     6\n 9    37 technici… married secondary no            1 yes     no    unknown     6\n10    28 services  single  secondary no         5090 yes     no    unknown     6\n# ℹ 11,152 more rows\n# ℹ 7 more variables: month &lt;chr&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;,\n#   previous &lt;int&gt;, poutcome &lt;chr&gt;, deposit &lt;chr&gt;\n\n\nDisponemos de más de 10000 observaciones y un total de 17 variables.\nPara averiguar qué tipo de variables manejamos, ejecutar:\n\nstr(bank)\n\ntibble [11,162 × 17] (S3: tbl_df/tbl/data.frame)\n $ age      : int [1:11162] 59 56 41 55 54 42 56 60 37 28 ...\n $ job      : chr [1:11162] \"admin.\" \"admin.\" \"technician\" \"services\" ...\n $ marital  : chr [1:11162] \"married\" \"married\" \"married\" \"married\" ...\n $ education: chr [1:11162] \"secondary\" \"secondary\" \"secondary\" \"secondary\" ...\n $ default  : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int [1:11162] 2343 45 1270 2476 184 0 830 545 1 5090 ...\n $ housing  : chr [1:11162] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ loan     : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ contact  : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ day      : int [1:11162] 5 5 5 5 5 5 6 6 6 6 ...\n $ month    : chr [1:11162] \"may\" \"may\" \"may\" \"may\" ...\n $ duration : int [1:11162] 1042 1467 1389 579 673 562 1201 1030 608 1297 ...\n $ campaign : int [1:11162] 1 1 1 1 2 2 1 1 1 3 ...\n $ pdays    : int [1:11162] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ previous : int [1:11162] 0 0 0 0 0 0 0 0 0 0 ...\n $ poutcome : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ deposit  : chr [1:11162] \"yes\" \"yes\" \"yes\" \"yes\" ...\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nIdentifica el tipo de cada variable y las unidades de medida.\n\n\n\n\n1.3.3 Escalas de medición\nUna escala de medición define cómo se cuantifican o categorizan las variables recogidas sobre un conjunto de datos, influyendo en el análisis estadístico aplicable.\n\nNominal: categorización sin orden inherente. Por ejemplo, el género, la nacionalidad o el tipo de sangre.\nOrdinal: categorización con un orden lógico. Por ejemplo, el nivel educativo, o una clasificación de hoteles.\nMétrica: medición de diferentes escalas:\n\nIntervalo: sin cero verdadero, por ejemplo la temperatura en Celsius.\nRazón: con cero verdadero, por ejemplo los ingresos o la distancia.\n\n\nEn estadística y análisis de datos, es muy común recurrir a conversión entre diferentes escalas de medición. Las más comunes son:\n\nFechas a categóricas: convertir fechas exactas en mes, día de la semana, etc.\nCuantitativas a cualitativas: crear clases o rangos a partir de datos numéricos. Por ejemplo convertir el nivel de ingresos en “bajo”, “medio” y “alto”.\nOrdinales como numéricas: pasar de un orden a unos valores numéricos puede ser peligroso. Esta transformación ha de ser empleada con precaución, especialmente cuando se trabaja con pocos datos (\\(&lt;100\\)). Ten en cuenta que combinar en índices puede ser más informativo.\nVariables calculadas: creación de nuevas variables a partir de las existentes es una práctica muy habitual en análisis de datos. Hay una gran cantidad de situaciones donde esta tarea será muy beneficiosa. Por ejemplo, se crea el Índice de Masa Corporal (IMC) a partir de peso y altura.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#población-y-muestra",
    "href": "intro.html#población-y-muestra",
    "title": "1  Introducción",
    "section": "1.4 Población y muestra",
    "text": "1.4 Población y muestra\nLa población es el conjunto completo de todos los elementos o individuos que se desean estudiar. Puede ser una colección de personas, objetos, eventos o cualquier unidad de observación que sea de interés en un estudio. Por ejemplo, la población podría ser todos los estudiantes de una universidad, todos los árboles en un bosque, o todos los productos fabricados en una planta.\nDado que es a menudo impráctico o imposible estudiar toda la población, se selecciona una muestra, que es un subconjunto de la población. El estudio de las observaciones de una población podría implicar destruir dichas observaciones (vida útil del componente). El coste del estudio de las características de las observaciones podría ser muy elevado (experimentos biológicos). A veces el tamaño de la población es tan elevado (very big data!) que es obligatorio utilizar métodos de muestreo. Eso sí, es importante que la muestra sea representativa para que las conclusiones que se extraen de ella sean válidas para la población completa.\n\n\n\n\n\n\nEjemplo. Sondeo Electoral de 1936\n\n\n\n\n\nEl caso del sondeo electoral de 1936, realizado por la revista “Literary Digest” en Estados Unidos, es un ejemplo clásico que ilustra la importancia de la representatividad de una muestra en la investigación estadística y en particular en la realización de encuestas.\nEn 1936, la revista “Literary Digest” llevó a cabo un sondeo para predecir el resultado de la elección presidencial entre el candidato demócrata Franklin D. Roosevelt y el candidato republicano Alf Landon. La revista enviaba millones de encuestas a sus lectores y a otras listas de individuos, como propietarios de automóviles y personas que aparecían en directorios telefónicos. En ese año, “Literary Digest” envió unos 10 millones de encuestas y recibió alrededor de 2.4 millones de respuestas, lo que constituía una muestra enorme para los estándares de la época.\nLa predicción del sondeo fue que Alf Landon ganaría la elección con un margen significativo. Sin embargo, el resultado real fue una victoria aplastante para Franklin D. Roosevelt, quien ganó con más del 60% del voto popular y el 98.5% del voto electoral. Este error de predicción se debió principalmente a problemas relacionados con la representatividad de la muestra.\nProblemas de Representatividad\n\nSesgo de selección: La muestra del sondeo no era representativa de la población general. La lista de encuestados estaba basada en suscriptores de la revista, propietarios de automóviles y personas que aparecían en directorios telefónicos. En la década de 1930, estos grupos eran, en su mayoría, personas de ingresos más altos y de tendencia republicana, que no representaban adecuadamente a la población estadounidense en general, que incluía una proporción significativa de votantes de ingresos más bajos y de tendencia demócrata.\nTasa de respuesta: Aunque el tamaño de la muestra era grande (2.4 millones de respuestas), la tasa de respuesta fue baja en relación al número de encuestas enviadas (10 millones). Esto puede introducir un sesgo adicional, ya que las personas que respondieron pueden no ser representativas de la población objetivo. Es posible que aquellos más inclinados a responder fueran también más inclinados a votar por Landon.\nMuestreo no aleatorio: El método de selección de la muestra no era aleatorio. Las listas utilizadas para enviar las encuestas no cubrían todas las demografías de manera equitativa. Un muestreo aleatorio hubiera asegurado que cada individuo de la población tuviera una probabilidad conocida y no nula de ser seleccionado, lo cual es crucial para la representatividad.\n\n\n\n\nLa probabilidad juega un papel crucial en el diseño del muestreo y en la inferencia estadística. En el contexto del muestreo, la probabilidad asegura que cada miembro de la población tiene una oportunidad conocida y, generalmente, igual de ser incluido en la muestra. Esto permite que las muestras sean representativas y que los resultados sean generalizables. La teoría de la probabilidad también se utiliza para modelar la incertidumbre y el comportamiento aleatorio inherente en los datos.\n\n\n\n\n\n\nAdvertencia\n\n\n\nProbablemente has estudiado estos conceptos en la asignatura de Probabilidad y Simulación.\n\n\nUna vez que se ha recolectado una muestra, la estadística descriptiva se utiliza para organizar, resumir y presentar los datos de manera comprensible. Las herramientas de estadística descriptiva incluyen:\n\nMedidas de tendencia central: como la media, mediana y moda, que resumen el centro de los datos.\nMedidas de dispersión: como el rango, la varianza y la desviación estándar, que describen la variabilidad de los datos.\nGráficos: como histogramas, gráficos de caja y gráficos de dispersión, que visualizan los datos.\n\nLa estadística descriptiva se centra en describir lo que los datos muestran, sin hacer inferencias o generalizaciones sobre la población.\nLa inferencia estadística utiliza los datos de la muestra para hacer estimaciones, predicciones y generalizaciones sobre la población completa. Esto incluye dos aspectos principales:\n\nEstimación: Utilizar los datos de la muestra para estimar parámetros de la población, como la media o la proporción. Las estimaciones pueden ser puntuales (un solo valor) o por intervalo (un rango de valores con un nivel de confianza asociado).\nContraste de hipótesis: Probar afirmaciones sobre la población utilizando los datos de la muestra. Esto implica formular una hipótesis nula y una hipótesis alternativa, y usar pruebas estadísticas para decidir cuál es más consistente con los datos observados.\n\nLa inferencia estadística se basa en la teoría de la probabilidad para evaluar la incertidumbre y la variabilidad en las estimaciones y pruebas.\nLa Figura 1.2 muestra la relación entre los conceptos de Población, Muestra, Inferencia Estadística, Probabilida y Estadística Descriptiva. Estos conceptos permiten a los estadísticos recolectar datos de manera eficiente, describir los datos recolectados y hacer inferencias significativas y precisas sobre la población a partir de la muestra.\n\n\n\n\n\n\nFigura 1.2: La esencia de la Estadística\n\n\n\nExiste un principio fundamental en el análisis de datos que podríamos simplificar así:\n\\[DATOS = MODELO + ERROR\\]\n\nLos datos representan la realidad (procesos de negocios, clientes, productos, actividades, fenómenos físicos, etc.) que se quiere comprender, predecir o mejorar.\nEl modelo es una representación simplificada de la realidad que proponemos para describirla e interpretarla más fácilmente.\nEl error refleja la diferencia entre nuestra representación simplificada de la realidad (el modelo) y los datos que relamente describen esa realidad de forma precisa.\n\nBuscaremos, especialmente cuando estudiemos Apendizaje Automático, encontrar el modelo que explique los datos minimizando al máximo el error.\n\n1.4.1 Muestreo\nEl muestreo estadístico es una técnica fundamental en la estadística que permite extraer conclusiones sobre una población basándose en el análisis de una parte más pequeña de dicha población, conocida como muestra. A continuación, presentamos algunas de las principales técnicas de muestreo estadístico. Cada una de estas técnicas tiene sus propias ventajas y limitaciones, y la elección de la técnica adecuada depende del objetivo del estudio, las características de la población y los recursos disponibles.\n\n\n\n\n\n\nMuestreo aleatorio simple\n\n\n\n\n\nEn el muestreo aleatorio simple, cada miembro de la población tiene la misma probabilidad de ser seleccionado. Diremos que la muestra es aletoria simple (m.a.s.) cuando empleamos este tipo de muestreo. Se suele realizar utilizando métodos aleatorios, como sorteo, tablas de números aleatorios o generadores de números aleatorios. Este método es simple y fácil de entender, pero puede no ser siempre el más eficiente, especialmente si la población es muy grande. Podemos realizar el muestro con o sin reemplazamiento. Diremos que un muestreo es con reemplazamiento cuando una observación poblacional puede ser elegida varias veces para formar parte de la muestra. La misma observación puede aparecer repetida. Habitualmente se recurre al muestro sin reemplazamiento, donde una observación de la población, una vez elegida para formar parte de la muestra, es eliminada de la población y no puede volver a ser elegida.\nLa siguiente sentencia de R elige \\(5\\) observaciones de la base de datos bank mediante un muestreo aleatorio simple.\n\nn=dim(bank)[1] # número de observaciones en la base de datos\nindices=sample(1:n,size=5,replace=FALSE)\nindices # indices de las observaciones elegidas\n\n[1]  6965   467  8381 10145  5805\n\nmuestra=bank[indices,] # muestra de observaciones \n\nEsta técnica será empleada en la asignatura de Aprendizaje Automático para crear las particiones de entrenamiento, test y validación.\n\n\n\n\n\n\n\n\n\nMuestreo sistemático\n\n\n\n\n\nEn el muestreo sistemático, se selecciona un punto de inicio al azar y luego se elige a cada n-ésimo individuo de la lista de la población. Por ejemplo, si se quiere una muestra del 10% de una población de 1000 individuos, se seleccionaría un punto de inicio al azar entre los primeros 10 individuos y luego se seleccionaría cada décimo individuo a partir de ese punto. Este método es más fácil de ejecutar que el muestreo aleatorio simple, pero puede introducir sesgos si hay un patrón en la lista de la población.\n\n\n\n\n\n\n\n\n\nMuestreo estratificado\n\n\n\n\n\nEl muestreo estratificado implica dividir la población en subgrupos o estratos homogéneos y luego tomar una muestra aleatoria de cada estrato. Los estratos se forman en base a una característica específica como la edad, el género, el nivel socioeconómico, etc. Este método asegura que todas las subpoblaciones importantes estén representadas en la muestra y puede proporcionar estimaciones más precisas que el muestreo aleatorio simple. En el Aprendizaje Automático la variable elegida será la variable respuesta, o variable objetivo. De este modo, cuando los datos están desequilibrados, es decir, cuando tenemos más observaciones de una clase que de otra en los valores de la variable respuesta, aseguramos que todas los grupos están igualmente representados en la muestra.\n\n\n\n\n\n\n\n\n\nMuestreo por conglomerados\n\n\n\n\n\nEn el muestreo por conglomerados, la población se divide en grupos o conglomerados, y se seleccionan algunos de estos conglomerados al azar. Luego, todos (o una muestra de) los individuos dentro de los conglomerados seleccionados se incluyen en la muestra. Este método es útil cuando es difícil o costoso crear una lista completa de la población, pero puede ser menos preciso si los conglomerados no son homogéneos.\n\n\n\n\n\n\n\n\n\nMuestreo por cuotas\n\n\n\n\n\nEl muestreo por cuotas es un tipo de muestreo no probabilístico en el que se selecciona una muestra que cumple con ciertas cuotas preestablecidas basadas en características específicas. Por ejemplo, se puede querer que la muestra tenga un cierto número de individuos de diferentes edades o géneros. Aunque este método es práctico y rápido, no permite estimaciones estadísticas precisas de la población porque no todos los individuos tienen la misma probabilidad de ser seleccionados.\n\n\n\n\n\n\n\n\n\nMuestreo de bola de nieve\n\n\n\n\n\nEl muestreo de bola de nieve es otra técnica no probabilística, utilizada principalmente cuando es difícil acceder a los miembros de la población. Se empieza con unos pocos individuos conocidos de la población, quienes a su vez refieren a otros individuos, y así sucesivamente. Este método es útil para estudios de poblaciones ocultas o difíciles de alcanzar, como personas sin hogar o usuarios de drogas, pero introduce un alto riesgo de sesgo. También es muy común utilizar este tipo de muestreo cuando se realizan encuestas en redes sociales puesto que, habitualmente, la población objetivo está oculta o es complejo acceder a ella.\n\n\n\n\n\n\n\n\n\nMuestreo intencional o dirigido\n\n\n\n\n\nEn el muestreo intencional o dirigido, se seleccionan individuos que cumplen con ciertos criterios específicos de la investigación. Es un método no probabilístico donde la selección se basa en el juicio del investigador. Es útil cuando se busca estudiar casos específicos, pero no permite hacer generalizaciones precisas sobre la población completa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#parámetros-y-estadísticos",
    "href": "intro.html#parámetros-y-estadísticos",
    "title": "1  Introducción",
    "section": "1.5 Parámetros y estadísticos",
    "text": "1.5 Parámetros y estadísticos\nLos parámetros de una población son (casi) siempre desconocidos. Son valores teóricos que se definen sobre la población y que son de interés para el investigador. Son sobre los que haremos inferencia. Normalmente se representan con letras griegas.\nDiremos que un estadístico (además de una persona que hace estadística) es una función definida sobre los datos de una muestra (valores de una o más variables). En cada muestra serán distintos, debido a la variabilidad inherente a la extracción de una muestra representativa de una población. Los estadísticos siguen una distribución en el muestro y se representa con letras latinas.\n\n\n\n\n\n\nFigura 1.3: Parámetros vs. Estadísticos\n\n\n\nEs decir, una vez obtenida una muestra, es necesario extraer información útil de la misma. Esto se hace a través de los estadísticos.\nDiremos que un estadístico \\(T= T(x_1,\\ldots,x_n)\\) es una función real de la muestra aleatoria \\((x_1,\\ldots,x_n)\\).\nUn estadístico es una variable aleatoria, y por lo tanto, tiene asociada una distribución. La ditribución de probabilidad correspondiente a un estadístico se denomina distribución muestral.\nPor ejemplo, sea \\((X_1,\\ldots,X_n)\\) una muestra aletoria de una población \\(X\\) con esperanza \\(\\mu\\) y vaianza \\(\\sigma^2\\). Consideremos los siguientes ejemplos de estadísticos:\n\nMedia Muestral (\\(\\bar{X}\\)): Utilizada para estimar la media poblacional. \\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\]\nVarianza Muestral (\\(V\\)): Utilizada para estimar la varianza poblacional. \\[\nV=\\frac{1}{n}\\sum_{i=1}^n(X_i-\\bar{X})^2\n\\]\nCuasivarianza muestral (\\(S^2\\)) \\[\nS^2=\\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2=\\frac{1}{n-1}\\left [ \\sum_{i=1}^nX_i^2-n\\bar{X}^2\\right]\n\\] Entonces se tienen las siguientes propiedades:\n\n\\[\nE(\\bar{X})=\\mu\n\\] \\[\nVar(\\bar{X})=\\frac{\\sigma^2}{n}\n\\]\n\\[\nE(V)=\\frac{n-1}{n}\\sigma^2\n\\] \\[\nE(S^2)=\\sigma^2\n\\]\n\n1.5.1 Uso de la muestra\nSupongamos que queremos estudiar una característica de cierta población. Esta característica se representa mediante una variable aleatoria \\(X\\) y el estudio se centra en su valor medio \\(E[X]\\). Para ello, se decide tomar una muestra y se obtiene un estadístico, la media muestral, que se utiliza para estimar el valor de \\(E[X]\\): \\[\n\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\n\\] Fíjate que este estadístico \\(\\bar{X}\\) es una variable aleatoria. Una vez tomada una muestra particular \\((x_1,\\ldots,x_n)\\) de la variable aleatoria \\(X\\) se obtiene un valor numérico particular para la variable aleatoria \\(\\bar{X}\\): \\[\n\\bar{x}=\\frac{1}{n}\\sum_{i=1}^nx_i\n\\]\n\n\n\n\n\n\nPara recordar\n\n\n\n\\[\\bar{X}\\neq\\bar{x}\\]\n\n\n\n\n\n\n\n\nEjemplo. Diferencia entre parámetro y estadístico\n\n\n\n\n\nSe quiere estudiar la temperatura de una solución líquida en un laboratorio y estimar el valor medio \\(\\mu\\) (parámetro de la población), que es desconocido. Se supone que la temperatura se puede aproximar mediante una variable aleatoria de la que se desconoce su distribución. Una opción razonable para estimarla sería escoger una muestra de la solución, medir su temperatura, y estimar \\(\\mu\\) mediante el promedio de esa muestra (estadístico muestral): \\[ \\hat{\\mu}=\\bar{X}\\]. Es importante señalar que podríamos emplear otros estadísticos diferentes.\nSupongamos ahora que se toma una muestra de 10 mediciones de temperatura de dicha solución, obteniendo estos valores:\n\n\n [1] 45.76 42.75 45.54 49.61 53.00 44.15 51.48 48.69 51.33 49.21\n\n\nSiendo su valor promedio \\(\\bar{x}=\\) 48.15. Este valor estima el verdadero valor desconocido del parámetro \\(\\mu\\).\nAhora, supongamos que se toma otra muestra de 5 mediciones de temperatura de dicha solución, obteniendo estos valores:\n\n\n[1] 50.18 43.89 46.18 47.74 46.59\n\n\nLa media de esta otra muestra es \\(\\bar{x}=\\) 46.92 otra estimación del desconocido valor del parámetro \\(\\mu\\).\n¿Cuál de las dos estimaciones te parece más fiable? ¿Por qué? ¿Cómo podríamos “asegurar” que nuestro estimador es altamente fiable?\nRepetimos el primer experimento \\(100\\) días consecutivos. Es decir, tomamos \\(10\\) muestras de tamaño 10 y estimamos, para cada muestra el valor del parámetro \\(\\mu\\) mediante la media muestral. El siguiente gráfico muestra los valores de esas \\(100\\) muestras:\n\n\n\n\n\n\n\n\n\n¿Cuál es, a tu juicio, un buen valor del estimador del parámetro? Supongamos que tu profesor ha simulado estos datos. Es decir, tu profesor conoce realmente el verdadero valor del parámetro. En ese caso, te propone el siguiente reto: Dame dos valores (uno bajo y otro alto) entre los cuales crees que está el verdadero valor que solo yo (el profesor) conozco. ¿Qué valores darías? Te propone un reto mayor: ¿Qué valores darías si quisieras estar seguro al \\(100\\%\\) de acertar? Es decir, que la probabilidad de fallo sea \\(0\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#estadística-paramétrica-y-no-paramétrica",
    "href": "intro.html#estadística-paramétrica-y-no-paramétrica",
    "title": "1  Introducción",
    "section": "1.6 Estadística paramétrica y no paramétrica",
    "text": "1.6 Estadística paramétrica y no paramétrica\nTal y como hemos visto hasta ahora, el objetivo general de la inferencia es obtener información acerca de la distribución de una variable aleatoria \\(X\\) mediante la observación de una muestra \\((X_1,\\ldots,X_n)\\). En este curso vamos a tratar con dos tipos de herramientas para realizar inferencia estadística: la estadística paramétrica y la no paramétrica. Veamos sus semejanzas y diferencias:\nEstadística Paramétrica\nLa estadística paramétrica se basa en la suposición de que los datos siguen una distribución conocida, como la distribución normal, binomial, Poisson, etc. Estas suposiciones (o hipótesis) deben ser comprobadas para dar validez a este tipo de pruebas. Los parámetros de estas distribuciones, como la media y la varianza, se utilizan para resumir la información de los datos y realizar inferencias.\n\n\n\n\n\n\nFamilias paramétricas\n\n\n\n\n\nSe tiene una variable aleatoria \\(X\\) cuya distribución se supone perteneciente a una cierta familia paramétrica {\\(f_\\theta\\)} donde \\(\\theta \\in \\Theta\\).\nLa distribución de \\(X\\) es conocida excepto por el valor del parámetro \\(\\theta\\), del cual lo único que se conoce es su rango de posibles valores, \\(\\Theta\\), denominado espacio paramétrico.\nEjemplos de familias paramétricas\n\n\\(X \\sim N(\\mu,\\sigma^2) \\rightarrow \\theta=(\\mu,\\sigma^2)\\)\n\\(X \\sim Bernoulli(p) \\rightarrow \\theta=p\\)\n\\(X \\sim Exp(\\lambda) \\rightarrow \\theta=\\lambda\\)\n\n\n\n\nPor tanto, el objeto de los métodos paramétricos es obtener información sobre el parámetro de interés mediante la obtención de muestras de la variable aleatoria.\nLos métodos paramétricos son más potentes que los no paramétricos (es decir, tienen una mayor probabilidad de detectar un efecto verdadero) si las suposiciones son correctas. Ejemplos de pruebas paramétricas incluyen la prueba t de Student, el análisis de varianza (ANOVA), que estudiaremos en el Capítulo 5 y la regresión lineal que veréis en el segundo cuatrimestre.\nEstadística no paramétrica\nLa estadística no paramétrica no hace suposiciones fuertes sobre la distribución de los datos. Estos métodos son más flexibles y robustos a las violaciones de las suposiciones, pero pueden ser menos potentes si las suposiciones de los métodos paramétricos son verdaderas. Los métodos no paramétricos a menudo se basan en el orden de los datos, en lugar de sus valores exactos. Ejemplos de pruebas no paramétricas incluyen la prueba de Mann-Whitney U, la prueba de Kruskal-Wallis y la prueba de Chi-cuadrado. Tendremos un capítulo (Capítulo 4), dedicado a este tipo de herramientas.\nLa elección entre métodos paramétricos y no paramétricos depende de la naturaleza de tus datos y de las suposiciones que estés dispuesto a hacer. Si tus datos cumplen con las suposiciones de una prueba paramétrica, esa prueba puede ser la opción más potente. Si no, una prueba no paramétrica puede ser más apropiada.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#frecuentistas-vs-bayesianos",
    "href": "intro.html#frecuentistas-vs-bayesianos",
    "title": "1  Introducción",
    "section": "1.7 Frecuentistas vs Bayesianos",
    "text": "1.7 Frecuentistas vs Bayesianos\nEn el campo de la estadística existen dos enfoques diferentes que han de ser comentados, la inferencia clásica (o frecuentista) y la inferencia Bayesiana.\nEnfoque Frecuentista\nLos frecuentistas interpretan la probabilidad como la frecuencia relativa de un evento en un número infinito de repeticiones del experimento. Se obtienen datos a través de una muestra y con técnicas estadísticas se extrae información de los mismos mediante, los llamados, estimadores. En base a esas estimaciones se toman decisiones en el dominio de aplicación.\nLos métodos frecuentistas son ampliamente utilizados y son la base de muchas técnicas estadísticas clásicas. Los parámetros son considerados como valores fijos y desconocidos que se estiman a partir de los datos. Los intervalos de confianza, que veremos más adelante en el Capítulo 3, se interpretan en términos de repetibilidad: si se repite el experimento muchas veces, el intervalo de confianza capturará el verdadero parámetro en un porcentaje dado de las repeticiones. Esta interpretación es un poco extraña para el no iniciado y trataremos de explicarlo en detalle en capítulos posteriores.\nEnfoque Bayesiano\nLa inferencia Bayesiana tiene su fundamento en el teorema de Bayes. El teorema de Bayes, también conocido como regla de Bayes, es un principio fundamental en la teoría de la probabilidad que describe la forma de actualizar las probabilidades de una hipótesis basándose en nueva evidencia o información. Fue formulado por el matemático británico Thomas Bayes en el siglo XVIII. Es posible que hayas estudiado este teorema en asignaturas anteriores del grado. En cualquier caso, es sencillo y dice así:\n\\[\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\]\ndonde:\n\n\\(P(A|B)\\) es la probabilidad de que ocurra el evento \\(A\\) dado que ha ocurrido el evento \\(B\\). Esta es conocida como la probabilidad a posteriori.\n\\(P(B|A)\\) es la probabilidad de que ocurra el evento \\(B\\) dado que ha ocurrido el evento \\(A\\). Esta es conocida como la probabilidad verosímil o likelihood.\n\\(P(A)\\) es la probabilidad de que ocurra el evento \\(A\\) sin ninguna información adicional sobre \\(B\\). Esta es conocida como la probabilidad a priori o simplemente la probabilidad previa.\n\\(P(B)\\) es la probabilidad de que ocurra el evento \\(B\\) bajo todas las posibles hipótesis. Esta es conocida como la probabilidad marginal de \\(B\\).\n\nVemos que el teorema de Bayes permite actualizar la probabilidad de una hipótesis \\(A\\) a la luz de nueva evidencia \\(B\\). Básicamente, proporciona una forma de ajustar nuestras creencias iniciales (probabilidad a priori) en base a la nueva información disponible (evidencia).\n\n\n\n\n\n\nEjemplo. Teorema de Bayes.\n\n\n\n\n\nImaginemos que estamos tratando de diagnosticar una enfermedad rara que afecta al \\(1\\%\\) de la población (es decir, \\(P(\\text{Enfermedad}) = 0.01\\)). Ademas, se sabe que existe una prueba para esta enfermedad que es \\(99\\%\\) precisa:\n\nSi una persona tiene la enfermedad, la prueba es positiva el \\(99\\%\\) de las veces, es decir \\(P(\\text{Positivo}|\\text{Enfermedad}) = 0.99\\).\nSi una persona no tiene la enfermedad, la prueba es negativa el \\(99\\%\\) de las veces \\(P(\\text{Negativo}|\\text{No Enfermedad}) = 0.99\\), lo que significa que tiene un \\(1\\%\\) de falsos positivos \\(P(\\text{Positivo}|\\text{No Enfermedad}) = 0.01\\).\n\nEn este caso, deseamos saber cuál es la probabilidad de que una persona tenga la enfermedad si la prueba ha sido positiva \\(P(\\text{Enfermedad}|\\text{Positivo})\\).\nAplicamos el teorema de Bayes:\n\\[\nP(\\text{Enfermedad}|\\text{Positivo}) = \\frac{P(\\text{Positivo}|\\text{Enfermedad}) \\cdot P(\\text{Enfermedad})}{P(\\text{Positivo})}\n\\]\nPrimero, calculamos \\(P(\\text{Positivo})\\), la probabilidad total de que la prueba sea positiva:\n\\[\nP(\\text{Positivo}) = P(\\text{Positivo}|\\text{Enfermedad}) \\cdot P(\\text{Enfermedad}) +\n\\] \\[P(\\text{Positivo}|\\text{No Enfermedad}) \\cdot P(\\text{No Enfermedad})\n\\]\n\\[\nP(\\text{Positivo}) = (0.99 \\times 0.01) + (0.01 \\times 0.99) = 0.0099 + 0.0099 = 0.0198\n\\]\nAhora, aplicamos el teorema de Bayes:\n\\[\nP(\\text{Enfermedad}|\\text{Positivo}) = \\frac{0.99 \\times 0.01}{0.0198} = \\frac{0.0099}{0.0198} \\approx 0.50\n\\]\nEsto significa que, a pesar de que la prueba es bastante precisa, si una persona da positivo en la prueba, la probabilidad de que realmente tenga la enfermedad es aproximadamente \\(50\\%\\), debido a la baja prevalencia de la enfermedad en la población.\n\n\n\nEl teorema de Bayes es una herramienta poderosa para la toma de decisiones y la inferencia estadística, ya que proporciona un marco formal para actualizar nuestras creencias en base a la evidencia disponible.\nLos bayesianos interpretan la probabilidad como una medida de la creencia o confianza en un evento. Esta creencia puede ser actualizada a medida que se obtiene más información. Los parámetros son considerados como variables aleatorias y se describe su incertidumbre a través de distribuciones de probabilidad. Los intervalos de credibilidad bayesianos proporcionan una medida directa de la incertidumbre del parámetro: hay una probabilidad dada de que el verdadero parámetro esté dentro del intervalo de credibilidad. Esto parece tener más lógica que el enfoque frecuentista, pero es menos habitual. Los métodos bayesianos permiten la incorporación directa de conocimientos previos en el análisis a través de la distribución a priori.\nPor tanto, la principal diferencia entre los enfoques frecuentista y bayesiano radica en cómo interpretan el concepto de probabilidad. El enfoque frecuentista se basa en frecuencias de eventos, mientras que el enfoque bayesiano se basa en la incertidumbre y la actualización de las creencias.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#notación",
    "href": "intro.html#notación",
    "title": "1  Introducción",
    "section": "1.8 Notación",
    "text": "1.8 Notación\nA lo largo de este libro vamos a usar la siguiente notación:\n\n\\(X, Y\\): Variables\n\\(i\\): Identificador o índice para cada observación o clase\n\\(x_i\\): Valor que toma la variable \\(X\\) en la observación \\(i\\)\n\\(c_i\\): Marca de clase en datos agrupados\n\\(n\\): Número total de observaciones\n\\(k\\): Número de clases\n\\(n_i\\): Número de observaciones en la clase \\(i\\)\n\\(\\bar{x}\\): Media muestral de la variable \\(X\\)\n\\(s^2\\): Varianza muestral de la variable \\(X\\)\n\\(s\\): Desviación típica muestral de la variable \\(X\\)\n\\(\\mu\\): Media poblacional\n\\(\\sigma^2\\): Varianza poblacional\n\\(\\hat{[\\cdot]}\\): Simboliza un estimador de \\(\\cdot\\). Por ejemplo, \\(s = \\hat{\\sigma}\\) quiere decir que la desviación típica muestral \\(s\\) es un estimador de la desviación típica poblacional \\(\\sigma\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "intro.html#resúmenes-gráficas-y-numéricas-útiles-en-la-inferencia-estadística",
    "href": "intro.html#resúmenes-gráficas-y-numéricas-útiles-en-la-inferencia-estadística",
    "title": "1  Introducción",
    "section": "1.9 Resúmenes gráficas y numéricas útiles en la inferencia estadística",
    "text": "1.9 Resúmenes gráficas y numéricas útiles en la inferencia estadística\n\n1.9.1 Métodos numéricos\nTal y como hemos indicado anteriormente, los métodos numéricos de inferencia estadística son técnicas y procedimientos utilizados para analizar datos y hacer inferencias sobre una población a partir de una muestra. Estos métodos se apoyan en herramientas matemáticas y computacionales para estimar parámetros, evaluar hipótesis y tomar decisiones informadas. A continuación, se describen los principales métodos numéricos en la inferencia estadística. No tienes que aprenderlos ahora puesto que vamos a trabajar con estos métodos a lo largo de todo el curso. Los veremos con mayor detalle en los próximos capítulos.\n\n\n\n\n\n\nEstimación puntual\n\n\n\n\n\nLa estimación puntual implica el uso de un solo valor estadístico de la muestra para estimar un parámetro de la población.\n\nMedia Muestral (\\(\\bar{x}\\)): Utilizada para estimar la media poblacional (\\(\\mu\\)).\nProporción Muestral (\\(\\hat{p}\\)): Utilizada para estimar la proporción poblacional (\\(p\\)).\nVarianza Muestral (\\(s^2\\)): Utilizada para estimar la varianza poblacional (\\(\\sigma^2\\)).\n\n\n\n\n\n\n\n\n\n\nEstimación por intervalo\n\n\n\n\n\nLa estimación por intervalo proporciona un rango de valores dentro del cual se espera que se encuentre el parámetro poblacional con un cierto nivel de confianza.\n\nIntervalo de confianza para la media:\n\nPara muestras grandes (\\(n \\ge 30\\)): \\(\\bar{x} \\pm Z_{\\alpha/2} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right )\\)\n\nPara muestras pequeñas (\\(n &lt; 30\\)) y cuando la distribución es normal: \\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\left(\\frac{s}{\\sqrt{n}}\\right)\\)\n\nIntervalo de confianza para la proporción: \\(\\hat{p} \\pm Z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\)\nIntervalo de confianza para la varianza: \\(\\left( \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2, n-1}} \\right)\\)\n\n\n\n\n\n\n\n\n\n\nContraste de hipótesis\n\n\n\n\n\nEl contraste de hipótesis es un procedimiento para tomar decisiones sobre los parámetros poblacionales basándose en la evidencia proporcionada por los datos muestrales.\n\nFormulación de hipótesis:\n\nHipótesis nula (\\(H_0\\)): Es la afirmación que se desea probar o refutar.\nHipótesis alternativa (\\(H_1\\)): Es la afirmación que se acepta si se rechaza (\\(H_0\\)).\n\nPruebas para la media:\n\nPrueba Z: Utilizada para muestras grandes (\\(n \\ge 30\\)) o cuando se conoce la desviación estándar poblacional (\\(\\sigma\\)).\nPrueba t: Utilizada para muestras pequeñas (\\(n &lt; 30\\)) y cuando no se conoce (\\(\\sigma\\)).\n\nPruebas para la proporción:\n\nPrueba Z para proporciones: Utilizada para evaluar hipótesis sobre una proporción poblacional.\n\nPruebas para la Varianza:\n\nPrueba Chi-cuadrado: Utilizada para evaluar hipótesis sobre la varianza poblacional.\n\n\n\n\n\n\n\n\n\n\n\nMétodos de resampling\n\n\n\n\n\nLos métodos de resampling, como el bootstrap y el jackknife, son técnicas computacionales utilizadas para estimar la precisión de los estadísticos de la muestra.\n\nBootstrap: Consiste en tomar múltiples muestras con reemplazo de los datos originales y calcular el estadístico de interés para cada muestra. Esto permite construir una distribución empírica del estadístico y estimar intervalos de confianza.\nJackknife: Involucra excluir sistemáticamente cada observación de la muestra y recalcular el estadístico de interés. Esto proporciona una manera de estimar el sesgo y la varianza del estimador.\n\n\n\n\n\n\n\n\n\n\nMétodos bayesianos\n\n\n\n\n\nLa inferencia bayesiana utiliza la probabilidad subjetiva para actualizar la creencia sobre los parámetros poblacionales basándose en la evidencia muestral.\n\nTeorema de Bayes: \\(P( \\theta |x) = \\frac{P(x | \\theta)P( \\theta)}{P(x)}\\), donde \\(P(\\theta|x)\\) es la distribución a posteriori, \\(P(x|\\theta)\\) es la verosimilitud, \\(P(\\theta)\\) es la distribución a priori y \\(P(x)\\) es la probabilidad marginal de los datos.\nSimulación Monte Carlo Markov Chain (MCMC): Es un método numérico para aproximar distribuciones posteriores complejas.\n\n\n\n\n\n\n\n\n\n\nPruebas no paramétricas\n\n\n\n\n\nCuando no se cumplen los supuestos de normalidad, se utilizan pruebas no paramétricas que no requieren asumir una distribución específica.\n\nPrueba de Wilcoxon: Para comparar medianas entre dos muestras emparejadas.\nPrueba de Mann-Whitney: Para comparar medianas entre dos muestras independientes.\nPrueba de Kruskal-Wallis: Extensión de la prueba de Mann-Whitney para más de dos grupos.\nPrueba de Chi-cuadrado: Para pruebas de independencia y homogeneidad en tablas de contingencia.\n\n\n\n\n\n\n1.9.2 Métodos gráficos\nA lo largo del curso usaremos numerosos métodos gráficos para explorar los datos dentro del contexto de la inferencia estadística. Algunas de los métodos básicos son:\n\n\n\n\n\n\nHistogramas\n\n\n\n\n\nGráficos de barras que muestran la distribución de frecuencias de datos cuantitativos.\n\n\n\n\n\n\n\n\n\nGráficos de caja (boxplots)\n\n\n\n\n\nMuestran la mediana, los cuartiles y los posibles valores atípicos.\n\n\n\n\n\n\n\n\n\nGráficos de dispersión (scatterplots)\n\n\n\n\n\nUtilizados para observar la relación entre dos variables cuantitativas."
  },
  {
    "objectID": "intro.html#teorema-central-del-límite",
    "href": "intro.html#teorema-central-del-límite",
    "title": "1  Introducción",
    "section": "1.10 Teorema Central del Límite",
    "text": "1.10 Teorema Central del Límite\nEl Teorema Central del Límite (TCL) es uno de los principios más fundamentales en la estadística y la probabilidad. Establece que, bajo ciertas condiciones, la distribución de la suma (o el promedio) de un gran número de variables aleatorias independientes e identicamente distribuidas tiende a seguir una distribución Normal, independientemente de la distribución original de las variables.\nFormalmente, el TCL establece que si, \\(X_1,X_2,\\ldots,X_n\\) son variables aleatorias independientes e idénticamente distribuidas, con media \\(\\mu\\) y varianza \\(\\sigma^2&lt;\\infty\\), entonces, para \\(n\\) suficientemente grande se verifica:\n\\[\\bar{X} \\approx N \\left ( \\mu,\\frac{\\sigma^2}{n}\\right )\\] Este resultado es válido tanto para variables discretas como continuas, sean simétricas o asimétricas, unimodales o multimodales.\n\n\n\n\n\n\nPara recordar\n\n\n\nEl Teorema Central del Límite asegura que con muestas suficientemente grandes se pueden utilizar estimaciones basadas en la distribución Normal independientemente del tipo de distribución que siga la variable que nos interesa.\n\n\n\n\n\n\n\n\nEjemplo. Teorema Central del Límite\n\n\n\n\n\nAcabamos de ver que el TCL establece que, para una gran cantidad de muestras aleatorias tomadas de una población con una distribución cualquiera (con una media y una varianza finitas), la distribución de las medias muestrales tiende a ser Normal, independientemente de la forma de la distribución original.\nAquí tienes un ejemplo en R que ilustra el Teorema Central del Límite. Vamos a tomar muestras de una distribución no normal (por ejemplo, una distribución uniforme). Calcularemos las medias de estas muestras y observaremos cómo las medias muestrales se aproximan a una distribución normal.\n\n# Configuraciones iniciales\nset.seed(123)          # Fijamos la semilla para reproducibilidad\nn_samples &lt;- 1000      # Número de muestras\nsample_size &lt;- 30      # Tamaño de cada muestra\n\n# Generamos las muestras de una distribución uniforme\nsample_means &lt;- numeric(n_samples)\nfor (i in 1:n_samples) {\n  sample &lt;- runif(sample_size, min = 0, max = 10)\n  sample_means[i] &lt;- mean(sample)\n}\n\n# Crear un data frame con las medias muestrales\ndata &lt;- data.frame(sample_means)\n\n# Graficamos el histograma de las medias muestrales utilizando ggplot2\nggplot(data, aes(x = sample_means)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  stat_function(fun = dnorm, args = list(mean = mean(sample_means), sd = sd(sample_means)),\n                color = \"red\", size = 1) +\n  labs(title = \"Distribución de las Medias Muestrales\",\n       x = \"Medias Muestrales\", y = \"Densidad\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = mean(sample_means), y = max(density(sample_means)$y) / 2,\n           label = \"Distribución Normal\", color = \"red\")\n\n\n\n\n\n\n\n\nPara visualizar el resultado, graficamos un histograma de las medias muestrales. Luego, superponemos una curva de densidad de una distribución normal con la misma media y desviación estándar que las medias muestrales para observar cómo se ajusta a una distribución normal.\nEl histograma de las medias muestrales se aproxima a una distribución normal, a pesar de que las muestras originales provienen de una distribución uniforme.\n\n\n\n\n\n\n\nHao, Jiangang, y Tin Kam Ho. 2019. «Machine learning made easy: a review of scikit-learn package in python programming language». Journal of Educational and Behavioral Statistics 44 (3): 348-61.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, y Jerome H Friedman. 2009. The elements of statistical learning: data mining, inference, and prediction. Vol. 2. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An introduction to statistical learning. Vol. 112. Springer.\n\n\nKelleher, John D, Brian Mac Namee, y Aoife D’arcy. 2020. Fundamentals of machine learning for predictive data analytics: algorithms, worked examples, and case studies. MIT press.\n\n\nOsmani, Addy. 2012. Learning JavaScript Design Patterns: A JavaScript and jQuery Developer’s Guide. \" O’Reilly Media, Inc.\".\n\n\nOualline, Steve. 2003. Practical C++ programming. \" O’Reilly Media, Inc.\".\n\n\nWirth, Rüdiger, y Jochen Hipp. 2000. «CRISP-DM: Towards a standard process model for data mining». En Proceedings of the 4th international conference on the practical applications of knowledge discovery and data mining, 1:29-39. Manchester.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "eda.html#preguntas",
    "href": "eda.html#preguntas",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "",
    "text": "John Tukey\n\n\n\nMucho mejor una respuesta aproximada a la pregunta correcta, que a menudo es vaga, que una respuesta exacta a la pregunta incorrecta, que siempre se puede precisar.\n\n\n\n\n\n\n\n\n\nPara recordar\n\n\n\nDurante la preparación y limpieza de los datos acumulamos pistas sobre los modelos de aprendizaje más adecuados que podrán ser aplicados en etapas posteriores.\n\n\n\n\n¿Cuál es el tamaño de la base de datos? Es decir:\n\n¿Cuántas observaciones hay?\n¿Cuántas variables/características están medidas?\n¿Disponemos de capacidad de cómputo en nuestra máquina para procesar la base de datos o necesitamos más recursos?\n¿Existen valores faltantes?\n\n¿Qué tipo variables aparecen en la base de datos?\n\n¿Qué variables son discretas?\n¿Cuáles son continuas?\n¿Qué categorías tienen las variables?\n¿Hay variables tipo texto?\n\nVariable objetivo: ¿Existe una variable de “respuesta”?\n\n¿Binaria o multiclase?\n\n¿Es posible identificar variables irrelevantes?. Estudiar variables relevantes requiere, habitualmente, métodos estadísticos.\n¿Es posible identificar la distribución que siguen las variables?\nCalcular estadísticos resumen (media, desviación típica, frecuencia,…) de todas las variables de interés. Estudiaremos las propiedades de estos estimadores en el próximo capítulo.\nDetección y tratamiento de valores atípicos.\n\n¿Son errores de media?\n¿Podemos eliminarlos?\n\n¿Existe correlación entre variables?\n\n\n\n\n\n\n\nPara recordar\n\n\n\nUna correcta preparación y limpieza de datos implica, sin duda, un ahorro de tiempo en etapas posteriores del proyecto.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#entender-el-negocio",
    "href": "eda.html#entender-el-negocio",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.2 Entender el negocio",
    "text": "2.2 Entender el negocio\nLa comprensión del problema que estamos abordando representa una de las primeras etapas en cualquier proyecto de ciencia de datos. En la mayoría de los casos, esta tarea se realiza en estrecha colaboración con expertos en el dominio correspondiente, quienes a menudo son las personas que han solicitado (y a menudo financian) el análisis de datos. Es importante recordar que cualquier estudio que involucre ciencia de datos requiere un conocimiento profundo del dominio, el cual debe ser compartido con el científico de datos. Por lo tanto, el profesional de la ciencia de datos debe poseer un conocimiento suficiente para enfrentar con confianza los diversos desafíos que puedan surgir. Esta comprensión inicial permite establecer los objetivos del proyecto y procesar los datos de manera correcta para obtener información valiosa. A través de esta información, se busca derivar conocimientos aplicables. Este conocimiento puede ser aprendido y almacenado para su uso futuro, lo que lleva a la sabiduría, según la jerarquía de conocimiento presentada en la Figura Figura 2.1.\n\n\n\n\n\n\nFigura 2.1: Jerarquía de Conocimiento\n\n\n\n\n\n\n\n\n\nClaude Lévi-Strauss\n\n\n\n“El científico no es una persona que da las respuestas correctas, sino una persona que hace las preguntas correctas.”",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#un-primer-vistazo-a-los-datos",
    "href": "eda.html#un-primer-vistazo-a-los-datos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.3 Un primer vistazo a los datos",
    "text": "2.3 Un primer vistazo a los datos\nEn este capítulo vamos a trabajar con los datos de Bank Marketing del repositorio UCI. En primer lugar debemos comprender el problema. ¿Qué sabes del marketing bancario? En el caso que nos ocupa, los datos están relacionados con campañas de marketing directo (llamadas telefónicas) de una entidad bancaria portuguesa. El objetivo final, que abordaremos en cursos posteriores, es predecir si el cliente suscribirá un depósito a plazo (variable objetivo). En este curso nos conformamos con adquirir conocimiento de los datos, planteando algunas hipótesis de interés.\nLas variables que debemos estudiar son:\nVariables de entrada:\n\nDatos del cliente bancario:\n\n\nedad (variable numérica)\nempleo : tipo de empleo (variable categórica con las siguientes categorías: “admin.”, “desconocido”, “desempleado”, “directivo”, “empleada del hogar”, “empresario”, “estudiante”, “obrero”, “autónomo”, “jubilado”, “técnico”, “servicios”)\nestado civil : estado civil (variable categórica con categorías: “casado”, “divorciado”, “soltero”; nota: “divorciado” significa divorciado o viudo)\neducación (variable categórica con categorías: “desconocida”, “secundaria”, “primaria”, “terciaria”)\nimpago: ¿tiene un crédito impagado? (variable binaria con dos posibles valores: “sí”, “no”)\nsaldo: saldo medio anual, en euros (variable numérica)\nvivienda: ¿tiene préstamo para vivienda? (variable binaria: “sí”, “no”)\npréstamo: ¿tiene préstamo personal? (variable binaria: “sí”, “no”)\n# relacionado con el último contacto de la campaña actual:\ncontacto: tipo de comunicación del contacto (variable categórica: “desconocido”, “teléfono”, “móvil”)\ndía: día del mes del último contacto (variable numérica)\nmes: mes del año del último contacto (variable categórica: “ene”, “feb”, “mar”, …, “nov”, “dic”)\nduración: duración del último contacto, en segundos (variable numérica)\n\n\nOtros atributos\n\n\ncampaña: número de contactos realizados durante esta campaña y para este cliente (variable numérica, incluye el último contacto)\npdays: número de días transcurridos desde que el cliente fue contactado por última vez en una campaña anterior (variable numérica, -1 significa que el cliente no fue contactado previamente)\nprevious: número de contactos realizados antes de esta campaña y para este cliente (variable numérica)\npoutcome: resultado de la campaña de marketing anterior (variable categórica: “desconocido”, “otro”, “fracaso”, “éxito”)\n\n\nVariable de salida (objetivo deseado):\n\n17 - y: ¿ha suscrito el cliente un depósito a plazo? (variable binaria: “sí”, “no”)\n\n\n\n\n\n\nPara recordar\n\n\n\nA veces (muchas veces) la descripción que encontramos en una primera etapa no coincide al completo con los datos que luego nos entrega el cliente. Los datos suelen ser más complejos que la teoría detrás de los datos.\nEn otras ocasiones no se dispone de la descripción de las variables. En ese caso, ¡hay que hacer lo imposible por conseguirla! Si no conocemos el significado de una variable, difícilmente podremos interpretar los resultados asociados a ella.\n\n\nLeemos los datos con R.\n\nlibrary(tidyverse)\nbank = read.csv('https://raw.githubusercontent.com/rafiag/DTI2020/main/data/bank.csv')\ndim(bank)\n\n[1] 11162    17\n\nbank=as.tibble(bank)\nbank\n\n# A tibble: 11,162 × 17\n     age job       marital education default balance housing loan  contact   day\n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;int&gt;\n 1    59 admin.    married secondary no         2343 yes     no    unknown     5\n 2    56 admin.    married secondary no           45 no      no    unknown     5\n 3    41 technici… married secondary no         1270 yes     no    unknown     5\n 4    55 services  married secondary no         2476 yes     no    unknown     5\n 5    54 admin.    married tertiary  no          184 no      no    unknown     5\n 6    42 manageme… single  tertiary  no            0 yes     yes   unknown     5\n 7    56 manageme… married tertiary  no          830 yes     yes   unknown     6\n 8    60 retired   divorc… secondary no          545 yes     no    unknown     6\n 9    37 technici… married secondary no            1 yes     no    unknown     6\n10    28 services  single  secondary no         5090 yes     no    unknown     6\n# ℹ 11,152 more rows\n# ℹ 7 more variables: month &lt;chr&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;,\n#   previous &lt;int&gt;, poutcome &lt;chr&gt;, deposit &lt;chr&gt;\n\n\nDisponemos de más de 10000 observaciones y un total de 17 variables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#tipo-de-variables",
    "href": "eda.html#tipo-de-variables",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.4 Tipo de variables",
    "text": "2.4 Tipo de variables\nPara averiguar qué tipo de variables manejamos, ejecutar:\n\nstr(bank)\n\ntibble [11,162 × 17] (S3: tbl_df/tbl/data.frame)\n $ age      : int [1:11162] 59 56 41 55 54 42 56 60 37 28 ...\n $ job      : chr [1:11162] \"admin.\" \"admin.\" \"technician\" \"services\" ...\n $ marital  : chr [1:11162] \"married\" \"married\" \"married\" \"married\" ...\n $ education: chr [1:11162] \"secondary\" \"secondary\" \"secondary\" \"secondary\" ...\n $ default  : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ balance  : int [1:11162] 2343 45 1270 2476 184 0 830 545 1 5090 ...\n $ housing  : chr [1:11162] \"yes\" \"no\" \"yes\" \"yes\" ...\n $ loan     : chr [1:11162] \"no\" \"no\" \"no\" \"no\" ...\n $ contact  : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ day      : int [1:11162] 5 5 5 5 5 5 6 6 6 6 ...\n $ month    : chr [1:11162] \"may\" \"may\" \"may\" \"may\" ...\n $ duration : int [1:11162] 1042 1467 1389 579 673 562 1201 1030 608 1297 ...\n $ campaign : int [1:11162] 1 1 1 1 2 2 1 1 1 3 ...\n $ pdays    : int [1:11162] -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...\n $ previous : int [1:11162] 0 0 0 0 0 0 0 0 0 0 ...\n $ poutcome : chr [1:11162] \"unknown\" \"unknown\" \"unknown\" \"unknown\" ...\n $ deposit  : chr [1:11162] \"yes\" \"yes\" \"yes\" \"yes\" ...\n\n\nCreamos las particiones sobre los datos y trabajamos sobre la partición de entrenamiento. Este paso cobrará significado en asignaturas posteriores, especialmente en las enfocadas en el aprendizaje automático (Machine Learning). Sin entrar en más detalles, podemos decir que la idea fundamental es estudiar las hipótesis en un subconjunto de la muestra disponible y evaluar la veracidad (o no) de dichas hipótesis en muestras diferentes.\n\n# Parciticionamos los datos\nset.seed(2138)\nn=dim(bank)[1]\nindices=seq(1:n)\nindices.train=sample(indices,size=n*.5,replace=FALSE)\nindices.test=sample(indices[-indices.train],size=n*.25,replace=FALSE)\nindices.valid=indices[-c(indices.train,indices.test)]\n\nbank.train=bank[indices.train,]\nbank.test=bank[indices.test,]\nbank.valid=bank[indices.valid,]\n\n\n\n\n\n\n\nAtrévete\n\n\n\n¿Te has hecho (ya) alguna pregunta sobre los datos? Si es así, no esperes más, busca la respuesta!\n\n\nPor ejemplo, ¿qué te parecen estas preguntas que nosotros proponemos?\n¿Qué día del año se producen más depósitos por parte de los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\") %&gt;% \n  count(month, day) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 3\n  month   day     n\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt;\n1 apr      30    79\n\n\n¿En qué mes del año realizan más depósitos los estudiantes?\n\n\nClick para ver el código\nbank.train %&gt;% \n  filter(deposit==\"yes\" & job==\"student\") %&gt;% \n  count(month) %&gt;% \n  top_n(1,n)\n\n\n# A tibble: 1 × 2\n  month     n\n  &lt;chr&gt; &lt;int&gt;\n1 apr      21\n\n\n¿Qué trabajo está asociado con el mayor porcentaje de depósitos?\n\n\nClick para ver el código\nbank.train %&gt;%\n  group_by(job) %&gt;%\n  mutate(d = n()) %&gt;%\n  group_by(job, deposit) %&gt;%\n  summarise(Perc = n()/first(d), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    id_cols = job,\n    names_from = deposit,\n    values_from = Perc\n  ) %&gt;%\n  top_n(1)\n\n\n# A tibble: 1 × 3\n  job        no   yes\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 student 0.218 0.782\n\n\n\n\n\n\n\n\nRepaso\n\n\n\ndplyr es un paquete en R diseñado para facilitar la manipulación y transformación de datos de manera eficiente y estructurada. Fue desarrollado por Hadley Wickham y se ha convertido en una de las herramientas más populares en la ciencia de datos y análisis de datos.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#variable-objetivo",
    "href": "eda.html#variable-objetivo",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.5 Variable objetivo",
    "text": "2.5 Variable objetivo\nEn el ámbito del Aprendizaje Automático, en problemas de clasificación (Aprendizaje Supervisado) existe una variable de interés fundamental, es la variable respuesta o variable objetivo. En el próximo cuatrimestre se trata con detalle este tipo de problemas. En el caso que nos ocupa dicha variable es la característica: “deposit”. Vamos a estudiar la información que nos proporciona dicha variable.\n\nlibrary(ggplot2)\ntable(bank.train$deposit)\n\n\n  no  yes \n2908 2673 \n\nggplot(data=bank.train,aes(x=deposit,fill=deposit)) +\n  geom_bar(aes(y=(..count..)/sum(..count..))) +\n  scale_y_continuous(labels=scales::percent) +\n  theme(legend.position=\"none\") +\n  ylab(\"Frecuencia relativa\") +\n  xlab(\"Variable respuesta: deposit\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#visualizar-distribuciones",
    "href": "eda.html#visualizar-distribuciones",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.6 Visualizar distribuciones",
    "text": "2.6 Visualizar distribuciones\nLa forma de visualizar la distribución de una variable dependerá de si la variable es categórica o continua. Una variable es categórica si sólo puede tomar uno de un pequeño conjunto de valores. En R, las variables categóricas suelen guardarse como factores o vectores de caracteres. Para examinar la distribución de una variable categórica, utiliza un gráfico de barras:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = contact))\n\n\n\n\n\n\n\n\nPuedes obtener los valores exactos en cada categoría como sigue:\n\nbank.train%&gt;% \n  count(contact)\n\n# A tibble: 3 × 2\n  contact       n\n  &lt;chr&gt;     &lt;int&gt;\n1 cellular   4025\n2 telephone   383\n3 unknown    1173\n\n\nUna variable es continua si puede tomar cualquiera de un conjunto infinito de valores ordenados. Para examinar la distribución de una variable continua, utiliza un histograma:\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = age), binwidth = 5)\n\n\n\n\n\n\n\n\nUn histograma divide el eje \\(x\\) en intervalos equidistantes y, a continuación, utiliza la altura de una barra para mostrar el número de observaciones que se encuentran en cada intervalo. En el gráfico anterior, la primera barra muestra unas \\(100\\) observaciones (realmente son \\(119\\)) tienen un valor de edad por debajo de \\(22.5\\) años. Puede establecer la anchura de los intervalos en un histograma con el argumento binwidth, que se mide en las unidades de la variable \\(x\\).\n\n\n\n\n\n\nPara recordar\n\n\n\nSiempre se deben explorar una variedad de anchos de intervalo cuando trabajamos con histogramas, ya que diferentes anchos de intervalo pueden revelar diferentes patrones.\n\n\nPodemos representar funciones de densidad de probabilidad.\n\nggplot(bank.train, aes(x = age)) +\ngeom_density() +\nggtitle('KDE de edad en datos bank')\n\n\n\n\n\n\n\n\nOtro gráfico muy utilizado para variables cuantitativas univariantes es el boxplot, también llamado box-and-whisker plot (diagrama de caja y bigotes). Es especialmente útil para detectar posibles datos atípicos en los valores de una variable, siempre que su distribución sea parecida a una distribución Normal. El gráfico muestra:\n\nUna caja cuyos límites son el primer y el tercel cuartil de la distribución de valores.\nUna línea central, que marca la mediana.\nLos bigotes, que por defecto (en R) se extienden hasta 1.5 veces el valor del rango intercuartílico (IQR) por encima y por debajo de la caja.\nPuntos individuales, que quedan más allá del límite de los bigotes, marcan posibles datos atípicos.\n\nEn distribuciones muy asimétricas o con muchos valores extremos, muy diferentes a una distribución Normal, aparecerán demasiados puntos más allá de los bigotes y no se podrán apreciar fácilmente los atípicos (demasiados puntos considerados como tales). En ese caso, es conveniente intentar una transformación de la variable antes de representar el boxplot.\n\nggplot(bank.train, aes(x=deposit, y=age, color=deposit)) +\n  geom_boxplot()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#transformación-de-variables",
    "href": "eda.html#transformación-de-variables",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.7 Transformación de variables",
    "text": "2.7 Transformación de variables\n\n2.7.1 Transformación de variables cuantitativas\nEn algunos métodos de inferencia estadística y aprendizaje automático será necesario contar con variables que cumplan requisitos de normalidad. Por ejemplo, si tomamos la transformación \\(log\\) sobre la variable edad obtenemos una distribución multimodal que, probablemente, corresponda a la combinación de dos (o más) normales.\n\nggplot(data = bank.train) +\n  geom_histogram(mapping = aes(x = log(age)), binwidth = .1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPara recordar\n\n\n\nLos modelos de aprendizaje serán tan buenos como lo sean las variables de entrada de dichos algoritmos.\n\n\n\n2.7.1.1 Transformaciones para igualar dispersión\nCon frecuencia, el objetivo de la transformación de variables cuantitativas es obtener una variable cuya distribución de valores sea:\n\nMás simétrica y con menor dispersión que la original.\nMás semejante a una distribución normal (e.g. para algunos modelos lineales).\nRestringida en un intervalo de valores (e.g. \\([0,1]\\) ).\n\nLa forma más sencilla de detectar que alguna de nuestras variables necesita ser transformada es representar un gráfico que muestre la distribución de valores de la variable. Por ejemplo, un histograma o un diagrama de densidad de probabilidad (o ambos).\nEl uso de los logaritmos tiene su propia recomendación en preparación de datos (Fox y Weisberg 2018):\n\n\n\n\n\n\nJohn Fox\n\n\n\n“Si la variable es estrictamente positiva, no tiene un límite superior para sus valores, y su rango abarca dos o más órdenes de magnitud (potencias de \\(10\\)), entonces la transformación logarítmica suele ser útil. A la inversa, cuando la variable tiene un rango de valores pequeño (menor de un orden de magnitud), el logaritmo o cualquier otra transformación simple no ayudará mucho.”\n\n\nLa versión general de esta transformación son las transformaciones de escala-potencia (scaled-power transformations), también denominadas transformaciones de Box-Cox.\n\\[x(\\lambda)= \\begin{cases} \\frac{x^\\lambda-1}{\\lambda},& \\text{cuando } \\lambda \\neq 0,\\\\ log_e(x), & \\text{cuando } \\lambda = 0 \\end{cases}\\]\nLa función car::symbox(...) permite probar varias combinaciones típicas del parámetro \\(\\lambda\\) , para comprobar con cuál de ellas obtenemos una distribución más simétrica de valores.\n\nlibrary(car)\n\nbank.train %&gt;% symbox(~ age, data = .)\n\n\n\n\n\n\n\n\n\n\n2.7.1.2 Transformaciones para igualar dispersión\nTambién es bastante común aplicar transformaciones en datos cuantitativos para igualar las escalas de representación de las variables. En muchos modelos, si una de nuestras variables tiene una escala mucho mayor que las demás, sus valores tienden a predominar en los resultados, enmascarando la influencia del resto de variables en el modelo.\nPor este motivo, en muchos modelos es importante garantizar que todas las variables se representan en escalas comparables, de forma que ninguna predomine sobre el resto. Conviene aclarar un poco algunos términos que se suelen emplear de forma indistinta:\n\nReescalado o cambio de escala: Consiste en sumar o restar una constante a un vector, y luego multiplicar o dividir por una constante. Por ejemplo, para transformar la unidad de medida de una variable (grados Farenheit → grados Celsius).\nNormalización: Consiste en dividir por la norma de un vector, por ejemplo para hacer su distancia euclídea igual a \\(1\\).\nEstandarización: Consiste en restar a un vector una medida de localización o nivel (e.g. media, mediana) y dividir por una medida de escala (dispersión). Por ejemplo, si restamos la media y dividimos por la desviación típica hacemos que la distribución tenga media \\(0\\) y desviación típica \\(1\\).\n\nAlgunas aternativas comunes son:\n\\[\nEstandarización \\rightarrow Y=\\frac{X-\\overline{x}}{s_x}\n\\]\n\\[\nEscalado \\space min-max \\rightarrow Y=\\frac{X-min_x}{max_x-min_x}\n\\]\nEn R, la función scale() se puede utilizar para realizar estas operaciones de estandarización. Automáticamente, puede actuar sobre las columnas de un data.frame, aplicando la misma operación a todas ellas (siempre que todas sean cuantitativas).\n\n\n\n2.7.2 Transformación de variables cualitativas\nA diferencia de las variables cuantitativas, que representan cantidades numéricas, las variables cualitativas, también conocidas como variables categóricas, se utilizan para describir características o cualidades que no tienen un valor numérico intrínseco. Las variables cualitativas son esenciales en la investigación y el análisis de datos, ya que a menudo se utilizan para clasificar, segmentar y comprender información sobre grupos, categorías o características. Algunas técnicas comunes para analizar variables cualitativas incluyen la creación de tablas de frecuencia para contar la ocurrencia de cada categoría y el uso de gráficos como gráficos de barras o diagramas de sectores para visualizar la distribución de categorías. Estos análisis pueden proporcionar información valiosa sobre patrones, tendencias y relaciones en los datos cualitativos, lo que puede ser fundamental para tomar decisiones informadas en una amplia gama de campos, desde marketing hasta investigación social y más.\nLas variables cualitativas se dividen en dos categorías principales:\n\n\n\n\n\n\nVariables Cualitativas Nominales\n\n\n\n\n\nLas variables nominales representan categorías o etiquetas que no tienen un orden inherente. Ejemplos comunes incluyen el género (masculino, femenino, otro), el estado civil (soltero, casado, divorciado) o los colores (rojo, azul, verde). No se pueden realizar operaciones matemáticas en variables nominales, como sumar o restar.\n\n\n\n\n\n\n\n\n\nVariables Cualitativas Ordinales\n\n\n\n\n\nLas variables ordinales representan categorías con un orden natural o jerarquía, pero la distancia entre las categorías no es necesariamente uniforme ni conocida. Ejemplos incluyen la calificación de satisfacción del cliente (muy insatisfecho, insatisfecho, neutral, satisfecho, muy satisfecho) o el nivel de educación (primaria, secundaria, universitaria). Aunque se pueden establecer comparaciones de orden (por ejemplo, “mayor que” o “menor que”), no es apropiado realizar operaciones matemáticas en variables ordinales.\n\n\n\nEn R, las variables categóricas se denominan factores (factors) y sus categorías niveles (levels). Es importante procesarlos adecuadamente para que los modelos aprovechen la información que contienen estas variables. Por otro lado, si se codifica incorrectamente esta información los modelos pueden estar realizando operaciones absurdas aunque nos devuelvan resultados aparentemente válidos.\n\n\n\n\n\n\nR\n\n\n\nPor defecto, R transforma columnas tipo string en factores al leer los datos de un archivo. Además, por defecto, R ordena los niveles de los factores alfabéticamente, según sus etiquetas. Debemos tener cuidado con esto, puesto que en muchos análisis es muy importante saber qué nivel se está tomando como referencia, de entre los valores posibles de un factor, para comparar con los restantes. En ciertos modelos, la elección como referencia de uno de los valores del factor (típicamente el primero que aparece en la lista de niveles) cambia por completo los resultados, así como la interpretación de los mismos.\n\n\nEn variables ordinales se debe respetar estrictamente el orden preestablecido de los niveles. Por ejemplo, una ordenación (“regular” &lt; “bueno” &lt; “malo”) es inaceptable. Para establecer una ordenación explícita entre los niveles hay que especificarla manualmente si no coincide con la alfabética, y además configurar el argumento ordered = TRUE en la función factor():\n\nsatisfaccion &lt;- rep(c(\"malo\", \"bueno\", \"regular\"), c(3,3,3))\nsatisfaccion &lt;- factor(satisfaccion, ordered = TRUE, levels = c(\"malo\", \"regular\", \"bueno\"))\nsatisfaccion\n\n[1] malo    malo    malo    bueno   bueno   bueno   regular regular regular\nLevels: malo &lt; regular &lt; bueno\n\n\nPara comprobar qué nivel se toma como referencia en cada uno de los factores de una base de datos usamos la funión levels():\n\nlevels(bank.train$marital)\n\nNULL\n\n\nY esto, ¿es correcto? Veamos la distribución de las observaciones en las categorías de la variable marital:\n\nggplot(data = bank.train) +\n  geom_bar(mapping = aes(x = marital))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecomendación\n\n\n\nHabitualmente será más recomendable elegir como categoría de referencia para variables categóricas aquella categoría con mayor número de observaciones.\n\n\nPor tanto, en este caso particular deberíamos modificar la categoría de referencia como sigue:\n\nreorder_marital = factor(bank.train$marital, levels=(c(' married', ' single', ' divorced')))\nlevels(reorder_marital)\n\n[1] \" married\"  \" single\"   \" divorced\"\n\n\nNótese que la nueva variable aquí creada, reorder_marital, no ha sido incluida (aún) en el tibble bank. Para ello:\n\nbank.train$marital = reorder_marital\n\n\n2.7.2.1 Conversión de variables cuantitativas a variables categóricas\nLa conversión de variables cuantitativas a variables categóricas es un proceso importante en EDA que implica transformar datos numéricos en categorías. Esto se realiza con el propósito de simplificar el análisis, resaltar patrones específicos y facilitar la interpretación de los resultados. A continuación, se destacan algunas situaciones comunes en las que se realiza esta conversión y cómo se lleva a cabo:\n\nAgrupación de datos numéricos: En ocasiones, es útil agrupar datos numéricos en intervalos o categorías para resaltar tendencias generales. Por ejemplo, en un estudio de edades de una población, en lugar de analizar cada edad individual, se pueden crear grupos como “menos de 18 años”, “18-30 años”, “31-45 años” y así sucesivamente.\nCreación de variables binarias: A menudo, se convierten variables numéricas en variables binarias (\\(1\\) o \\(0\\)) para simplificar el análisis. Por ejemplo, en un estudio de satisfacción del cliente, se puede crear una variable binaria donde “\\(1\\)” indica clientes satisfechos y “\\(0\\)” indica clientes insatisfechos.\nCategorización de variables continuas: Las variables continuas, como ingresos o puntuaciones, se pueden convertir en categorías para segmentar la población. Esto puede ser útil en análisis demográficos o de segmentación de mercado.\nSimplificación de modelos: Algunos modelos de ML pueden beneficiarse de la conversión de variables cuantitativas a categóricas para mejorar la interpretación y la eficacia del modelo.\n\n\n\n\n\n\n\nPara recordar\n\n\n\nEl proceso de conversión de variables cuantitativas a categóricas generalmente implica definir criterios o reglas claras para agrupar los valores numéricos en categorías significativas. Estos criterios pueden basarse en conocimiento previo del dominio, EDA o consideraciones específicas del problema. En esta etapa te vendrá genial contar con la ayuda de un experto en el dominio de aplicación, y puedes llevar a cabo cambios catastróficos en caso de no contar con esa ayuda.\n\n\nEs importante tener en cuenta que la conversión de variables cuantitativas a categóricas debe realizarse de manera cuidadosa y considerar el impacto en el análisis. La elección de cómo categorizar los datos debe estar respaldada por una comprensión sólida del problema y los objetivos del estudio. Además, se debe documentar claramente el proceso de conversión para que otros puedan replicarlo y comprender las categorías resultantes.\nA modo de ejemplo, vamos a categorizar la varible age en la base de datos bank. Para ello elegimos (elegimos!!!) las siguientes agrupaciones en la variable edad: (0,40],(40,60],(60,100].\n\n bank.train &lt;- within(bank.train, {   \n  age.cat &lt;- NA # need to initialize variable\n  age.cat[age &lt;= 40] &lt;- \"Low\"\n  age.cat[age &gt; 40 & age &lt;= 60] &lt;- \"Middle\"\n  age.cat[age &gt; 60] &lt;- \"High\"\n   } )\n\nbank.train$age.cat &lt;- factor(bank.train$age.cat, levels = c(\"Low\", \"Middle\", \"High\"))\nsummary(bank.train$age.cat)\n\n   Low Middle   High \n  3116   2151    314",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#valores-comunes-y-atípicos",
    "href": "eda.html#valores-comunes-y-atípicos",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.8 Valores comunes y atípicos",
    "text": "2.8 Valores comunes y atípicos\nLos gráficos de barras relacionados con variables cualitativas nos han ayudado a identificar los valores más frecuentes o las categorías más repetidas en esas variables. Estos gráficos reflejan la frecuencia de cada categoría, es decir, el número de veces que aparece en el conjunto de datos. A veces ese número se representa en porcentaje respecto al número total de observaciones, proporcionando una visión relativa de la prevalencia en cada categoría. La moda es la categoría que aparece con mayor frecuencia en el conjunto de datos. Es especialmente útil para identificar la categoría más común y es aplicable a variables categóricas.\nEn el caso de las variables cuantitativas, el histograma de frecuencias se convierte en una herramienta gráfica sumamente útil para alcanzar este mismo objetivo.\n\n2.8.1 Estadísticos resumen\nA continuación te explicamos algunas medidas que resumen el comportamiento de una variable aleatoria cuantitativa:\n\n\n\n\n\n\nMedia\n\n\n\n\n\nLa media aritmética es el promedio de todos los valores de la variable. Se calcula sumando todos los valores y dividiendo por el número de observaciones. La media proporciona una indicación de la tendencia central de los datos.\n\n\n\n\n\n\n\n\n\nMediana\n\n\n\n\n\nLa mediana es el valor central en un conjunto de datos ordenados en forma ascendente o descendente. Divide el conjunto de datos en dos mitades iguales. La mediana es menos sensible a valores extremos que la media y es especialmente útil cuando los datos no siguen una distribución (aproximadamente) normal.\n\n\n\n\n\n\n\n\n\nModa\n\n\n\n\n\nLa moda es el valor que ocurre con mayor frecuencia en un conjunto de datos. Puede haber una o más modas en un conjunto de datos, y esta medida es especialmente útil para variables discretas.\n\n\n\n\n\n\n\n\n\nRango\n\n\n\n\n\nEl rango es la diferencia entre el valor máximo y el valor mínimo en un conjunto de datos. Proporciona una indicación de la dispersión o variabilidad de los datos.\n\n\n\n\n\n\n\n\n\nDesviación Estándar\n\n\n\n\n\nLa desviación estándar mide la dispersión de los datos con respecto a la media, y tiene sus mismas unidades de medida. Valores más altos indican mayor variabilidad. Es especialmente útil cuando se asume una distribución normal.\n\n\n\n\n\n\n\n\n\nCuartiles y Percentiles\n\n\n\n\n\nLos cuartiles dividen un conjunto de datos en cuatro partes iguales, mientras que los percentiles dividen los datos en cien partes iguales. Los cuartiles y percentiles son útiles para identificar valores atípicos y comprender la distribución de los datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de Variación\n\n\n\n\n\nEl coeficiente de variación es una medida de la variabilidad relativa de los datos y se calcula como la desviación estándar dividida por la media. Se expresa como un porcentaje y es útil para comparar la variabilidad entre diferentes conjuntos de datos.\n\n\n\nEn R, podemos obtener algunos estadísticos resumen mediante la opción summary.\n\nsummary(bank.train$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   32.00   39.00   41.24   49.00   95.00 \n\n\nCuriosamente, R no tiene una función estándar incorporada para calcular la moda. Así que creamos una función de usuario para calcular la moda de un conjunto de datos en R. Esta función toma el vector como entrada y da el valor de la moda como salida.\n\n# Create the function.\nsummary_moda &lt;- function(v) {\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\nsummary_moda(bank.train$age)\n\n[1] 31\n\n\n\n\n2.8.2 Valores atípicos\nLos valores atípicos (outliers en inglés) son observaciones inusuales, puntos de datos que no parecen encajar en el patrón o el rango de la variable estudiada. A veces, los valores atípicos son errores de introducción de datos; otras veces, sugieren nuevos datos científicos importantes.\nCuando es posible, es una buena práctica llevar a cabo el análisis con y sin los valores atípicos. Si se determina que su influencia en los resultados es insignificante y no se puede identificar su origen, puede ser razonable reemplazarlos con valores faltantes y continuar con el análisis. Sin embargo, si estos valores atípicos tienen un impacto sustancial en los resultados, no se deben eliminar sin una justificación adecuada. En este caso, será necesario investigar la causa subyacente (por ejemplo, un error en la entrada de datos) y documentar su exclusión en el informe correspondiente.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#valores-faltantes",
    "href": "eda.html#valores-faltantes",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.9 Valores faltantes",
    "text": "2.9 Valores faltantes\nLos valores faltantes (missing), también conocidos como valores nulos o valores ausentes, son observaciones o datos que no están disponibles o que no han sido registrados para una o más variables en un conjunto de datos. Estos valores pueden surgir por diversas razones, como errores de entrada de datos, respuestas incompletas en una encuesta, fallos en la medición o simplemente porque cierta información no está disponible en un momento dado.\n\n\n\n\n\n\nPara recordar\n\n\n\nLa presencia de valores faltantes en un conjunto de datos es un problema común en el análisis de datos y puede tener un impacto significativo en la calidad de los resultados. Es importante abordar adecuadamente los valores faltantes, ya que pueden sesgar los análisis y conducir a conclusiones incorrectas si no se manejan correctamente.\n\n\nAlgunas de las estrategias comunes para tratar los valores faltantes incluyen:\n\nEliminación de filas o columnas: Si la cantidad de valores faltantes es pequeña en comparación con el tamaño total del conjunto de datos, una opción es eliminar las filas o columnas que contengan valores faltantes. Sin embargo, esta estrategia puede llevar a la pérdida de información importante.\nImputación de valores: Esta estrategia implica estimar o llenar los valores faltantes con valores calculados a partir de otros datos disponibles. Esto puede hacerse utilizando técnicas como la imputación media (rellenar con la media de la variable), imputación mediana (rellenar con la mediana), imputación de vecinos más cercanos o técnicas más avanzadas como regresión u otras técnicas de modelado.\nMarcadores especiales: En algunos casos, es útil asignar un valor específico (como “N/A” o “-999”) para indicar que un valor está ausente. Esto puede ser útil cuando se desea mantener un registro explícito de los valores faltantes sin eliminarlos o imputarlos. Es importante que, en este caso, el valor asignado no tenga otro significado. Por ejemplo, asignamos “-999” como marcador de valor faltante y sin embargo, es un valor plausible dentro del rango de valores de la variable.\nMétodos basados en modelos: Utilizar modelos estadísticos o de aprendizaje automático para predecir los valores faltantes en función de otras variables disponibles. Esto puede ser especialmente eficaz cuando los datos faltantes siguen un patrón que puede ser capturado por el modelo.\n\nLa elección de la estrategia adecuada para tratar los valores faltantes depende del contexto del análisis, la cantidad de datos faltantes y la naturaleza de los datos. Es fundamental abordar este problema de manera cuidadosa y transparente, documentando cualquier procedimiento de imputación o tratamiento de valores faltantes utilizado en el análisis para garantizar la integridad y la validez de los resultados.\n\n\n\n\n\n\nPeligro\n\n\n\nSustituir valores faltantes por otros obtenidos con técnicas y métodos estadísticos o de aprendizaje automático siempre es un riesgo, pues implica “inventar” datos allá donde no los hay.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "eda.html#correlación-entre-variables",
    "href": "eda.html#correlación-entre-variables",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "2.10 Correlación entre variables",
    "text": "2.10 Correlación entre variables\nExisten varios métodos y técnicas para estudiar la correlación entre variables, lo que ayuda a comprender las relaciones entre las diferentes características en un conjunto de datos. En próximos cursos estudiarás que es de especial interés estudiar las relaciones entre la variable objetivo y las variables explicativas.\nPuedes desplegar los paneles siguientes para averiguar alguno de los métodos más comunes.\n\n\n\n\n\n\nMatriz de correlación\n\n\n\n\n\nLa matriz de correlación es una tabla que muestra las correlaciones entre todas las combinaciones de variables en un conjunto de datos. Los valores de correlación varían entre \\(-1\\) y \\(1\\), donde \\(-1\\) indica una correlación negativa perfecta, \\(1\\) indica una correlación positiva perfecta y \\(0\\) indica la ausencia de correlación. Este método es especialmente útil para identificar relaciones lineales entre variables numéricas.\n\n\n\n\n\n\n\n\n\nGráficos de dispersión\n\n\n\n\n\nLos gráficos de dispersión muestran la relación entre dos variables numéricas mediante puntos en un plano cartesiano. Estos gráficos permiten visualizar patrones de dispersión y tendencias entre las variables. Si los puntos se agrupan en una forma lineal, indica una posible correlación lineal.\n\n\n\n\n\n\n\n\n\nMapas de calor\n\n\n\n\n\nLos mapas de calor son representaciones visuales de la matriz de correlación en forma de un gráfico de colores. Permiten identificar rápidamente las relaciones fuertes o débiles entre variables y son útiles para resaltar patrones en grandes conjuntos de datos.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Pearson\n\n\n\n\n\nEste coeficiente mide la correlación lineal entre dos variables numéricas. Varía entre \\(-1\\) y \\(+1\\), donde valores cercanos a \\(-1\\) o \\(+1\\) indican una correlación fuerte, mientras que valores cercanos a \\(0\\) indican una correlación débil o nula.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Spearman\n\n\n\n\n\nEste coeficiente evalúa la correlación monotónica entre dos variables, lo que significa que puede detectar relaciones no lineales. Es útil cuando las variables no siguen una distribución normal.\n\n\n\n\n\n\n\n\n\nCoeficiente de correlación de Kendall\n\n\n\n\n\nSimilar al coeficiente de Spearman, evalúa la correlación entre variables, pero se centra en la concordancia de los rangos de datos, lo que lo hace útil para datos no paramétricos y muestras pequeñas.\n\n\n\n\n\n\n\n\n\nPruebas estadísticas\n\n\n\n\n\nLas pruebas estadísticas, como la prueba t de Student o la ANOVA, pueden utilizarse para evaluar si existe una diferencia significativa en los promedios de una variable entre diferentes categorías de otra variable. Si la diferencia es significativa, puede indicar una correlación entre las variables.\n\n\n\nVamos a estudiar la relación existente entre la variable objetivo deposit y la variable duration de la base de datos bank.\n\nggplot(bank.train, aes(x = log(duration), colour = deposit)) +\n  geom_density(lwd=2, linetype=1)\n\n\n\n\n\n\n\n\nPuede observarse una relación. Valores altos de la variable duración parecen estar relacionados con observaciones con deposit igual a ‘yes’.\n\ndf = bank.train %&gt;% \n      select(duration,deposit)%&gt;%\n      mutate(log.duration=log(duration))\n\n# Resumen para los casos de depósito\nsummary(df %&gt;% filter(deposit==\"yes\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.079   5.497   6.073   6.046   6.593   8.087 \n\n# Resumen para los casos de no depósito\nsummary(df %&gt;% filter(deposit==\"no\") %&gt;% .$log.duration)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6931  4.5433  5.0999  5.0308  5.6276  7.5022 \n\n\nGráficamente, podemos comparar los boxplots.\n\nggplot(df, aes(deposit, log.duration)) +\n        geom_boxplot()\n\n\n\n\n\n\n\n\nPodemos determinar la importancia de relación. Por ejemplo, podemos realizar un test de la T para igualdad de medias. Estudiaremos estos conceptos en el Capítulo 3.\n\nt.test(log.duration ~ deposit, data = df)\n\n\n    Welch Two Sample t-test\n\ndata:  log.duration by deposit\nt = -45.828, df = 5464.5, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group no and group yes is not equal to 0\n95 percent confidence interval:\n -1.0583835 -0.9715488\nsample estimates:\n mean in group no mean in group yes \n         5.030821          6.045787 \n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nComprenderás este resultado a lo largo del curso. De momento, puedes preguntar al profesor. Dejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\nEs posible estudiar la relación entre dos variables categóricas de manera gráfica.\n\nggplot(data = bank.train, aes(x = housing, fill = deposit)) +\n    geom_bar()\n\n\n\n\n\n\n\n\nParece haber una relación, estando asociados las observaciones de personas con casa propia a un mayor porcentaje de `no’ en la variable respuesta. Podemos obtener la tabla de contingencia:\n\ndata1=table(bank.train$housing, bank.train$deposit)\n\n\ndimnames(data1) &lt;- list(housing = c(\"no\", \"yes\"),\n                        deposit = c(\"no\", \"yes\"))\ndata1\n\n       deposit\nhousing   no  yes\n    no  1246 1688\n    yes 1662  985\n\n\nY el contraste correspondiente para la hipótesis nula de no existencia de relación. Estudiaremos estos conceptos en el Capítulo 3.\n\nchisq.test(bank.train$housing, bank.train$deposit)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  bank.train$housing and bank.train$deposit\nX-squared = 229.44, df = 1, p-value &lt; 2.2e-16\n\n\n\n\n\n\n\n\nEjercicio\n\n\n\nDejamos como ejercicio para el alumno la interpretación del resultado del test.\n\n\n\n\n\n\nFox, John, y Sanford Weisberg. 2018. An R companion to applied regression. Sage publications.\n\n\nTukey, John W et al. 1977. Exploratory data analysis. Vol. 2. Reading, MA.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  },
  {
    "objectID": "para.html#definición-de-estadístico",
    "href": "para.html#definición-de-estadístico",
    "title": "3  Estimación y contraste paramétrico",
    "section": "",
    "text": "Media: Promedio aritmético de los valores de una variable en la muestra.\nMediana: Valor que divide la muestra en dos partes iguales respecto a una variable.\nModa: Valor de una variable que aparece con mayor frecuencia en la muestra.\nVarianza: Medida de la dispersión de los datos respecto a la media.\nDesviación estándar: Raíz cuadrada de la varianza, que también mide la dispersión.\nCoeficiente de correlación: Medida de la relación entre dos variables.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#estimación-puntual",
    "href": "para.html#estimación-puntual",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.2 Estimación puntual",
    "text": "3.2 Estimación puntual\nLa estimación puntual es una técnica estadística que consiste en utilizar los datos de una muestra para calcular un valor único, denominado estimador puntual, que se usa como mejor aproximación de un parámetro desconocido de la población. Este parámetro puede ser, por ejemplo, la media, la varianza, la proporción, entre otros. La estimación puntual proporciona una forma simple y directa de hacer inferencias sobre parámetros poblacionales a partir de una muestra, aunque su simplicidad también implica que no proporciona información sobre la precisión o variabilidad de la estimación, aspectos que se abordan mediante la estimación por intervalos y otras técnicas inferenciales.\n\n3.2.1 Conceptos clave en la estimación puntual\nEstimador: Es una fórmula o función que se aplica a los datos de la muestra para obtener la estimación puntual. Por ejemplo, la media muestral (\\(\\bar{x}\\)) es un estimador de la media poblacional (\\(\\mu\\)). Formalmente, dada una variable aleatoria \\(X\\) con función de distribución \\(F_\\theta\\), con parámetro \\(\\theta\\) desconocido, un estadístico o estimador \\(T = T(X_1,\\ldots,X_n)\\) es una función real de la muestra aleatoria simple (m.a.s.) \\((X_1,\\ldots, X_n)\\) que estima el valor del parámetro desconocido. \\[\nT = T(X_1,\\ldots, X_n) = \\hat{\\theta}\n\\] Un estadístico es una variable aleatoria, y por lo tanto, tiene asociada una distribución que se denomina distribución muestral.\nPor ejemplo, la media \\[T_1(X_1,\\ldots,X_n)=\\bar{X}=\\frac{X_1+\\ldots+X_n}{n}\\] y la mediana \\[T_2(X_1,\\ldots,X_n)=\\frac{X_{(n/2)}+X_{(n/2+1)}}{2}\\] son estimadores.\nEstimación: Es el valor numérico específico obtenido al aplicar el estimador a una muestra concreta de datos. Por ejemplo, si \\(\\bar{x} = 5.4\\), esa es la estimación puntual de \\(\\mu\\).\n\n\n3.2.2 Ejemplos de estimadores puntuales\n\nMedia muestral (\\(\\bar{x}\\)): Utilizada para estimar la media poblacional (\\(\\mu\\)). Aplicamos el estimador a una realización de la muestra (\\(x_1,\\ldots,x_n\\)), obteniendo el estimador muestral: \\[\n\\bar{x} = T_1(x_1,\\ldots,x_n)= \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\nVarianza muestral (\\(s^2\\)): Utilizada para estimar la varianza poblacional (\\(\\sigma^2\\)). \\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\n\\]\nDesviación típica muestral (\\(s\\)). Es la raíz cuadrada de la varianza muestral. Tiene las mismas unidades de medida que la variable original.\nProporción muestral (\\(\\hat{p}\\)): Utilizada para estimar la proporción poblacional (\\(p\\)). \\[\n\\hat{p} = \\frac{x}{n}\n\\] donde \\(x\\) es el número de éxitos en la muestra y \\(n\\) es el tamaño de la muestra.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#propiedades-de-los-estimadores",
    "href": "para.html#propiedades-de-los-estimadores",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.3 Propiedades de los estimadores",
    "text": "3.3 Propiedades de los estimadores\nExisten diferentes métodos para obtener estimadores de un parámetro poblacional. ¿Cómo elegir el estimador más adecuado para un parámetro desconocido? ¿Cuáles son las propiedades de un buen estimador?\nPara que un estimador sea considerado adecuado, generalmente debe cumplir con ciertas propiedades:\n\nInsesgadez: Un estimador es insesgado (o centrado) si, en promedio, coincide con el valor verdadero del parámetro que se estima. Es decir, el valor esperado del estimador es igual al parámetro poblacional.\n\n\\[E(\\hat{\\theta}) = \\theta\\] La comparaciones que implican estimadores sesgados a menudo se basan en el error cuadrático medio definido como: \\[\nECM(\\hat{\\theta})=E[(\\hat{\\theta}- \\theta)^2]=Var(\\hat{\\theta})+(E(\\hat{\\theta})- \\theta)^2=Eficiencia+  Sesgo\n\\] En este grado vas a volver a oir hablar de esta medida en la asignatura de regresión. En ese caso, la medida de error más empleada es: \\[\nECM=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{f}(x_i))^2\n\\] donde \\(\\hat{f}(x_i)\\) e sla predicción que hace un modelo de regresión mediante una función \\(\\hat{f}\\) para la i-ésima observación muestral \\(x_i\\).\n\nConsistencia: Un estimador es consistente si, a medida que el tamaño de la muestra aumenta, la estimación se aproxima al valor verdadero del parámetro. Es decir: \\[\nlim_{n  \\rightarrow \\infty}P(|\\hat{\\theta}-\\theta|\\geq\\delta)=0, \\forall\\delta&gt;0\n\\] donde \\(n\\) es el tamaño muestral.\nEficiencia: La varianza de un estimador debe ser lo más pequeña posible. Entre dos estimadores insesgados, el más eficiente es el que tiene menor varianza, es decir, el que proporciona estimaciones más precisas.\nSuficiencia: Un estimador es suficiente si utiliza toda la información contenida en la muestra sobre el parámetro que se está estimando.\n\n\n\n\n\n\n\nEjemplo Práctico. Insesgadez\n\n\n\n\n\nPara entender la propiedad de insesgadez en inferencia estadística, es útil realizar una simulación en R. Como hemos visto, la insesgadez de un estimador significa que, en promedio, el estimador coincide con el parámetro verdadero de la población.\nVamos a realizar una simulación para ilustrar esta propiedad utilizando la media muestral como estimador de la media poblacional. Generaremos muchas muestras aleatorias de una distribución normal y compararemos la media de las medias muestrales con la media verdadera de la población.\n\n# Cargar la librería ggplot2\nlibrary(ggplot2)\n\n# Parámetros de la simulación\nset.seed(123)  # Para reproducibilidad\nn_muestras &lt;- 1000  # Número de muestras\ntamano_muestra &lt;- 30  # Tamaño de cada muestra\nmedia_poblacional &lt;- 50  # Media verdadera de la población\ndesviacion_estandar &lt;- 10  # Desviación estándar de la población\n\n# Generar muestras y calcular medias muestrales\nmedias_muestrales &lt;- numeric(n_muestras)\nfor (i in 1:n_muestras) {\n  muestra &lt;- rnorm(tamano_muestra, mean = media_poblacional, sd = desviacion_estandar)\n  medias_muestrales[i] &lt;- mean(muestra)\n}\n\n# Calcular la media de las medias muestrales\nmedia_de_medias_muestrales &lt;- mean(medias_muestrales)\n\n# Imprimir resultados\ncat(\"Media verdadera de la población:\", media_poblacional, \"\\n\")\n\nMedia verdadera de la población: 50 \n\ncat(\"Media de las medias muestrales:\", media_de_medias_muestrales, \"\\n\")\n\nMedia de las medias muestrales: 49.93807 \n\n# Crear un data frame para ggplot\ndatos &lt;- data.frame(medias_muestrales)\n\n# Graficar las medias muestrales usando ggplot2\nggplot(datos, aes(x = medias_muestrales)) +\n  geom_histogram(bins = 30, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(aes(xintercept = media_poblacional), color = \"red\", linetype = \"dashed\", size = 1.2) +\n  geom_vline(aes(xintercept = media_de_medias_muestrales), color = \"blue\", linetype = \"dashed\", size = 1.2) +\n  labs(title = \"Distribución de las Medias Muestrales\",\n       x = \"Medias Muestrales\",\n       y = \"Frecuencia\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  annotate(\"text\", x = media_poblacional, y = max(table(datos$medias_muestrales)) * 0.9, label = \"Media Verdadera\", color = \"red\", angle = 90, vjust = -0.5) +\n  annotate(\"text\", x = media_de_medias_muestrales, y = max(table(datos$medias_muestrales)) * 0.9, label = \"Media de las Medias Muestrales\", color = \"blue\", angle = 90, vjust = 1.5)\n\n\n\n\n\n\n\n\nHemos creado un bucle para generar n_muestras muestras aleatorias de una distribución normal con la media y desviación estándar especificadas. Para cada muestra, calculamos la media muestral y la almacenamos en el vector medias_muestrales. Calculamos la media de todas las medias muestrales generadas y mostramos la media verdadera de la población y la media de las medias muestrales.\nEl gráfico resultante muestra un histograma de las medias muestrales con líneas verticales indicando la media verdadera de la población y la media de las medias muestrales. Esto ilustra visualmente la propiedad de insesgadez del estimador de la media.\n\n\n\n\n\n\n\n\n\nEjemplo Práctico. Consistencia\n\n\n\n\n\nPara ilustrar la propiedad de consistencia de un estimador, podemos realizar una simulación similar a la anterior, pero en lugar de enfocarnos en la media de las medias muestrales, nos centraremos en cómo el estimador se aproxima a la verdadera media poblacional a medida que aumenta el tamaño de la muestra. En otras palabras, mostraremos cómo el estimador se vuelve más preciso a medida que se incrementa el tamaño de la muestra.\n\n# Cargar la librería ggplot2\nlibrary(ggplot2)\n\n# Parámetros de la simulación\nset.seed(12443)  # Para reproducibilidad\nn_simulaciones &lt;- 1000  # Número de simulaciones\ntamanos_muestra &lt;- seq(10, 1000, by = 10)  # Tamaños de las muestras\nmedia_poblacional &lt;- 50  # Media verdadera de la población\ndesviacion_estandar &lt;- 10  # Desviación estándar de la población\nmedias_estimadas &lt;- numeric(length(tamanos_muestra))  # Vector para almacenar medias estimadas\n\n# Realizar simulaciones para diferentes tamaños de muestra\nfor (i in 1:length(tamanos_muestra)) {\n  # Generar muestras y calcular medias muestrales\n  medias_muestrales &lt;- replicate(n_simulaciones, mean(rnorm(tamanos_muestra[i], mean = media_poblacional, sd = desviacion_estandar)))\n  # Calcular la media de las medias muestrales\n  medias_estimadas[i] &lt;- mean(medias_muestrales)\n}\n\n# Crear un data frame para ggplot\ndatos &lt;- data.frame(tamanos_muestra, medias_estimadas)\n\n# Graficar las medias estimadas vs. el tamaño de la muestra usando ggplot2\nggplot(datos, aes(x = tamanos_muestra, y = medias_estimadas)) +\n  geom_line(color = \"blue\") +\n  geom_hline(yintercept = media_poblacional, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Convergencia del Estimador a la Media Verdadera\",\n       x = \"Tamaño de la Muestra\",\n       y = \"Media Estimada\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nEl gráfico resultante muestra cómo las medias estimadas convergen hacia la media verdadera de la población a medida que aumenta el tamaño de la muestra. Esto ilustra la propiedad de consistencia del estimador. La línea roja representa la media verdadera de la población, mientras que la línea azul representa las medias estimadas en función del tamaño de la muestra. A medida que el tamaño de la muestra aumenta, las medias estimadas se acercan cada vez más a la media verdadera.\n\n\n\n\n3.3.1 Distribuciones muestrales\nHemos visto que usamos estadísticos para estimar los parámetros desconocidos de la población. Estamos interesados en estadísticos con buenas propiedades. Además, estos estadísticos son variables aleatorias con distribución de probabilidad.\nSabemos, por el TCL visto en la Introducción, que, teniendo el tamaño muestral adecuado, la distribución de los estadísticos será una Normal.\nPor ejemplo, dadas \\(X_1,\\ldots,X_n\\) variables aleatorias independientes e idénticamente distribuidas con media \\(\\mu\\) y varianza \\(\\sigma^2\\) conocida, la media muestral: \\[\n\\bar{X}=\\frac{X_1+\\ldots+X_n}{n}\n\\] tiene media igual a \\(E[\\bar{X}]=\\mu\\) y varianza igual a \\(V[\\bar{X}]=\\sigma^2/n\\).\nEntonces, por el TCL, con \\(n\\) suficientemente grande: \\[\n\\bar{X} \\sim N \\left ( \\mu,\\frac{\\sigma^2}{n} \\right )\n\\]\nOtro ejemplo sería, dadas \\(X_1,\\ldots,X_n\\) variables aleatorias independientes e idénticamente distribuidas con una distribución Bernoulli de parámetro \\(p\\). Para \\(n\\) suficientemente grande se tiene que: \\[\n\\hat{p} \\sim N \\left (p,\\frac{p(1-p)}{n} \\right ) \\Leftrightarrow \\frac{\\hat{p}-p}{\\sqrt{p(1-p)/n}} \\sim N(0,1)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#método-de-los-momentos",
    "href": "para.html#método-de-los-momentos",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.4 Método de los momentos",
    "text": "3.4 Método de los momentos\nEl método de los momentos es una técnica utilizada en estadística para estimar los parámetros desconocidos de una distribución de probabilidad. Fue introducido por el estadístico Karl Pearson en 1984. Este método se basa en igualar los momentos muestrales (calculados a partir de los datos observados) con los momentos teóricos (expresados en términos de los parámetros de la distribución).\n\n3.4.1 Definición de Momentos\nEn estadística, los momentos de una distribución son medidas que describen diversas características de la distribución, como su media, varianza, simetría y curtosis. Los momentos más comunes son:\n\nPrimer Momento (Media): \\(\\mu = E[X]\\)\nSegundo Momento (Varianza):\\(\\mu_2 = E[X^2]\\)\nTercer Momento (Asimetría):\\(\\mu_3 = E[X^3]\\)\nCuarto Momento (Curtosis):\\(\\mu_4 = E[X^4]\\)\n\n\n\nk-esimo Momento:\\(\\mu_k = E[X^k]\\)\n\nLos momentos poblacionales pueden ser vistos como funciones de los parámetros desconocidos \\(\\theta_1,\\ldots,\\theta_k\\). Se asume que se conoce el modelo de probabilidad de la variable objeto de estudio.\n\n\n3.4.2 Pasos del Método de los Momentos\nEl método de los momentos consiste en resolver un conjunto de ecuaciones y tiene los siguientes pasos:\n\nCalcular momentos muestrales: Se calculan los momentos muestrales de los datos observados. El (k)-ésimo momento muestral se define como: \\(m_k = \\frac{1}{n} \\sum_{i=1}^{n}X_i^k\\) donde \\(n\\) es el tamaño de la muestra y \\(X_i\\) son los valores de la muestra.\nIgualar momentos muestrales y teóricos: Se igualan los momentos muestrales con los momentos teóricos de la distribución. Los momentos teóricos se expresan en términos de los parámetros desconocidos que se desean estimar.\nResolver el sistema de ecuaciones: Se resuelve el sistema de ecuaciones resultante para encontrar los estimadores de los parámetros desconocidos. Fíjate que tenemos \\(k\\) ecuaciones y \\(k\\) parámetros (\\(\\theta_1,\\ldots,\\theta_k\\)). De modo que es posible despejar los parámetros de estas ecuaciones, que quedando estos parámetros en función de los momentos. En estas ecuaciones se sustituyen los momentos poblacionales por sus correspondientes momentos poblacionales. Esto da como resultado estimaciones de esos parámetros.\n\n\n\n\n\n\n\nEjemplo, distribución Normal\n\n\n\n\n\nSupongamos que deseamos estimar los parámetros (\\(\\mu\\)) y (\\(\\sigma^2\\)) de una distribución Normal (\\(N(\\mu, \\sigma^2)\\)).\n\nCalcular los momentos muestrales: \\[m_1 = \\frac{1}{n} \\sum_{i=1}^n X_i \\] \\[m_2 = \\frac{1}{n}\\sum_{i=1}^nX_i^2 \\]\nIgualar los momentos muestrales con los momentos teóricos: Para una distribución Normal, el primer momento teórico (media) es\n\n\\[\\mu_1=E(X)=\\mu\\]\ny el segundo momento teórico es:\n\\[\\mu_2=E(X^2)=Var(X)+E(X)^2=\\mu^2 + \\sigma^2\\]\nIgualando estos con los momentos muestrales obtenidos de los datos: \\[m_1 = \\bar{X}=\\mu\\] \\[m_2 = \\frac{1}{n}\\sum_{i=1}^n X_i^2=\\mu_2= \\mu^2 + \\sigma^2\\]\n\nResolver el sistema de ecuaciones: De la primera ecuación, tenemos:\n\n\\[\\hat{\\mu} = \\bar{X}\\]\nSustituyendo en la segunda ecuación: \\[\\hat{\\sigma}^2=\\frac{1}{n}\\sum_{i=1}^n X_i^2-\\bar{X}^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X})^2\\]\nQue son los estimadores de los parámetros.\n\n\n\n\n\n\n\n\n\nEjemplo, distribución Binomial\n\n\n\n\n\nSea \\(X_1,\\ldots,X_n\\) una muestra aleatoria simple de una \\(Binom(k,p)\\), con \\(k\\) y \\(p\\) desconocidos. Entonces, los momentos poblacionales son:\n\\[\\mu_1=E(X)=kp\\] \\[\\mu_2=E(X^2)=Var(X)+E(X)^2=kp(1-p)+k^2p^2\\] Los momentos muestrales son:\n\\[\nm_1=\\bar{X}\n\\] \\[\nm_2=\\frac{1}{n}\\sum_{i=1}^nX_i^2\n\\] Igualando los momentos poblacionales a los muestrales, obtenemos:\n\\[\nm_1=\\bar{X}=kp\n\\]\n\\[\nm_2=\\frac{1}{n}\\sum_{i=1}^nX_i^2=\\mu_2=kp(1-p)+k^2p^2\n\\] Despejando \\(k\\) y \\(p\\), obtenemos los estimadores:\n\\[\n\\hat{k}^2=\\frac{\\bar{X}^2}{\\bar{X}-(1/n)\\sum_{i=1}^n(X_i- \\bar{X})^2}\n\\] \\[\\hat{p}=\\frac{\\bar{X}}{\\hat{k}}\\]\n\n\n\n\n\n3.4.3 Ventajas y limitaciones\nLos estimadores de los momentos presentan interesantes propiedades estadísticas, aunqeu también tienen sus limitaciones.\nVentajas:\n\nSimplicidad: El método de los momentos es relativamente sencillo de aplicar y no requiere técnicas complejas de optimización.\nIntuición: Ofrece una interpretación intuitiva de los parámetros en términos de momentos.\n\nLimitaciones:\n\nPrecisión: Los estimadores de los momentos no siempre son los estimadores más eficientes (no tienen la mínima varianza posible).\nAplicabilidad: En algunas distribuciones complejas, los momentos pueden no existir o ser difíciles de calcular.\nConsistencia: Los estimadores de momentos no siempre son consistentes, especialmente en muestras pequeñas.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#método-de-la-máxima-verosimilud",
    "href": "para.html#método-de-la-máxima-verosimilud",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.5 Método de la máxima verosimilud",
    "text": "3.5 Método de la máxima verosimilud\nEl método de la máxima verosimilitud es una técnica estadística ampliamente utilizada para estimar los parámetros desconocidos de una distribución de probabilidad. Este método se basa en encontrar los valores de los parámetros que maximicen la función de verosimilitud, la cual mide la probabilidad de observar los datos dados los parámetros. El método de máxima verosimilitud es el método más popular para obtener un estimador. La idea básica es seleccionar el valor del parámetro que hace que los datos sean más probables.\nDado un modelo estadiıstico (es decir, una familia de distribuciones \\(f(·|\\theta)| \\theta \\in \\Theta\\) donde \\(\\theta\\) es el parámetro del modelo), el método de máxima verosimilitud encuentra el valor del parámetro del modelo \\(\\theta\\) que maximiza la función de verosimilitud:\n\\[\n\\hat{\\theta}(x)= \\max_{\\theta \\in \\Theta}L(\\theta|\\mathbf{x})\n\\]\nPara una muestra aleatoria \\(\\mathbf{x}=(x_1,\\ldots,x_n)\\) de una variable aleatoria \\(X\\), la verosimilitud es proporcional al producto de las probabilidades asociadas a los valores individuales: \\[\n\\prod_jP(X=x_j)\n\\] El término verosimilitud fue acuñado por Sir Roland Fisher.\nCuando \\(X\\) es una variable aleatoria continua, un valor muestral \\(x_j\\) debe considerarse como que está (en general) en el intervalo \\((x_j-\\delta,x_j+\\delta)\\), donde \\(\\delta\\) representa la precisión de la medición. La verosimilutd es entonces proporcional a: \\[\n\\prod_jP(x_j-\\delta&lt;X&lt;x_j+\\delta).\n\\] Si \\(\\delta\\) es suficientemente pequeño, esta expresión es aproximadamente proporcional a: \\[\n\\prod_jf(x_j).\n\\] donde \\(f\\) es la función de densidad de \\(X\\). Por lo tanto, la verosimilitd describe lo plausible que es un valor del parámetro poblacional, dadas unas observaciones concretas de la muestra.\n\n3.5.1 Conceptos básicos\n\nFunción de verosimilitud: La función de verosimilitud, \\(L(\\theta|\\mathbf{x})\\), para un conjunto de datos \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) y un vector de parámetros \\(\\theta\\), es el producto de las funciones de densidad (o de probabilidad) de los datos observados, dadas las posibles realizaciones de \\(\\theta\\): \\[\nL(\\theta|\\mathbf{x}) = f(\\mathbf{x}|\\theta)=f(x_1,\\ldots,x_n|\\theta)=f(x_1|\\theta)f(x_2|\\theta)\\ldots f(x_n|\\theta)=\\prod_{i=1}^n f(x_i| \\theta)\n\\] donde \\(f(x_i|\\theta)\\) es la función de densidad (o de probabilidad) de \\(x_i\\) dado \\(\\theta\\).\nLog-Verosimilitud: Debido a que la función de verosimilitud puede implicar productos de muchos términos, es más práctico trabajar con su logaritmo natural, conocido como la log-verosimilitud: \\[\n\\ell(\\theta|\\mathbf{x}) = \\log L(\\theta|\\mathbf{x}) = \\sum_{i=1}^n \\log f(x_i|\\theta)\n\\]\n\n\n\n3.5.2 Procedimiento del método de Máxima Verosimilitud\n\nEspecificar la función de verosimilitud: Identificar la función de verosimilitud correspondiente a los datos observados y a la distribución supuesta.\nCalcular la Log-Verosimilitud: Tomar el logaritmo natural de la función de verosimilitud para obtener la función de log-verosimilitud.\nDerivar y resolver: Derivar la función de log-verosimilitud con respecto a cada parámetro y resolver las ecuaciones obtenidas igualando a cero (puntos críticos) para encontrar los estimadores de máxima verosimilitud (EMV).\nVerificar máximos: Asegurarse de que las soluciones encontradas corresponden a máximos y no a mínimos o puntos de inflexión, típicamente verificando la segunda derivada.\n\n\n\n\n\n\n\nEjemplo, distribución Normal\n\n\n\n\n\nSupongamos que tenemos una muestra \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) de una distribución Normal con media \\(\\mu\\) y varianza \\(\\sigma^2\\), y queremos estimar estos parámetros.\n\nFunción de verosimilitud: La función de densidad para una distribución normal es: \\[\nf(x_i|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\] Por lo tanto, la función de verosimilitud es: \\[\nL(\\mu, \\sigma^2| \\mathbf{x}) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n\\]\nLog-Verosimilitud: Tomamos el logaritmo natural de la función de verosimilitud: \\[\n\\ell(\\mu, \\sigma^2|\\mathbf{x}) = \\sum_{i=1}^n \\left[ -\\frac{1}{2} \\log(2\\pi\\sigma^2) - \\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right]\n\\]\nDerivadas y resolución: Derivamos la log-verosimilitud con respecto a \\(\\mu\\) y \\(\\sigma^2\\) y las igualamos a cero: \\[\n\\frac{\\partial \\ell}{\\partial \\mu} = \\sum_{i=1}^n \\frac{x_i - \\mu}{\\sigma^2} = 0 \\implies \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\] \\[\n\\frac{\\partial \\ell}{\\partial \\sigma^2} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^n (x_i - \\mu)^2 = 0 \\implies \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\hat{\\mu})^2\n\\]\nAsí, los estimadores de máxima verosimilitud para \\(\\mu\\) y \\(\\sigma^2\\) son la media muestral y la varianza muestral, respectivamente.\n\n\n\n\n\n\n\n\n\n\nEjemplo, distribución Binomial\n\n\n\n\n\nSupongamos que tenemos una muestra de tamaño \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) de una variable aleatoria Binomial con parámetro \\(p\\) que deseamos estimar. Se emplea dicha variable para describir el número de errores en las \\(n\\) pruebas asociadas. Se realiza un experimento y se obtienen un total de \\(4\\) errores en las \\(10\\) pruebas.\n\nFunción de verosimilitud: La verosimilud viene dada por:\n\n\\[\nL(p|\\mathbf{x}) = {10 \\choose 4}p^4(1-p)^6\n\\]\n\nLog-Verosimilitud: Tomamos el logaritmo natural de la función de verosimilitud: \\[\n\\ell(p|\\mathbf{x}) = log\\left({10 \\choose 4}\\right) +4log(p)+6log(1-p)\n\\]\nDerivadas y resolución: Derivamos la log-verosimilitud con respecto a \\(p\\) y las igualamos a cero: \\[\n\\frac{\\partial \\ell}{\\partial p} =\\frac{4}{p} -\\frac{6}{1-p}= 0 \\implies \\frac{4}{p}=\\frac{6}{1-p}\\implies 4-4p=6p\\implies 4=10p \\implies p=4/10=0.4\n\\]\n\nLa función de verosimilud nos informa, dados los datos, sobre los valores más plausibles (o creíbles) para el parámetroo \\(p\\).\n\n\n\n\n\n3.5.3 Ventajas y limitaciones\nLos estimadores de máxima verosimilutd presentan buenas propiedades estadísticas.\nVentajas:\n\nConsistencia: Los estimadores de máxima verosimilitud son consistentes, es decir, convergen en probabilidad al valor verdadero del parámetro a medida que el tamaño de la muestra aumenta.\nEficiencia: En muchos casos, los estimadores de máxima verosimilitud son eficientes, alcanzando la varianza mínima entre los estimadores insesgados (cumplen la igualdad de Cramér-Rao). El estimador máximo verosimil es asintóticamente eficiente y su distribución converge a la distribución Normal con valor esperado \\(\\theta\\) y la varianza es igual al inverso de la información de Fisher. La informacioon de Fisher es la cantidad de información que una muestra proporciona sobre el valor de un parámetro desconocido.\nFlexibilidad: Se puede aplicar a una amplia gama de distribuciones y modelos complejos.\nInvariantes: Si \\(T\\) es el estimador de máxima verosimilitud para \\(\\theta\\), entonces \\(\\tau(T)\\) es el estimador de máxima verosimilutd para \\(\\tau(\\theta)\\) para cualquier función \\(\\tau\\).\n\nLimitaciones:\n\nComplejidad computacional: Encontrar los estimadores de máxima verosimilitud puede implicar resolver ecuaciones no lineales, lo cual puede ser complejo y requerir técnicas numéricas.\nExistencia y unicidad: Los estimadores de máxima verosimilitud no siempre existen y, si existen, no siempre son únicos. En problemas reales, la derivada de la función de verosimilitud es, a veces, analíticamente intratable. En esos casos, se utilizan métodos iterativos para encontrar soluciones numéricas para las estimaciones de los parámetros.\nSesgo en muestras pequeñas: Los estimadores pueden ser sesgados en muestras pequeñas, aunque el sesgo disminuye a medida que el tamaño de la muestra aumenta.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#estimación-por-intervalo",
    "href": "para.html#estimación-por-intervalo",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.6 Estimación por intervalo",
    "text": "3.6 Estimación por intervalo\nLa estimación puntual proporciona una aproximación razonable para un parámetro de la población, pero no tiene en cuenta la variabilidad debido al tamaño muestral, la variabilidad en la población, el conocimiento de otros parámetros, etc.\nLa estimación por intervalo es una técnica en estadística que, a diferencia de la estimación puntual que proporciona un único valor, ofrece un rango de valores dentro del cual se espera que se encuentre el verdadero parámetro poblacional desconocido con un cierto nivel de confianza. Este rango se denomina intervalo de confianza.\nLa estimación por intervalos es una herramienta esencial en la Inferencia Estadística, ya que no solo ofrece una estimación del parámetro poblacional, sino que también proporciona un marco para entender la precisión y confiabilidad de esa estimación. Esto la convierte en una técnica poderosa para hacer inferencias más robustas y útiles basadas en datos muestrales.\n\n3.6.1 Conceptos clave en la estimación por intervalo\nIntervalo de Confianza (IC): Es un rango de valores calculado a partir de los datos de la muestra, que se utiliza para estimar el parámetro poblacional desconocido. Se expresa comúnmente como \\((\\text{Límite Inferior}, \\text{Límite Superior})\\).\nDada una muestra aleatoria simple \\(\\mathbf{X}=(X_1,X_2,\\ldots,X_n)\\) de una población \\(X\\) con función de distribución \\(F\\) que depende de un parámetro desconocido \\(\\theta\\), diremos que un estimador por intervalos de confianza del parámetro \\(\\theta\\) con un nivel de confianza de \\((1-\\alpha)=100*(1-\\alpha)\\%\\) es un intervalo de la forma \\((T_{inf}(\\mathbf{X}),T_{sup}(\\mathbf{X}))\\) que satisface: \\[P(\\theta \\in (T_{inf}(\\mathbf{X}),T_{sup}(\\mathbf{X})))=1-\\alpha\\]\nNivel de Confianza: Es la probabilidad de que el intervalo de confianza contenga el verdadero valor del parámetro poblacional. Se denota como \\(1 - \\alpha\\), donde \\(\\alpha\\) es el nivel de significancia. Un nivel de confianza común es el \\(95\\%\\), lo que significa que estamos un \\(95\\%\\) seguros de que el intervalo contiene el parámetro verdadero. Si repetimos el experimento \\(N\\) veces, en el \\(95\\%\\) de las ocasiones el verdadero valor del parámetro estará incluido en el intervalo proporcionado. Sin embargo es importante señalar que, dado que el experimento solo suele realizarse en una ocasión, no podemos estar seguros de que el verdadero valor del parámetro está incluido en nuestro intervalo. Estará incluido o no estará incluido, pero no podemos saber en qué situación nos encontramos. Estar seguro sería tanto como decir que conocemos el verdadero valor del parámetro. En ese caso, obviamente, no necesitaríamos estimación ninguna.\nError Estándar (SE): Es una medida de la variabilidad de un estimador. Se utiliza para calcular los límites del intervalo de confianza.\n\n\n3.6.2 Cálculo del Intervalo de Confianza\nEl cálculo de un intervalo de confianza generalmente sigue la fórmula:\n\\[\n\\text{Estimación Puntual} \\pm (\\text{Valor Crítico} \\times \\text{Error Estándar})\n\\] Para alcanzar el intervalo de confianza, generalmente se busca una cantidad (aleatoria) \\(C(\\mathbf{X},\\theta)\\) relacionada con el parámetro desconocido \\(\\theta\\) y con la muestras \\(\\mathbf{X}\\), cuya distribución sea conocida y no dependa del valor del parámetro. Esta cantidad recibe el nombre de pivote o cantidad pivotal para \\(\\theta\\).\nDado que conocemos la distribución del pivote, podemos usar los cuartiles \\(1-\\alpha/2\\) y \\(\\alpha/2\\) de dicha distribución, y la desviación estándar dle estimador por intervalos de confianza, para plantear la siguiente ecuación: \\[\nP(1-\\alpha/2 \\text{ cuantil}&lt; C(\\mathbf{X},\\theta)&lt;\\alpha/2 \\text{ cuantil}) = 1- \\alpha\n\\]Para obtener los extremos (inferior y superior) del estimador por intervalos de confianza \\(T_{inf}(\\mathbf{X})\\) y \\(T_{sup}(\\mathbf{X})\\), se resuelve la doble desigualdad en \\(\\theta\\). De este modo el intervalo de confianza al \\(100(1-\\alpha)\\%\\) para \\(\\theta\\) es \\((T_{inf}(\\mathbf{x}),T_{sup}(\\mathbf{x}))\\)\n\n\n3.6.3 Importancia de la estimación por intervalos\nA diferencia de la estimación puntual, el intervalo de confianza proporciona información sobre la precisión de la estimación y la variabilidad inherente en los datos muestrales.\nAdemás, la estimación por intervalo proporciona un rango de valores que es útil para la toma de decisiones en el dominio de aplicación.\nPodemos señalar que la estimación por intervalos es menos susceptible a errores muestrales y proporciona una medida más realista del parámetro poblacional que la obtenida con la estimación puntual.\n\n\n3.6.4 Intervalo de Confianza para la media (cuando la varianza es conocida)\nSea una muestra aleatoria simple \\(\\mathbf{X}\\) de tamaño \\(n\\) obtenida de \\(X\\). Supongamos que \\(X\\) sigue una distribución Normal con parámetros (\\(\\mu\\)) y varianza conocida (\\(\\sigma^2\\)). Fijate que este último supuesto es muy poco realista (no conocemos la media, pero conocemos la varianza). En este caso, el estadístico \\(\\bar{X}\\) tiene una distribución normal: \\[\n\\bar{X} \\sim N \\left( \\mu,\\sigma_{\\bar{X}}=\\frac{\\sigma}{\\sqrt{n}}\\right )\n\\]La desviación típica de \\(\\bar{X}\\) ( o de cualquier otro estadístico) se conoce como su error estándar.\nLa cantidad pivotal para \\(\\mu\\) es: \\[\nZ=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim N \\left( 0,1 \\right )\n\\] Ahora, si \\(z_{1-\\alpha/2}\\) y \\(z_{\\alpha/2}\\) son los cuartiles \\((1-\\alpha/2)\\) y \\(\\alpha/2\\) de la distribución \\(N(0,1\\), entonces tenemos: \\[\nP(z_{1-\\alpha/2}&lt;Z&lt;z_{\\alpha/2})=1-\\alpha\n\\] Es decir: \\[\nP\\left (z_{1-\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}&lt;z_{\\alpha/2} \\right)=1-\\alpha\n\\] Hay que notar que para la distribución Normal: \\(z_{1-\\alpha/2}=-z_{\\alpha/2}\\)\nResolvemos la doble desigualdad para \\(\\mu\\): \\[\n-z_{\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}&lt;z_{\\alpha/2}\n\\] \\[\n-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}&lt;\\bar{X}-\\mu&lt;z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\] \\[ -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}-\\bar{X}&lt;-\\mu&lt;-\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\] \\[ -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}+\\bar{X}&gt;\\mu&gt;\\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\n\\] De modo que el estimador por intervalos de confianza es: \\[\n\\left ( \\bar{X}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}},\\bar{X}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right)\n\\] y por tanto, el intervalo de confianza para la media se calcula como: \\[     \nIC_{1-\\alpha}(\\mu)=\\left (\\bar{x} -z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}},\\bar{x} +z_{\\alpha/2}  \\frac{\\sigma}{\\sqrt{n}} \\right )= \\left( \\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\right)\n\\] donde \\(\\bar{x}\\) es la media muestral, \\(z_{\\alpha/2}\\) es el valor crítico del estadístico \\(z\\) para el nivel de confianza deseado, \\(\\sigma\\) es la desviación estándar poblacional, y \\(n\\) es el tamaño de la muestra.\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para la media de una distribución normal\n\n\n\n\n\nSe ha probado que la altura de los alumnas de primer curso de la URJC se puede aproximar mediante una variable aleatoria con distribución normal con desviación típica \\(\\sigma=10\\) cm pero la media \\((\\mu)\\) desconocida. En un estudio con \\(50\\) alumnas se obtiene una media de \\(166\\) cm. Vamos a construir un intervalo de confianza al \\(95\\%\\) para \\(\\mu\\).\nSea \\(X\\) la altura, y sabemos que las variables independientes y identicamente distribuidas: \\[\nX_1,X_2,\\ldots,X_{50}\\sim N(\\mu,\\sigma^2=10^2).\n\\] Dado que: \\[\n\\frac{\\sigma^2}{n}=\\frac{10^2}{50}=2,\n\\] sabemos que: \\[\n\\bar{X}\\sim N(\\mu,2),\n\\] y por tanto: \\[\n\\frac{\\bar{X}-\\mu}{\\sqrt{2}}\\sim N(0,1).\n\\] Además los cuartiles de la distribución normal nos dicen que si \\(Z\\sim N(0,1)\\), entonces: \\[\nP(-1,96&lt;Z&lt;1,96)=0,95.\n\\] Por tanto: \\[\nP\\left(-1,96&lt;\\frac{\\bar{X}-\\mu}{\\sqrt{2}}&lt;1,96\\right)=0,95.\n\\] Despejamos \\(\\mu\\):\n\\[\nP\\left(\\bar{X}-1,96\\sqrt{2}&lt;\\mu&lt;\\bar{X}+1,96\\sqrt{2}\\right)=0,95.\n\\] Por tanto, si \\(\\bar{x}\\) es una realización particular de la variable aleatoria \\(\\bar{X}\\) en la muestra observada, el intervalo de confianza al \\(95\\%\\) será: \\[\nIC_{0.5}(\\mu)=\\bar{x}\\pm1.96 \\sqrt{2} = \\bar{x}\\pm2.77\n\\] En nuestro caso particular como la media era \\(\\bar{x}=166\\) cm, tenemos: \\[\nIC_{0.5}(\\mu)= 166\\pm2.77=(163.23 , 168.77)\n\\]\n\n\n\n\n\n3.6.5 Intervalo de Confianza para la media (cuando la varianza es desconocida)\nSea una muestra aleatoria simple \\(\\mathbf{X}\\) de tamaño \\(n\\) obtenida de \\(X\\). Supongamos que \\(X\\) sigue una distribución Normal con parámetros (\\(\\mu\\)) y varianza desconocida (\\(\\sigma^2\\)). Este supuesto es más realista que el caso anterior. Lo habitual es no disponer de información sobre la varianza poblacional.\nLa cantidad pivotal para \\(\\mu\\) es: \\[\nT=\\frac{\\bar{X}-\\mu}{s/\\sqrt{n}}\\sim t_{n-1}\n\\] donde \\(s^2\\) es la es la cuasi-varianza muestral: \\(s^2=\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\) y \\(t_n\\) es la distribución \\(t\\) de Student con \\(n\\) grados de libertad.\n\n\n\n\n\n\nRepaso\n\n\n\nEs posible que hayas estudiado la distribución \\(t\\) de Student en la asignatura de Probabilidad del primer curso del grado en Ciencia e Ingeniería de datos. En cualquier caso, repasamos: Si \\(T\\sim t_n\\) entonces: \\[\nE[T]=0\n\\] y \\[\nVar[T]=\\frac{n}{n-2}\n\\]\n\n\nSi \\(t_{n-1;1-\\alpha/2}\\) y \\(t_{n-1;\\alpha/2}\\) son los cuantiles \\((1-\\alpha/2)\\) y \\((\\alpha/2)\\) respectivamente de una distribución \\(t\\) de Student con \\(n-1\\) grados de libertad: \\[\nP(t_{n-1;1-\\alpha/2}&lt;T&lt;t_{n-1;\\alpha/2})=1-\\alpha\n\\] Es decir: \\[\nP(-t_{n-1;\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{s/\\sqrt{n}}&lt;t_{n-1;\\alpha/2})=1-\\alpha\n\\] Se resuelve la doble desigualdad para \\(\\mu\\) y se obtiene el estimador por intervalos de confianza: \\[\n\\left ( \\bar{X}-t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}}, \\bar{X}+t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}} \\right )\n\\] Resultando el intervalo de confianza: \\[\nIC_{1-\\alpha}(\\mu) = \\left ( \\bar{\\mathbf{x}}-t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}}, \\bar{\\mathbf{x}}+t_{n-1;\\alpha/2}\\frac{s}{\\sqrt{n}} \\right )\n\\]\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para la media de una distribución normal con varianza desconocida\n\n\n\n\n\nSe ha medido la temperatura media de una muestra aleatoria de \\(10\\) soluciones salinas, obteniendo los siguiente resultados:\n\n\n [1] 37.2 34.1 35.5 34.5 32.9 37.3 32.0 33.1 42.0 34.8\n\n\nSe nos mide calcular el IC al \\(90\\%\\) para la temperatura media, suponiendo que la temperatura de la solución salina se puede aproximar mediante una variable aleatoria con distribución normal.\nVemos que la población a estudiar es “X=temperatura de una solución salina” donde \\(X \\sim N(\\mu,\\sigma^2)\\) con \\(\\sigma^{2}\\) desconocida.\nTenemos: \\(n=10\\), \\(\\bar{x}=\\) 35.3, \\(s^2=\\) 8.5, \\(s=\\) 2.9. Además: \\(t_{n-1;\\alpha/2}=t_{9;0.05}=1.83\\).\nPor tanto el intervalo de confianza que buscamos es: \\[\nIC_{0.9}(\\mu)=\\left (35.3\\pm 1.83\\frac{2.91}{\\sqrt{10}} \\right )=(35.3\\pm 1.68)=(33.62;36.98)\n\\]\n\n\n\n\n\n3.6.6 Media poblacional para muestras grandes\nSea \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\) una muestra aleatoria simple de tamaño \\(n\\) de una variable aleatoria \\(X\\). Supongamos que \\(X\\) sigue una distribución (conocida o no) con parámetros \\(\\mu\\) y \\(\\sigma^2\\). Además, supongamos que \\(n \\geq 30\\). Entonces, por el Teorema Central del Límite se tiene que la cantidad pivotal para \\(\\mu\\) cumple la siguiente propiedad:\n\\[\nZ=\\frac{\\bar{X}-\\mu}{\\hat{\\sigma}/\\sqrt{n}}\\sim  N(0,1)\n\\] Si \\(z_{1-\\alpha/2}\\) y \\(z_{\\alpha/2}\\) son los cuantiles \\((1-\\alpha/2)\\) y \\(\\alpha/2\\) de \\(N(0,1)\\), tenemos: \\[\nP(z_{1-\\alpha/2}&lt;Z&lt;z_{\\alpha/2})=1-\\alpha\n\\]\nY así, tenemos la condición: \\[\nP\\left ( -z_{\\alpha/2}&lt;\\frac{\\bar{X}-\\mu}{\\hat{\\sigma}/\\sqrt{n}}&lt;z_{\\alpha/2} \\right )=1-\\alpha\n\\] Obtenemos el estimador por intervalos de confianza resolviendo la doble desigualdad para \\(\\mu\\): \\[\n\\left ( \\bar{X}-z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}}, \\bar{X}+z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}} \\right )\n\\] El intervalo de confianza es: \\[\nIC_{1-\\alpha}(\\mu)=\\left ( \\bar{\\mathbf{x}}-z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}}, \\bar{\\mathbf{x}}+z_{\\alpha/2}\\frac{\\hat{\\sigma}}{\\sqrt{n}} \\right )\n\\]\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para media de muestras grandes\n\n\n\n\n\nSupongamos que estamos interesados en estimar la media del tiempo diario que las personas pasan en redes sociales en la URJC. Hemos tomado una muestra aleatoria de \\(200\\) estudiantes y medido el tiempo que pasan en redes sociales. Los resultados muestran una media muestral \\(\\bar{x}\\) de \\(2.5\\) horas al día con una desviación estándar muestral \\(s\\) de \\(0.8\\) horas. Es decir, de \\(2\\) horas y \\(30\\) minutos.\nQueremos calcular un intervalo de confianza del \\(95\\%\\) para la media del tiempo que la población pasa en redes sociales.\nCalculamos el error estándar de la media (SE):\n\\[\nSE = \\frac{s}{\\sqrt{n}}= \\frac{0.8}{\\sqrt{200}} \\approx 0.0566\n\\]\nDe este modo el IC al \\(95\\%\\) queda como sigue: \\[\nIC_{0.95}(\\mu)=\\left ( 2.5\\pm1.96 * 0.0566\\right) =(2.389, 2.611)\n\\]\nCon un \\(95\\%\\) de confianza, podemos decir que la media del tiempo diario que las personas pasan en redes sociales en la población está entre \\(2.389\\) y \\(2.611\\) horas. Esto es, entre \\(2\\) horas y \\(23\\) minutos y \\(2\\) horas y \\(37\\) minutos, aproximadamente.\n\n\n\n\n\n3.6.7 Intervalo de Confianza para la proporción\nSea \\(\\mathbf{X}=(X_1,\\ldots,X_n)\\) una muestra aleatoria simple de tamaño \\(n\\) de una variable aleatoria \\(X\\). Supongamos que \\(X\\) sigue una distribución de Bernoulli con parámetro \\(p\\). Esto es: \\[\n\\mu=E[X]=p\n\\] y \\[\n\\sigma^2=Var[X]=p(1-p)\n\\] Además, supongamos que \\(n \\geq 30\\). Entonces, por el Teorema Central del Límite se tiene que la cantidad pivotal para \\(\\hat{p}=\\bar{X}\\) cumple la siguiente propiedad:\n\\[\nZ=\\frac{\\hat{p}-p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}\\sim  N(0,1)\n\\] EL intervalo de confianza para estimar una proporción poblacional (\\(p\\)) es: \\[\nIC_{1-\\alpha}(p)=\\left ( \\hat{p} \\pm z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} \\right )\n\\]\n\n\n\n\n\n\nEjemplo. Intervalo de confianza para proporción\n\n\n\n\n\nSupongamos que estamos realizando una encuesta para determinar la proporción de personas que apoyan una nueva política ambiental en una ciudad. Hemos encuestado a \\(1000\\) personas, y \\(560\\) de ellas han respondido que apoyan la nueva política.\nEs decir, la proporción muestral es: \\(\\hat{p} = \\frac{560}{1000} = 0.56\\)\nQueremos calcular un intervalo de confianza del \\(95\\%\\) para la proporción de apoyo en toda la población.\nEl error estándar para la proporción es:\n\\[\nSE = \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}=\\sqrt{\\frac{0.56 \\times (1 - 0.56)}{1000}}  \\approx 0.016\n\\]\nObteniendo: \\[\nIC_{0.95}(p)= 56 \\pm 1.96 * 0.016 =  (0.529, 0.591)\n\\]\nCon un \\(95\\%\\) de confianza, podemos decir que la proporción de personas en la población que apoyan la nueva política ambiental está entre el \\(52.9\\%\\) y el \\(59.1\\%\\).\n\n\n\n\n\n3.6.8 Interpretación de los intervalos de confianza\nSi calculamos un intervalo de confianza del \\(95\\%\\) para la media poblacional y, por ejemplo, obtenemos un intervalo de \\((5, 10)\\), esto no significa que hay un \\(95\\%\\) de probabilidad de que la media poblacional esté en ese intervalo en un caso particular, sino que, si repetimos este procedimiento muchas veces, el \\(95\\%\\) de los intervalos construidos contendrán la verdadera media poblacional. Podríamos decir que estamos un \\(95\\%\\) seguros de que la media poblacional se encuentra entre \\(5\\) y \\(10\\), pero ¡ojo!, la media poblacional (cuyo valor desconocemos) estará o no estará en ese intervalo.\n\n\n\n\n\n\nSimulación Intervalo de Confianza\n\n\n\nVamos a realizar un ejercicio de simulación para interpretar correctamente el concepto frecuentista de intervalo de confianza. Para ello, generamos \\(100\\)muestras de tamaño \\(n=50\\) de una distribución \\(X\\sim N(\\mu=10,sigma^2=1)\\). Para cada una de estas muestras, se construye un intervalo de confianza para la media con \\(\\alpha=0.05\\). Y representamos todos esos intervalos de confianza en un único gráfico. En verde se pintan los intervalos de confianza que incluyen el verdadero valor del parámetro \\(10\\). En rojo los que no.\n\n\nClick para ver el código\nset.seed(3983)\nlibrary(dplyr)\nlibrary(ggplot2)\nic=matrix(0,100,5)\nic[,1]=seq(1:100)\nic=as.data.frame(ic)\ncolnames(ic)=c(\"id\",\"estimador\",\"inferior\",\"superior\",\"resultado\")\nic$resultado=\"in\"\nfor (i in 1:100){\n  muestra=rnorm(50,10,1)\n  ic[i,2]=mean(muestra)\n  ic[i,3]=mean(muestra)-1.96*1/sqrt(50)   \n  ic[i,4]=mean(muestra)+1.96*1/sqrt(50) \n}\nic$resultado=!(ic[,3]&gt;10 | ic[,4]&lt;10)\nic %&gt;%\n  ggplot(aes(estimador, id, color = resultado)) +\n  geom_point() +\n  geom_segment(aes(x =inferior, y = id, xend = superior, yend = id, color = resultado))+\n  geom_vline(xintercept = 10, linetype = \"dashed\") +\n  ggtitle(\"Varios Intervalos de Confianza\") +\n  scale_color_manual(values = c(\"#FF3333\", \"#009900\")) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank(), \n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        legend.position = \"none\",\n        plot.title.position = \"plot\")\n\n\n\n\n\n\n\n\n\n\n¿Cuántos intervalos de confianza, de entre los \\(100\\), contienen al verdadero valor del parámetro? Razona ese resultado.\n¿Cuándo se toma una única muestra, cómo podrías estar seguro de estar en uno de los intervalos de confianza que recoge el verdadero valor del parámetro?\n¿Cómo crees que afecta a la longitud del intervalo de confianza los siguientes aspectos:?\n\nTamaño muestral\nNivel de confianza\n\n\nDiscute estas cuestiones con tu profesor.\n\n\n\n\n3.6.9 Determinación del tamaño muestral\nDeterminar el tamaño adecuado de una muestra es crucial en la inferencia estadística, ya que un tamaño muestral adecuado garantiza que los intervalos de confianza sean precisos y que las conclusiones obtenidas sean representativas de la población. Las técnicas para determinar el tamaño muestral están relacionadas directamente con los intervalos de confianza y se basan en varios factores, entre los que se incluyen el nivel de confianza deseado, la precisión (o margen de error) deseada y la variabilidad esperada en la población.\nLos factores clave para determinar el tamaño muestral son:\n\nNivel de confianza (\\(1-\\alpha\\)):\n\nEl nivel de confianza indica el grado de certeza de que el intervalo de confianza contiene el parámetro poblacional. Tal y como hemos indicado anteriormente, niveles de confianza comunes son \\(90\\%\\), \\(95\\%\\) y \\(99\\%\\). Un nivel de confianza más alto requiere una muestra más grande para asegurar la misma precisión.\n\nMargen de error (E):\n\nEl margen de error es la máxima diferencia tolerable entre la estimación muestral y el valor real del parámetro poblacional. Un margen de error más pequeño requiere una muestra más grande para asegurar una estimación precisa.\n\nVariabilidad poblacional (\\(\\sigma\\)):\n\nLa variabilidad en la población, medida por la desviación estándar, afecta directamente al tamaño muestral. Una mayor variabilidad requiere una muestra más grande para obtener una estimación precisa.\n\n\nA continuación mostramos algunos ejemplos del cálculo del tamaño muestral para diferentes situaciones.\n\n3.6.9.1 Tamaño muestral para estimar una media poblacional de una Normal\nEl tamaño muestral \\(n\\) necesario para estimar una media poblacional con un margen de error \\(E\\) y un nivel de confianza \\(1 - \\alpha\\) se puede calcular usando la fórmula:\n\\[\nn = \\left( \\frac{z_{\\alpha/2} \\cdot \\sigma}{E} \\right)^2\n\\]\ndonde:\n\n\\(z_{\\alpha/2}\\) es el valor crítico del estadístico \\(z\\) correspondiente al nivel de confianza deseado.\n\\(\\sigma\\) es la desviación estándar de la población (si es desconocida, se puede usar la desviación estándar de la muestra \\(s\\)).\n\nEfectivamente, teníamos que el intervalo de confianza para la media se obtenía mediante la fórmula: \\[\n\\left ( \\bar{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\right )\n\\] Y buscamos el \\(n\\) tal que la desviación respecto a la media sea menor que \\(E\\), es decir: \\[\nz_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}  &lt; E\n\\] Esto es: \\[\nn &gt;  \\left ( \\frac{z_{\\alpha/2} \\sigma}{\\sqrt{n}} \\right )^2\n\\]\n\n\n\n\n\n\nEjemplo. Tamaño muestral para la estimación de una Media\n\n\n\n\n\nSupongamos que deseamos estimar la media de una población con un nivel de confianza del \\(95\\%\\), un margen de error de \\(5\\) unidades y se estima que la desviación estándar de la población es \\(15\\) unidades. El valor crítico \\(z_{\\alpha/2}\\) para un nivel de confianza del \\(95\\%\\) es aproximadamente \\(1.96\\).\n\\[\nn = \\left( \\frac{1.96 \\cdot 15}{5} \\right)^2 = \\left( \\frac{29.4}{5} \\right)^2 \\approx 34.57\n\\]\nPor lo tanto, necesitamos una muestra de al menos \\(35\\) individuos.\n\n\n\n\n\n3.6.9.2 Tamaño muestral para estimar una proporción poblacional\nEl tamaño muestral \\(n\\) necesario para estimar una proporción poblacional \\(p\\) con un margen de error \\(E\\) y un nivel de confianza \\(1 - \\alpha\\) se puede calcular usando la fórmula:\n\\[\nn = \\frac{z_{\\alpha/2}^2 \\cdot p \\cdot (1 - p)}{E^2}\n\\]\ndonde:\n\n\\(p\\) es la proporción esperada (si no se conoce, se usa \\(p = 0.5\\) para maximizar el tamaño muestral).\n\\(z_{\\alpha/2}\\) es el valor crítico del estadístico \\(z\\) correspondiente al nivel de confianza deseado.\n\n\n\n\n\n\n\nEjemplo. Tamaño muestral para la estimación de una proporción\n\n\n\n\n\nSupongamos que deseamos estimar la proporción de personas que aprueban una nueva ley con un nivel de confianza del \\(95\\%\\), un margen de error del \\(3\\%\\) \\((0.03)\\) y se estima que la proporción esperada es \\(p = 0.5\\). El valor crítico \\(z_{\\alpha/2}\\) para un nivel de confianza del \\(95\\%\\) es aproximadamente \\(1.96\\).\n\\[\nn = \\frac{1.96^2 \\cdot 0.5 \\cdot (1 - 0.5)}{0.03^2} = \\frac{3.8416 \\cdot 0.25}{0.0009} = \\frac{0.9604}{0.0009} \\approx 1067.11\n\\]\nPor lo tanto, necesitamos una muestra de al menos \\(1068\\) individuos.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "para.html#contraste-de-hipótesis",
    "href": "para.html#contraste-de-hipótesis",
    "title": "3  Estimación y contraste paramétrico",
    "section": "3.7 Contraste de hipótesis",
    "text": "3.7 Contraste de hipótesis\nLos contrastes de hipótesis son una herramienta fundamental en la inferencia estadística utilizada para tomar decisiones basadas en datos muestrales. Permiten evaluar si los datos disponibles proporcionan suficiente evidencia en contra de una hipótesis previamente establecida sobre una población.\nEl contraste de hipótesis es un proceso estructurado para evaluar afirmaciones sobre parámetros poblacionales utilizando datos muestrales. Mediante la formulación de hipótesis, selección de niveles de significancia, elección de estadísticas de prueba y evaluación del valor p, podemos tomar decisiones informadas y cuantitativamente justificadas. Este enfoque es fundamental en muchas áreas de investigación y análisis de datos, proporcionando un marco riguroso para la Inferencia Estadística.\n\n3.7.1 Conceptos básicos\nHipótesis Nula \\((H_0)\\):\n\nLa hipótesis nula es una afirmación sobre un parámetro poblacional que se asume verdadera hasta que se presente suficiente evidencia en contra. Se asume inicialmente que la hipótesis nula es correcta (semejante a suponer inocencia en un juicio a menos que se pruebe la culpabilidad). Habitualmente corresponde al estatus quo. Esto es, generalmente, la hipótesis nula representa un estado de “no efecto” o “no diferencia”.\n\nEjemplo: \\((H_0: \\mu = 50)\\) (la media poblacional es \\(50\\)). En este ejemplo, la idea fundamental del contraste sería toma una muestra aleatoria simple de la población, estudiar su media, y ver si hay evidencia suficiente como para rechazar la hipótesis nula establecida. La probabilidad de que la media sea exactamente igual a \\(50\\) en la muestra es muy baja. Es decir, probablemente \\(\\bar{\\mathbf{x}} \\neq 50\\). Sin embargo, lo importante para rechazar la hipótesis nula es si la diferencia encontrada entre la media muestral y \\(50\\) es tan grande como para rechazar que podría ser \\(50\\). Esto va a depender fuertemente del tamaño muestral y de la variabilidad de las observaciones en la muestra.\n\n\nHipótesis Alternativa \\((H_1)\\):\n\nLa hipótesis alternativa es una afirmación que contrasta con la hipótesis nula y representa el efecto o diferencia que se desea detectar.\n\nEjemplo: \\((H_1: \\mu \\neq 50)\\) (la media poblacional no es 50).\nEjemplo: \\((H_1: \\mu \\geq 50)\\) (la media poblacional es mayor o igual que 50).\n\n\n\n\n\n\n\n\nEjemplo \\(H_0\\) vs \\(H_1\\). Media poblacional\n\n\n\n\n\nSupongamos que una empresa de educación en línea afirma que sus estudiantes pasan en promedio al menos \\(4\\) horas diarias estudiando en su plataforma. Queremos comprobar si esta afirmación es cierta basándonos en una muestra de estudiantes.\n\nLa hipótesis nula es la afirmación que queremos poner a prueba y que asumimos verdadera inicialmente. En este caso, la hipótesis nula es que la media del tiempo de estudio diario es de \\(4\\) horas. \\[\nH_0: \\mu \\geq 4 \\text{ horas}\n\\]\nLa hipótesis alternativa es lo que queremos demostrar y se contrapone a la hipótesis nula. En este caso, queremos ver si el tiempo de estudio diario es menor de \\(4\\) horas. Fíjate que la empresa podría estar “inflando” sus resultados y lo “intersante” en este caso es “demostrar” que realmente los alumnos pasan menos tiempo en la plataforma. \\[\nH_1: \\mu &lt; 4 \\text{ horas}\n\\]\n\n\n\n\n\n\n\n\n\n\nEjemplo \\(H_0\\) vs \\(H_1\\). Proporción poblacional\n\n\n\n\n\nSupongamos que el rectorado de la URJC afirma que menos del \\(20\\%\\) de los estudiantes de sus grados, fuman. Queremos verificar si la proporción de fumadores es mayor al \\(20\\%\\).\n\nLa hipótesis nula es la afirmación que queremos poner a prueba y que asumimos verdadera inicialmente. En este caso, la hipótesis nula es que la proporción de fumadores es menor o igual al \\(20\\%\\). \\[\nH_0: p \\leq 0.20\n\\]\nLa hipótesis alternativa es lo que queremos demostrar y se contrapone a la hipótesis nula. En este caso, queremos ver si la proporción de fumadores es mayor al \\(20\\%\\). \\[\nH_1: p &gt; 0.20\n\\]\n\n\n\n\n\n\n3.7.2 Pasos en un Contraste de Hipótesis\n\nFormular las hipótesis:\n\nDefinir \\(H_0\\) y \\(H_1\\) claramente.\n\nSeleccionar el nivel de significatividad estadística \\((\\alpha)\\):\n\nEl nivel de significatividad estadística es la probabilidad de rechazar \\(H_0\\) cuando es verdadera. Comúnmente, se utilizan \\(\\alpha = 0.05\\), \\(\\alpha = 0.01\\), o \\(\\alpha = 0.10\\).\n\nElegir el estadístico de prueba:\n\nSeleccionar un estadístico que siga una distribución conocida bajo \\(H_0\\) (por ejemplo, la distribución Normal, t de Student o Chi-cuadrado).\n\nCalcular el \\(p-valor\\):\n\nEl \\(p-valor\\) es la probabilidad de observar un valor tan extremo o más extremo que el observado, bajo la suposición de que \\(H_0\\) es verdadera. Después volveremos sobre este valor.\n\nTomar una decisión:\n\nLa regla de decisión de un contraste de hipótesis se basa en la “distancia” entre los datos muestrales y los valores esperados si \\(H_0\\) es cierta. Esta distancia se calcula a partir de un estadístico del contraste y se considera “grande” o no, en base a la distribución del mismo y a la probabilidad de observar realizaciones “más extremas” de dicho estadístico. Para tomar la decisión, comparamos el valor p con \\(\\alpha\\):\nSi \\(p-valor \\leq \\alpha\\), se rechaza \\(H_0\\). Hay suficiente evidencia en la muestra como para rechazar la hipótesis nula. El valor del parámetro establecido en \\(H_0\\) es poco creíble dada la muestra observada.\nSi \\(p-valor &gt; \\alpha\\), no se rechaza \\(H_0\\).\n\n\n\n\n\n\n\n\n¡Importante!\n\n\n\nNo rechazar la hipótesis nula no significa que la hipótesis nula sea cierta. La interpretación es que no existe, en la muestra que hemos observado, suficiente evidencia en contra de la hipótesis nula como para recharzarla.\n\n\nTenemos por tanto que el \\(p-valor\\) es una medida que nos dice cuán probable sería obtener nuestros datos observados si la hipótesis nula fuera verdadera. En otras palabras, mide la evidencia en contra \\(H_0\\). Si el \\(p-valor\\) es pequeño (generalmente menor que \\(0.05\\)), tenemos razones para rechazar \\(H_0\\). Si es grande, no tenemos suficiente evidencia para rechazarla.\n\n\n\n\n\n\nEjemplo Práctico. Contraste de Hipótesis\n\n\n\n\n\nSupongamos que una empresa afirma que el tiempo promedio de espera en su servicio al cliente es de \\(10\\) minutos. Queremos probar esta afirmación con una muestra de \\(30\\) clientes que tienen un tiempo promedio de espera de \\(12\\) minutos y una desviación estándar de \\(3\\) minutos.\nFormulamos las hipótesis: - \\(H_0: \\mu = 10\\) - \\(H_1: \\mu \\neq 10\\)\nSeleccionamos el nivel de significancia estadística deseado: \\(\\alpha = 0.05\\).\nElegimos el estadístico de prueba: usamos una prueba \\(t\\) (dado que la muestra es pequeña y no conocemos la desviación estándar poblacional) y calculamos su valor:\n\\[\n   t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}} = \\frac{12 - 10}{3 / \\sqrt{30}} \\approx \\frac{2}{0.5477} \\approx 3.65\n\\]\nA continuación, obtenemos el \\(p-valor\\) correspondiente al estadístico para la distribución \\(t\\) de Studento con \\(29\\) grados de libertad. Para \\(t = 3.65\\), el \\(p-valor\\) es menor que \\(0.001\\).\nDado que \\(p &lt; 0.05\\), rechazamos la hipótesis nula \\(H_0\\). Podemos afirmar que los resultados muestrales son “demasiado extraños” para aceptar la hipótesis nula.\n\n\n\n\n\n3.7.3 Errores tipo I y tipo II. Potencia\nComo hemos visto, una vez especificadas las hipótesis nula y alternativa y recogida la información muestral, se toma una decisión sobre la hipótesis nula (rechazar o no rechazar \\(H_0\\)). Sin embargo, existe la posibilidad de llegar a una conclusión equivocada, porque solo se dispone de una muestra aleatoria y no se puede tener la certeza de que \\(H_0\\) sea correcta o no.\nEn la Inferencia Estadística, cuando realizamos un contraste de hipótesis, hay dos tipos de errores que pueden ocurrir: el error de tipo I o \\(\\alpha\\) y el error de tipo II o \\(\\beta\\). Entender estos errores es fundamental para interpretar correctamente los resultados de cualquier prueba estadística. El balance entre \\(\\alpha\\) y \\(\\beta\\), así como el tamaño de la muestra, juegan un papel importante en la fiabilidad de los resultados obtenidos.\n\n3.7.3.1 Error de Tipo I \\((\\alpha)\\)\nEl error de tipo I ocurre cuando rechazamos la hipótesis nula \\(H_0\\) siendo esta verdadera. En otras palabras, concluimos que hay un efecto o una diferencia cuando, en realidad, no la hay. El nivel de significancia \\(\\alpha\\) es la probabilidad de cometer un error de tipo I.\n\\[\n\\alpha=P(\\text{rechazar } H_0 \\text{ | } H_0 \\text{ es correcta})\n\\]\nRecuerda que establecemos de antemano esta valor, comúnmente \\(0.05\\), \\(0.01\\) o \\(0.10\\). Si el \\(p-valor\\) de nuestra prueba es menor o igual a \\(\\alpha\\), rechazamos \\(H_0\\).\nAsí, por ejemplo, Si \\(\\alpha = 0.05\\), esto significa que estamos dispuestos a aceptar un \\(5\\%\\) de probabilidad de rechazar \\(H_0\\) cuando es verdadera.\n\n\n\n\n\n\nAtención\n\n\n\n¿Qué relación existe entre el error de tipo I y el \\(\\%\\) de un IC?\n\n\n\n\n3.7.3.2 Error de Tipo II \\((\\beta)\\)\nEl error de tipo II ocurre cuando no rechazamos la hipótesis nula \\(H_0\\) siendo esta falsa. En otras palabras, concluimos que no hay un efecto o una diferencia cuando, en realidad, sí la hay.\n\\[\n\\beta=P(\\text{No rechazar } H_0 \\text{ | } H_0 \\text{ es incorrecta})\n\\]\nPotencia del test: La potencia de una prueba estadística es la probabilidad de rechazar \\(H_0\\) cuando \\(H_0\\) es falsa. Se calcula como \\(1 - \\beta\\). Una alta potencia es deseable ya que indica una mayor probabilidad de detectar un efecto o diferencia cuando realmente existe.\n\\[\nPotencia=1-\\beta=P(\\text{Rechazar } H_0 \\text{ | } H_1 \\text{ es correcta})\n\\]\nAsí, por ejemplo, si \\(\\beta = 0.20\\), esto significa que hay un \\(20\\%\\) de probabilidad de no rechazar \\(H_0\\) cuando es falsa. La potencia de la prueba sería \\(0.80\\) (o del \\(80\\%\\)). La probabilidad de detectar un efecto cuando relamente existe es del \\(80\\%\\)\n\n\n\n\n\n\nEjemplo Práctico. Errores Tipo I y II\n\n\n\n\n\nSupongamos que estamos evaluando la efectividad de un nuevo medicamento.\n\nHipótesis Nula \\(H_0\\): El medicamento no tiene efecto \\((\\mu = 0)\\).\nHipótesis Alternativa \\(H_1\\): El medicamento tiene un efecto \\((\\mu \\neq 0)\\).\n\n\nError de Tipo I: Si el medicamento no tiene ningún efecto pero el estudio concluye que sí lo tiene, hemos cometido un error de tipo I. Esto podría llevar a la aprobación y uso de un medicamento ineficaz.\nError de Tipo II: Si el medicamento tiene un efecto, pero el estudio concluye que no lo tiene, hemos cometido un error de tipo II. Esto podría llevar a la no aprobación de un medicamento potencialmente beneficioso.\n\n\n\n\n\n\n3.7.3.3 Relación entre errores de tipo I y II\n\nInversamente proporcionales: Reducir \\(\\alpha\\) (haciendo la prueba más conservadora y menos propensa a rechazar \\(H_0\\) generalmente aumenta \\(\\beta\\) (haciendo la prueba más propensa a no detectar un efecto cuando realmente existe), y viceversa. Fíjate que los errores de Tipo I y de Tipo II no se pueden comenter simultáneamente:\n\nEl error de Tipo I solo puede darse si \\(H_0\\) es correcta.\nEl error de Tipo II solo puede darse si \\(H_0\\) es incorrecta.\n\nTamaño de la muestra: Aumentar el tamaño de la muestra puede reducir ambos tipos de errores, incrementando la precisión de la prueba.\n\nLa siguiente tabla refleja la relación entre los dos tipos de errores en relación con la decisión del contraste y la verdadera situación en la población:\n\n\n\n\n\n\n\n\n\nVerdadera situación\n\n\n\n\n\nDecisión\n\\(H_0\\) correcta\n\\(H_0\\) incorrecta\n\n\nNo rechazar \\(H_0\\)\nSin error (\\(1-\\alpha\\))\nError de Tipo II (\\(\\beta\\))\n\n\nRechazar \\(H_0\\)\nError de Tipo I (\\(\\alpha\\))\nSin error (\\(1-\\beta\\)=potencia)\n\n\n\nEs importante notar que, si todo lo demás no cambia, entonces la potencia del contraste disminuye cuando:\n\nLa diferencia entre el valor supuesto para el parámetro y el valor real disminuye.\nLa variabilidad de la población aumenta.\nEl tamaño muestra disminuye.\n\n\n\n\n\n\n\nEjemplo Práctico. Errores Tipo I y II\n\n\n\n\n\nLos errores de tipo I y tipo II son inversamente proporcionales. Al disminuir la probabilidad de cometer un error de tipo I (haciendo la prueba más conservadora), aumentamos la probabilidad de cometer un error de tipo II (haciendo la prueba menos sensible), y viceversa.\nSupongamos que estamos evaluando la efectividad de un nuevo medicamento para reducir la presión arterial. Queremos probar la siguiente hipótesis:\n\nHipótesis Nula \\((H_0)\\): El nuevo medicamento no reduce la presión arterial (\\(\\mu = 0\\)).\nHipótesis Alternativa \\((H_1)\\): El nuevo medicamento reduce la presión arterial \\((\\mu \\neq 0)\\).\n\nInicialmente fijamos el nivel de significancia (\\(\\alpha\\)) en \\(0.05\\). Consideramos un tamaño de muestra inicial de \\(100\\) pacientes.\nEn este caso el error de Tipo I, significa que estamos dispuestos a aceptar un \\(5\\%\\) de probabilidad de concluir que el medicamento es efectivo cuando en realidad no lo es.\nLa probabilidad de error de Tipo II (\\(\\beta\\)) y, por tanto, la pontencia del contraste depende de varios factores, incluidos el tamaño del efecto, el tamaño de la muestra y el nivel de significancia.\nCaso 1: (\\(\\alpha = 0.05\\))\nEn este caso somos bastante conservadores con el riesgo de falso positivo. Supongamos que el poder estadístico de la prueba con \\((\\alpha = 0.05)\\) y una muestra de \\(100\\) es \\(0.80\\), lo que significa que (\\(\\beta = 0.20\\)).\nCaso 2: (\\(\\alpha = 0.01\\))\nAhora somos más conservadores, reduciendo la probabilidad de cometer un error de tipo I. Al ser más conservadores y reducir (\\(\\alpha\\)), la prueba se vuelve menos sensible a detectar el efecto real. Esto aumenta la probabilidad de cometer un error de tipo II, por ejemplo, supongamos que (\\(\\beta\\)) aumenta a \\(0.30\\). Se reduce la potencia del contraste.\nCaso 3: (\\(\\alpha = 0.10\\))\nAhora somos menos conservadores, aumentando la probabilidad de cometer un error de tipo I. Al ser menos conservadores y aumentar (\\(\\alpha\\)), la prueba se vuelve más sensible a detectar el efecto real. Esto disminuye la probabilidad de cometer un error de tipo II, por ejemplo, supongamos que (\\(\\beta\\)) disminuye a 0.10 y, por tanto, aumenta la potencia del contraste.\nEn resumen:\n\n\n\n\\(\\alpha\\)\n\\(\\beta\\)\nPotencia (\\(1-\\beta\\))\n\n\n\n\n0.05\n0.20\n0.80\n\n\n0.01\n0.30\n0.70\n\n\n0.10\n0.10\n0.90\n\n\n\ny como conclusión\n\nCaso 1 (\\(\\alpha = 0.05\\)): Balance estándar entre el riesgo de falso positivo y falso negativo.\nCaso 2 (\\(\\alpha = 0.01\\)): Reducimos el riesgo de falso positivo (\\(\\alpha\\)), pero aumentamos el riesgo de falso negativo (()).\nCaso 3 (\\(\\alpha = 0.10\\)): Aumentamos el riesgo de falso positivo (\\(\\alpha\\)), pero reducimos el riesgo de falso negativo (\\(\\beta\\)).\n\n\n\n\n\n\n\n3.7.4 Contraste para la media de una población normal con varianza conocida\nEl contraste para la media de una población normal con varianza conocida es un procedimiento estadístico utilizado para determinar si la media de una población difiere de un valor específico (hipótesis nula). Sin embargo, como hemos indicado anteriormente, es poco realista pensar que conocemos la varianza de una variable aleatoria en la población.\nEl parámetro de estudio es la media de la variable aleatoria:\n\\[ X \\sim N(\\mu,\\sigma^2) \\]\nEn primer lugar fijamos las hipótesis:\n\n\\(H_0\\): \\(\\mu = \\mu_0\\) (La media de la población es igual a \\(\\mu_0\\))\n\nTenemos varias opciones para la hipótesis alternativa:\n\n\\(H_1\\): \\(\\mu \\neq \\mu_0\\) (Contraste bilateral)\n\\(H_1\\): \\(\\mu &gt; \\mu_0\\) (Contraste unilateral derecho)\n\\(H_1\\): \\(\\mu &lt; \\mu_0\\) (Contraste unilateral izquierdo)\n\nDebemos fijar el nivel de el nivel de significación (\\(\\alpha\\)). Recordemos, \\(\\alpha\\) es la probabilidad de rechazar la hipótesis nula cuando esta es verdadera.\nEl estadístico de prueba se calcula utilizando la distribución Normal estándar (\\(Z\\)), dado que la varianza (\\(\\sigma^2\\)) es conocida. La fórmula para el estadístico de prueba es:\n\\[ Z = \\frac{\\bar{\\mathbf{X}} - \\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0,1) \\]\nDonde \\(\\bar{\\mathbf{X}}\\) es la media poblacional. Calculamos el valor observado del estadístico: \\[ z = \\frac{\\bar{\\mathbf{x}} - \\mu_0}{\\sigma/\\sqrt{n}} \\] donde \\(\\bar{\\mathbf{x}}\\) es la media muestral.\nDespués calculamos el \\(p-valor\\) como sigue:\n\nContraste bilateral: \\(p-valor=P(|Z|\\geq z)\\)\nContraste unilateral derecho: \\(p-valor=P(Z\\geq z)\\)\nContraste unilateral izquierdo: \\(p-valor=P(Z\\leq z)\\)\n\nSi esta probabilidad es menor o igual que el valor de referencia \\(\\alpha\\), entonces rechazamos la hipótesis nula en favor de la alternativa.\nOtra forma práctica de plantear el contraste de hipótesis es determinando el rechazo de \\(H_0\\). Para ello, debemos comparar el valor del estadístico \\(Z\\) con los valores críticos de la distribución Normal estándar.\n\nPara un contraste bilateral (dos colas):\n\nRechaza \\(H_0\\) si \\(|z| &gt; z_{\\alpha/2}\\).\n\nPara un contraste unilateral derecho (una cola):\n\nRechaza \\(H_0\\) si \\(z &gt; z_{\\alpha}\\).\n\nPara un contraste unilateral izquierdo (una cola):\n\nRechaza \\(H_0\\) si \\(z &lt; -z_{\\alpha}\\).\n\n\nAquí, \\(z_{\\alpha}\\) y \\(z_{\\alpha/2}\\) son los valores críticos de la distribución Normal estándar correspondientes al nivel de significación \\(\\alpha\\).\nEs decir, decidimos si rechazamos o no la hipótesis nula del siguiente modo:\n\nSi el valor del estadístico de prueba está en la región crítica, rechaza (H_0).\nSi el valor del estadístico de prueba no está en la región crítica, no rechaces (H_0).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis media normal, varianza conocida\n\n\n\n\n\nSupón que queremos probar si la edad media de los profesores de la URJC es igual a \\(50\\) años con una desviación estándar conocida de \\(10\\) años, y tienes una muestra de \\(36\\) observaciones con una media muestral de \\(52\\).\nFormulamos las hipótesis:\n\n\\(H_0\\): \\(\\mu = 50\\)\n\\(H_1\\): \\(\\mu \\neq 50\\)\n\nFijamos el nivel de significación: \\(\\alpha = 0.05\\).\nCalculamos el estadístico de prueba: \\[ z = \\frac{52 - 50}{10/\\sqrt{36}}  \\approx 1.20 \\]\nPara \\(\\alpha = 0.05\\) en un contraste bilateral, los valores críticos son \\(z_{0.05/2}=\\pm 1.96\\). Como \\(|1.20| &lt; 1.96\\), no tenemos evidencia en la muestra como para rechazar \\(H_0\\).\nSi hubiéramos calculado el \\(p-valor\\): \\[p-valor=P(|Z| \\geq 1.20)=2*P(Z \\geq 1.20)=2*0.115\\approx0.23\\] Como el \\(p-valor\\) es mayor que el nivel de significatividad estadística, no podemos rechazar la hipótesis nula en favor de la alternativa.\n\n\n\n\n\n3.7.5 Contraste para la media de una población normal con varianza desconocida\nEl contraste de hipótesis para la media de una población normal con varianza desconocida es similar al caso con varianza conocida, pero utilizamos la distribución \\(t\\) de Student en lugar de la distribución Normal estándar.\nEn este caso, el cuando la varianza poblacional es desconocida, se utiliza la desviación estándar muestral (\\(s\\)) y el estadístico de prueba se basa en la distribución \\(t\\) de Student con (\\(n - 1\\)) grados de libertad. La fórmula es:\n\\[ T=\\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}} \\]\nDonde \\(\\bar{\\mathbf{X}}\\) es la media poblacional. Calculamos el valor observado del estadístico: \\[ t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}  \\]\nEl p-valor es la probabilidad de obtener un valor del estadístico de prueba al menos tan extremo como el observado, bajo la suposición de que \\(H_0\\) es verdadera. Dependiendo del tipo de contraste, el p-valor se calcula de diferentes formas:\n\nPara un contraste bilateral:\n\np-valor = \\(2 \\cdot P(T_{n-1} &gt; |t|)\\)\n\nPara un contraste unilateral derecho:\n\np-valor = \\(P(T_{n-1} &gt; t)\\)\n\nPara un contraste unilateral izquierdo:\n\np-valor = \\(P(T_{n-1} &lt; t)\\)\n\n\nAquí, \\(T_{n-1}\\) es una variable aleatoria con una distribución \\(t\\) de Student con \\(n - 1\\) grados de libertad.\nLa decisión asociada al contraste es:\n\nSi el \\(p-valor \\leq \\alpha\\), rechazar \\(H_0\\).\nSi el \\(p-valor &gt; \\alpha\\), no rechazar \\(H_0\\).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis media normal, varianza desconocida\n\n\n\n\n\nSupón que una empresa quiere verificar si el tiempo promedio de entrega de sus pedidos es mayor de \\(30\\) minutos. Toma una muestra aleatoria de \\(16\\) entregas y encuentra que el tiempo promedio de entrega es de \\(32\\) minutos con una desviación estándar muestral de \\(4\\) minutos. Realiza un contraste de hipótesis con un nivel de significación del \\(0.05\\) para ver si el tiempo promedio de entrega es mayor de \\(30\\) minutos.\nEn primer lugar establecemos las hipótesis: - \\(H_0\\): $= 30 (El tiempo promedio de entrega es de 30 minutos) - \\(H_1\\): $&gt; 30 (El tiempo promedio de entrega es mayor de 30 minutos)\nCalculamos el estadístico de Prueba \\[ t = \\frac{\\bar{X} - \\mu_0}{s/\\sqrt{n}} = \\frac{32 - 30}{4/\\sqrt{16}} = \\frac{2}{1} = 2.00 \\]\nEstamos ante un contraste unilateral derecho con \\(n - 1 = 16 - 1 = 15\\) grados de libertad. El \\(p-valor\\) es: \\[P(T_{15} &gt; 2.00) \\approx 0.031\\]\nComo el \\(p-valor\\) es menor que el grado de significatividad estadística \\(0.05\\), entonces podemos rechazar la hipótesis nula en favor de la alternativa. Hay suficiente evidencia para rechazar la hipótesis nula y concluir que el tiempo promedio de entrega es mayor de \\(30\\) minutos con un nivel de significación del \\(5\\%\\).\n\n\n\n\n\n3.7.6 Contraste de hipótesis para la igualdad de medias de dos muestras independientes\nCuando se desea comparar las medias de dos muestras independientes asumiendo que los datos siguen una distribución normal, se puede usar el contraste de hipótesis paramétrico conocido como la prueba \\(t\\) de Student para muestras independientes. Este método es robusto y se basa en suposiciones claras acerca de la normalidad de las distribuciones subyacentes.\nSuposiciones:\n\nLas dos muestras son independientes.\nLos datos de cada muestra se distribuyen normalmente. Si las muestras son suficientemente grandes, se puede invocar el Teorema Central del Límite, que establece que la distribución de la media muestral se aproxima a una distribución normal independientemente de la forma de la distribución original.\nLas varianzas poblacionales son desconocidas, pero se pueden asumir iguales para una versión específica del test t (si esta suposición es razonable).\n\nSupongamos m.a.s. independientes con medias, desviaciones típicas y tamaño muestral igual a: \\(\\bar{\\mathbf{x}}_1\\),\\(\\bar{\\mathbf{x}}_2\\),\\(s_1^2\\),\\(s_2^2\\),\\(n_1\\),y \\(n_2\\), respectivamente.\nFormulamos las hipótesis:\n\nHipótesis nula (\\(H_0\\)): Las medias de las dos poblaciones son iguales (\\(\\mu_1=\\mu_2\\)).\nHipótesis alternativa (\\(H_1\\)): Las medias de las dos poblaciones son diferentes (\\(\\mu_1 \\neq \\mu_2\\)).\n\nConsideramos un nivel de significancia estadística \\(\\alpha\\). Típicamente \\(\\alpha=0.05\\).\nCalculamos el estadístico muestras, en este caso: \\[\nt = \\frac{\\bar{\\mathbf{x}}_1-\\bar{\\mathbf{x}}_2}{SE}\n\\] donde: \\[\nSE = \\sqrt{S^2_p \\left( \\frac{1}{n₁} + \\frac{1}{n₂} \\right)}\n\\] siendo \\(S_p\\) la desviación típica combinada: \\[\nS^2_p= \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}\n\\]\nPara un nivel de significancia \\(\\alpha= 0.05\\) y grados de libertad \\(df = n_1+n_2-2\\), buscamos el valor crítico \\(t\\) para la distribución \\(t\\) de Student para una prueba de dos colas.\nComparamos el valor del estadístico \\(t\\) calculado con el valor crítico:\n\nSi \\(|t| &gt; T_{df,1-\\alpha/2}\\), rechazamos \\(H_0\\).\nSi \\(|t| \\leq T_{df,1-\\alpha/2}\\), no rechazamos \\(H_0\\).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis igualdad de medias\n\n\n\n\n\nSupongamos que un investigador quiere comparar la efectividad de dos métodos de enseñanza de matemáticas. Se seleccionan dos grupos de estudiantes al azar, uno para cada método. Después de un semestre, se mide el puntaje de un examen final de matemáticas.\nDatos:\n\nGrupo A (Método 1):\n\nTamaño de la muestra (\\(n_1\\)) = 12\nPuntajes: 85, 78, 92, 88, 75, 84, 90, 91, 83, 79, 87, 86\n\nGrupo B (Método 2):\n\nTamaño de la muestra (\\(n_2\\)) = 10\nPuntajes: 82, 77, 85, 80, 79, 81, 83, 78, 82, 76\n\n\nLas hipótesis del contraste son:\n\nHipótesis nula (\\(H_0\\)): Las medias de las dos poblaciones son iguales (\\(\\mu_1=\\mu_2\\)).\nHipótesis alternativa (\\(H_1\\)): Las medias de las dos poblaciones son diferentes (\\(\\mu_1 \\neq \\mu_2\\)).\n\nConsideramos un nivel de significancia \\(\\alpha=0.05\\).\nPara el Grupo A, se tiene:\n\nTamaño de la muestra (\\(n_1= 12\\))\nMedia (\\(\\bar{\\mathbf{x}}_1 = 85.08\\))\nDesviación estándar (\\(s_1=5.33\\))\n\nPara el Grupo B:\n\nTamaño de la muestra (\\(n_2= 10\\))\nMedia (\\(\\bar{\\mathbf{x}}_2 = 80.3\\))\nDesviación estándar (\\(s_2=2.49\\))\n\nUsaremos la prueba t para muestras independientes. Calculamos la varianza combinada: \\[\nS^2_p=  \\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}=\\frac{(12-1)5.33^2 + (10-1)2.49^2}{12+10-2} \\approx 18.41419\n\\] El erro estándar combinados: \\[\nSE = \\sqrt{S^2_p \\left( \\frac{1}{n₁} + \\frac{1}{n₂} \\right)}= \\sqrt{18.4149 \\left( \\frac{1}{12} + \\frac{1}{10} \\right)}\\approx 1.84\n\\] Para calcular el estadístico \\(t\\): \\[\nt = \\frac{\\bar{\\mathbf{x}}_1-\\bar{\\mathbf{x}}_2}{SE}=\\frac{85.08-80.3}{1.84}\\approx2.60\n\\]\nPara un nivel de significancia \\(\\alpha= 0.05\\) y grados de libertad \\(df = n_1+n_2- 2 = 20\\), el valor crítico t de t de Student para una prueba de dos colas es aproximadamente \\(\\pm2.086\\).\nEn nuestro caso, \\(|t| = 2.60\\), que es mayor que \\(2.086\\) y por tanto, eechazamos la hipótesis nula (\\(H_0\\)). Esto indica que hay evidencia suficiente para afirmar que existe una diferencia significativa en los puntajes de los exámenes de matemáticas entre los dos grupos de estudiantes.\n\n\n\n\n\n3.7.7 Contraste de hipótesis para la diferencia de proporciones\nEl contraste de hipótesis para la diferencia de proporciones se utiliza para determinar si hay una diferencia significativa entre las proporciones de éxito en dos grupos independientes. Supongamos dos variables aleatorias \\(X\\) e \\(Y\\) que siguen una distribución binomial de parámetros \\(p_1\\) y \\(p_2\\) respectivamente.\nFormulamos las hipótesis:\n\nHipótesis nula (\\(H_0\\)): Las proporciones de las dos poblaciones son iguales (\\(p_1=p_2\\)).\nHipótesis alternativa (\\(H_1\\)): Las proporciones de las dos poblaciones son diferentes (\\(p_1 \\neq p_2\\)).\n\nConsideremos dos m.a.s. de tamaño \\(n_1\\) y \\(n_2\\), siendo \\(\\mathbf{x}\\) y \\(\\mathbf{y}\\) el número de observaciones que cumplen un criterio, de modo que: \\[\n\\hat{p}_1=\\frac{\\mathbf{x}}{n_1}, \\hat{p}_2=\\frac{\\mathbf{y}}{n_1}\n\\] son los estimadores de máxima verosimilutd de \\(p_1\\) y \\(p_2\\) respectivamente.\nConsideramos un nivel de significancia estadística \\(\\alpha\\). Típicamente \\(\\alpha=0.05\\).\nCalculamos el estadístico muestral, en este caso: \\[\nZ = \\frac{\\hat{p}_1-\\hat{p_2}}{SE}\n\\]\ndónde \\[\nSE=\\sqrt{\\hat{p}(1-\\hat{p})}\\left ( \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}\\right )\n\\] Donde \\(\\hat{p}=\\frac{\\mathbf{x}+\\mathbf{y}}{n_1+n_2}\\).\nPara valores grandes de \\(n_1\\) y \\(n_2\\), la distribución de \\(Z\\) es Normal de media \\(0\\) y desviación típica \\(1\\).\nPara un nivel de significancia \\(\\alpha= 0.05\\), buscamos el valor crítico \\(Z\\) para la distribución Normal.\nComparamos el valor del estadístico \\(Z\\) calculado con el valor crítico:\n\nSi \\(|Z| &gt; Z_{1-\\alpha/2}\\), rechazamos \\(H_0\\).\nSi \\(|Z| \\leq Z_{1-\\alpha/2}\\), no rechazamos \\(H_0\\).\n\n\n\n\n\n\n\nEjemplo Práctico. Contraste de hipótesis igualdad de proporciones\n\n\n\n\n\nSupongamos que una empresa de marketing quiere evaluar la efectividad de dos campañas publicitarias diferentes (Campaña A y Campaña B) para atraer clientes. La empresa desea saber si hay una diferencia significativa en la proporción de clientes que responden positivamente a cada campaña.\n\nCampaña A:\n\nNúmero de personas que recibieron la campaña: \\(500\\)\nNúmero de personas que respondieron positivamente: \\(75\\)\n\nCampaña B:\n\nNúmero de personas que recibieron la campaña: \\(600\\)\nNúmero de personas que respondieron positivamente: \\(120\\)\n\n\nLas hipótesis son:\n\nHipótesis Nula (\\(H_0\\)): No hay diferencia en la proporción de éxito entre las dos campañas \\((p_1 = p_2)\\).\nHipótesis Alternativa (\\(H_1\\)): Hay una diferencia en la proporción de éxito entre las dos campañas \\((p_1 \\neq p_2)\\).\n\nCalculamos las proporciones muestrales:\n\\[\\hat{p}_1 = \\frac{75}{500} = 0.15, \\hat{p}_2 = \\frac{120}{600} = 0.20\\] calculamos la proporción combinada: \\[  \\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{75 + 120}{500 + 600} = \\frac{195}{1100} = 0.1773\n\\]\nCalcular el Error Estándar de la diferencia de proporciones \\[  SE = \\sqrt{\\hat{p} (1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)} = \\sqrt{0.1733 \\times 0.8227 \\left( \\frac{1}{500} + \\frac{1}{600} \\right)} \\approx 0.0231\n\\]\nA continuación calculamos el estadístico de prueba: \\[z = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE} = \\frac{0.15 - 0.20}{0.0231} \\approx -2.16\\] El \\(p-valor\\) asociado para una prueba bilateral es aproximadamente \\(P(|Z|\\geq 2.16)\\approx 0.031\\).\nDado que el \\(p-valor\\) es menor que el nivel de significancia típico (\\(\\alpha = 0.05\\)), rechazamos la hipótesis nula. Por tanto, hay evidencia suficiente para afirmar que existe una diferencia significativa en las proporciones de éxito entre la Campaña A y la Campaña B.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "anova.html#modelo-con-un-factor",
    "href": "anova.html#modelo-con-un-factor",
    "title": "5  Análisis de varianza",
    "section": "",
    "text": "Hipótesis:\n\nHipótesis Nula (\\(H_0\\)): Todas las medias de los grupos son iguales (\\(\\mu_1 = \\mu_2 = \\cdots = \\mu_k\\)).\nHipótesis Alternativa (\\(H_a\\)): Al menos una de las medias de los grupos es diferente.\n\n\n\n\n\nNormalidad: Las distribuciones de las poblaciones de las que provienen las muestras son normales.\nHomogeneidad de varianzas: Las varianzas de las poblaciones son iguales.\nIndependencia: Las observaciones son independientes entre sí.\n\n\n\n\\(Y_{ij}\\) es la observación \\(j\\)-ésima del grupo \\(i\\)-ésimo.\n\\(\\mu\\) es la media general.\n\\(\\epsilon_{ij}\\) es el término de error aleatorio.\n\\(\\tau_i\\) es el efecto del grupo i-ésimo en la media de la variable respuesta \\(Y\\). Esto es, cuánto aumenta o disminuye la media de \\(Y\\) por pertenecer la observación a la categoría \\(i\\). De modo que podemos llamar \\[\nY_i=\\mu+\\tau_i\n\\] al efecto medio del grupo \\(i\\)-esimo.\n\n\n\n\n\n\n\n\nMSB (Mean Square Between): Media cuadrática entre grupos.\nMSW (Mean Square Within): Media cuadrática dentro de los grupos.\n\n\n\n\\(df_B=k-1\\) son los grados de libertad entre los grupos\n\\(df_W=N-k\\) son los grado sd elibertado dentro de los grupos, siendo \\(N\\) el número total de observaciones.\n\n\n\n\n\n\n\n\n\n\n\nFuente de variación\nSuma de cuadrados\nGrados de libertad\nCuadrado Medio\n\n\n\n\nDisferencias entre grupos\nSSB\nk-1\nMSB\n\n\nDiferencias dentro de los grupos, Residual o Error\nSSW\nN-k\nMSW\n\n\nTotal\nSST\nN-1\n\n\n\n\n\n\n\n\n\n\nPara el futuro\n\n\n\nLa proporción de variabilidad explicada por los grupos se calcula como: \\[\nR^2=1-SSW/SST\n\\] Este valor, será muy importante en la asignatura de Regresión del Grado en Ciencia e Ingeniería de Datos.\n\n\n\n\n\n\n\n\n\n\n\nEjemplo. ANOVA de un factor\n\n\n\n\n\nVamos a realizar un ejemplo completo de ANOVA de un factor, donde calcularemos todos los pasos del contraste, incluidos el valor de F y el p-valor.\nSupongamos que tenemos tres tratamientos (A, B y C) y sus correspondientes muestras de datos son:\n\nGrupo A: \\([5, 7, 6, 9, 6]\\)\nGrupo B: \\([8, 12, 9, 11, 10]\\)\nGrupo C: \\([14, 10, 13, 15, 12]\\)\n\nNuestro objetivo es determinar si existe una diferencia significativa entre las medias de estos tres grupos.\nEn primer lugar calculamos la media de cada grupo y la media general:\n\nMedia de Grupo A (\\(\\bar{Y}_A\\)): \\[\n\\bar{Y}_A = \\frac{5 + 7 + 6 + 9 + 6}{5} = \\frac{33}{5} = 6.6\n\\]\nMedia de Grupo B (\\(\\bar{Y}_B\\)): \\[\n\\bar{Y}_B = \\frac{8 + 12 + 9 + 11 + 10}{5} = \\frac{50}{5} = 10.0\n\\]\nMedia de Grupo C (\\(\\bar{Y}_C\\)): \\[\n\\bar{Y}_C = \\frac{14 + 10 + 13 + 15 + 12}{5} = \\frac{64}{5} = 12.8\n\\]\nMedia General (\\(\\bar{Y}\\)): \\[\n\\bar{Y} = \\frac{6.6 + 10.0 + 12.8}{3} = \\frac{29.4}{3} \\approx 9.8\n\\]\n\nCalculamos la Suma de Cuadrados entre Grupos (SSB)\n\\[\n\\text{SSB} = n_A (\\bar{Y}_A - \\bar{Y})^2 + n_B (\\bar{Y}_B - \\bar{Y})^2 + n_C (\\bar{Y}_C - \\bar{Y})^2\n\\] donde \\(n_A = n_B = n_C = 5\\) (número de observaciones en cada grupo). Fíjate que el número de observaciones en cada grupo podría ser diferente. En este ejemplo, son iguales.\n\\[\n\\text{SSB} = 5 (6.6 - 9.8)^2 + 5 (10.0 - 9.8)^2 + 5 (12.8 - 9.8)^2 =96.4\n\\]\nA continuación calculamos la Suma de Cuadrados Dentro de los Grupos (SSW)\n\\[\n\\text{SSW} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2\n\\]\nPara cada grupo, calculamos la suma de las diferencias al cuadrado entre cada dato y la media del grupo:\n\nGrupo A: \\[\n(5 - 6.6)^2 + (7 - 6.6)^2 + (6 - 6.6)^2 + (9 - 6.6)^2 + (6 - 6.6)^2 = 9.2\n\\]\nGrupo B: \\[\n(8 - 10.0)^2 + (12 - 10.0)^2 + (9 - 10.0)^2 + (11 - 10.0)^2 + (10 - 10.0)^2 = 10.0\n\\]\nGrupo C: \\[\n(14 - 12.8)^2 + (10 - 12.8)^2 + (13 - 12.8)^2 + (15 - 12.8)^2 + (12 - 12.8)^2 = 14.8\n\\]\n\nEntonces, la Suma de Cuadrados Dentro de los Grupos (SSW) es: \\[\n\\text{SSW} = 9.2 + 10.0 + 14.8 = 34.0\n\\]\nTenemos, por tanto que la Suma Total de Cuadrados (SST) es: \\[\n\\text{SST} = \\text{SSB} + \\text{SSW} = 96.4 + 34.0 = 130.4\n\\]\nPara realizar el contraste, necesitamos calcular los Grados de Libertad del estadístico:\n\nGrados de libertad entre los grupos (\\(\\text{df}_B = k-1=3-1=2\\)):\nGrados de libertad dentro de los grupos (\\(\\text{df}_W=N-k=15-3=12\\)):\n\nCalcularemos las varianzas como sigue:\n\nVarianza entre los grupos (MSB): \\[\n\\text{MSB} = \\frac{\\text{SSB}}{\\text{df}_B} = \\frac{96.4}{2} = 48.2\n\\]\nVarianza dentro de los grupos (MSW): \\[\n\\text{MSW} = \\frac{\\text{SSW}}{\\text{df}_W} = \\frac{34.0}{12} \\approx 2.8333\n\\]\n\nYa estamos en disposición de calcular el estadístico de contraste : \\[\nF = \\frac{\\text{MSB}}{\\text{MSW}} = \\frac{48.2}{2.8333} \\approx 17.01\n\\]\nEl p-valor se obtiene utilizando la distribución \\(F\\) con \\(\\text{df}_B = 2\\) y \\(\\text{df}_W = 12\\). Para este ejemplo, podemos utilizar software estadístico o tablas de distribución F.\nUsando R:\n\npf(17.01, df1 = 2, df2 = 12, lower.tail = FALSE)\n\n[1] 0.0003143459\n\n\nEl p-valor es muy pequeño, mucho menor que el grado de significancia \\(\\alpha=0.05\\), indicando una diferencia significativa entre los grupos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "anova.html#modelo-con-dos-factores-con-y-sin-interacción",
    "href": "anova.html#modelo-con-dos-factores-con-y-sin-interacción",
    "title": "5  Análisis de varianza",
    "section": "5.2 Modelo con dos factores con y sin Interacción",
    "text": "5.2 Modelo con dos factores con y sin Interacción\nEl modelo de ANOVA de dos factores se utiliza cuando se estudian dos factores simultáneamente para evaluar su efecto individual y conjunto en una variable dependiente. Podemos verlo como una generalización del caso de ANOVA con un único factor. Este modelo es más complejo y permite entender no solo los efectos principales de cada factor, sino también si hay una interacción entre ellos.\nSean A y B dos factores que se desean estudiar, con \\(m_A\\) y \\(m_B\\) niveles. Trabajaremos con las siguientes hipótesis nulas:\n\nHipótesis:\n\nHipótesis Nula para los efectos principales (\\(H_0\\)):\n\nNo hay efecto del primer factor.\nNo hay efecto del segundo factor.\n\nHipótesis Nula para la interacción (\\(H_0\\)): No hay interacción entre los dos factores.\n\n\nModelo sin Interacción:\n\\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\epsilon_{ijk} \\]\nDonde:\n\n\\(Y_{ijk}\\) es la observación \\(k\\)-ésima del nivel \\(j\\)-ésimo del factor B y nivel \\(i\\)-ésimo del factor A. - \\(\\mu\\) es la media general.\n\\(\\alpha_i\\) es el efecto del nivel \\(i\\)-ésimo del factor A.\n\\(\\beta_j\\) es el efecto del nivel \\(j\\)-ésimo del factor B. - \\(\\epsilon_{ijk}\\) es el término de error aleatorio.\n\nEn este caso, la tabla ANOVA queda como sigue:\n\n\n\n\n\n\n\n\n\nFuente de variación\nSuma de cuadrados\nGrados de libertad\nCuadrado Medio\n\n\n\n\nDiferencias entre niveles del factor A\n\\(SSB_A\\)\n\\(m_A-1\\)\n\\(MSB_A\\)\n\n\nDiferencias entre niveles del factor B\n\\(SSB_B\\)\n\\(m_B-1\\)\n\\(MSB_B\\)\n\n\nError\n\\(SSW\\)\n\\(N-m_A-m_B+1\\)\n\\(MSW\\)\n\n\nTotal\n\\(SST\\)\n\\(N-1\\)\n\n\n\n\nPara estudiar la importancia de cada factor se calcula el estadístico \\(F\\) particular para cada uno de ellos como sigue: \\[\nF_A=\\frac{MSB_A}{MSW} \\sim F_{m_A-1,N-m_A-m_B+1}\n\\] y \\[\nF_B=\\frac{MSB_B}{MSW}\\sim F_{m_B-1,N-m_A-m_B+1}\n\\] A partir de estos estadísticos de prueba podemos contrastar las hipótesis nulas de no existencia de efectos asociados a los factores \\(A\\) y \\(B\\) respectivamente.\nModelo con Interacción: \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk} \\]\nDonde: \\((\\alpha\\beta)_{ij}\\) representa el efecto de interacción entre el nivel \\(i\\)-ésimo del factor A y el nivel \\(j\\)-ésimo del factor B.\nEn este caso, la tabla ANOVA añade el factor de interacción:\n\n\n\n\n\n\n\n\n\nFuente de variación\nSuma de cuadrados\nGrados de libertad\nCuadrado Medio\n\n\n\n\nDiferencias entre niveles del factor A\n\\(SSB_A\\)\n\\(m_A-1\\)\n\\(MSB_A\\)\n\n\nDiferencias entre niveles del factor B\n\\(SSB_B\\)\n\\(m_B-1\\)\n\\(MSB_B\\)\n\n\nDiferencias debidas la interacción\n\\(SSB_{AB}\\)\n\\((m_A-1)*(m_B-1)\\)\n\\(MSB_{AB}\\)\n\n\nError\n\\(SSW\\)\n\\(N-m_A*m_B\\)\n\\(MSW\\)\n\n\nTotal\n\\(SST\\)\n\\(N-1\\)\n\n\n\n\nPara estudiar la importancia de cada de la interacción se calculan el estadístico \\(F\\) correspondiente:\n\\[\nF_{AB}=\\frac{MSB_{AB}}{MSW}\\sim F_{(m_A-1)*(m_B-1),N-m_A*m_B}\n\\]\nA partir de este estadísticos de prueba podemos contrastar la hipótesis nula de no existencia de interacción entre los dos factores \\(A\\), \\(B\\). Si podemos rechazar esa hipótesis, es decir, si existe interacción entre los factores, entonces hemos terminado. Es decir, no podemos eliminar ningún factor del modelo. En cambio, si no rechazamos la hipótesis nula, es decir, si no existe interacción entre los factores, podemos eliminar dicho efecto (la interacción) del modelo y pasar a un modelos sin interacción como el anteriormente descrito.\n\n\n\n\n\n\nEjemplo. ANOVA de dos factores con interacción\n\n\n\n\n\nSupongamos que estamos estudiando el efecto de dos factores sobre el rendimiento de los estudiantes: el método de enseñanza (con dos niveles: Tradicional y Experimental) y el tipo de material de estudio (con tres niveles: Libro, Video, y Online). Queremos saber si estos factores, y su posible interacción, tienen un efecto significativo en el rendimiento.\n\nrendimiento &lt;- c(85, 88, 90, 83, 87, 85, 86, 89, 91, 84, 88, 87,\n                 78, 79, 80, 76, 77, 75, 78, 79, 81, 76, 77, 76,\n                 90, 92, 91, 93, 90, 89, 91, 92, 91, 93, 92, 91)\nmetodo &lt;- factor(rep(c(\"Tradicional\", \"Experimental\"), each=18))\nmaterial &lt;- factor(rep(c(\"Libro\", \"Video\", \"Online\"), each=6, times=2))\n\n# Crear un data frame\ndatos &lt;- data.frame(rendimiento, metodo, material)\n\n# Visualizar los datos\nlibrary(\"ggpubr\")\nggline(datos, x = \"material\", y = \"rendimiento\", color = \"metodo\",\n       add = c(\"mean_se\", \"dotplot\"),\n       palette = c(\"#00AFBB\", \"#E7B800\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nAhora que tenemos nuestros datos, vamos a realizar el ANOVA de dos factores con interacción.\n\n# Realizar el ANOVA de dos factores con interacción\nanova_result &lt;- aov(rendimiento ~ metodo * material, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nmetodo           1   81.0    81.0   21.83 5.88e-05 ***\nmaterial         2  309.7   154.9   41.73 2.16e-09 ***\nmetodo:material  2  771.2   385.6  103.90 3.26e-14 ***\nResiduals       30  111.3     3.7                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLos resultados del ANOVA se interpretan observando los \\(p-valores\\) para cada uno de los componentes del modelo:\n\nmetodo: Efecto principal del método de enseñanza.\nmaterial: Efecto principal del tipo de material de estudio.\nmetodo:material: Interacción entre el método de enseñanza y el tipo de material de estudio.\n\nEl \\(p-valor\\) asociado a la interacción es menor que \\(0.05\\), lo que indica que la interacción entre el método de enseñanza y el tipo de material de estudio es significativa y por lo tanto no debe ser eliminada del modelo.\nEfectivamente, viendo la figura anterior concluimos que el rendimiento depende de la interacción entre los dos factores. Así, por ejemplo, cuando el matería es proporcionado de modo “online” o en “vídeo” el rendimiento es más elevado en el método experimental que en el método tradicional. La diferencia entre los dos métodos es especialmente notable cuando el material es “online”. Sin embargo, cuando el material se ofrece en modo “libro” el método tradicional ofrece mejores resultados que el método experimental.\n\n\n\n\n\n\n\n\n\nEjemplo. ANOVA de dos factores no significativos\n\n\n\n\n\nSupongamos que estamos estudiando el efecto del tipo de fertilizante (con dos niveles: \\(A\\) y \\(B\\)) y el tipo de riego (con tres niveles: Goteo, Aspersión, Manual) sobre el crecimiento de las plantas.\nEn primer lugar observamos los datos:\n\n# Crear datos simulados\ncrecimiento &lt;- c(20, 21, 23, 22, 24, 25, 26, 27, 28, 29, 30, 31,\n                 22, 23, 25, 24, 26, 27, 28, 29, 30, 31, 32, 33,\n                 21, 23, 22, 24, 23, 25, 26, 28, 27, 29, 28, 30)\nfertilizante &lt;- factor(rep(c(\"A\", \"B\"), each=18))\nriego &lt;- factor(rep(c(\"Goteo\", \"Aspersión\", \"Manual\"), each=6, times=2))\n\n# Crear un data frame\ndatos &lt;- data.frame(crecimiento, fertilizante, riego)\n\n# Visualizar los datos\nlibrary(\"ggpubr\")\nggline(datos, x = \"riego\", y = \"crecimiento\", color = \"fertilizante\",\n       add = c(\"mean_se\", \"dotplot\"),\n       palette = c(\"#00AFBB\", \"#E7B800\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nRealizamos el ANOVA de dos factores con interacción.\n\n# Realizar el ANOVA de dos factores con interacción\nanova_result &lt;- aov(crecimiento ~ fertilizante * riego, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n                   Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfertilizante        1   36.0   36.00  12.000  0.00162 ** \nriego               2    3.5    1.75   0.583  0.56424    \nfertilizante:riego  2  283.5  141.75  47.250 5.36e-10 ***\nResiduals          30   90.0    3.00                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEl \\(p-valor\\) asociado a la interaccióni es mayor que \\(0.05\\) lo, lo que indica que la interacción entre el tipo de fertilizante y el tipo de riego no es significativa y debe de ser eliminada del modelo.\nPor tanto, obtenemos la tabla ANOVA para los dos factores sin interacción:\n\n# Realizar el ANOVA de dos factores sin interacción\nanova_result &lt;- aov(crecimiento ~ fertilizante + riego, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfertilizante  1   36.0   36.00   3.084 0.0886 .\nriego         2    3.5    1.75   0.150 0.8614  \nResiduals    32  373.5   11.67                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUna vez eliminado el efecto de la interacción del modelo, podemos observar como ninguno de los dos factores para estadísticamente significativo, puesto que los \\(p-valores\\) asociados son mayores que \\(0.05\\). Sin embargo, hemos de actuar con cautela. Veamos el modelo cuando se elimina el factor menos significativo riego:\n\n# Realizar el ANOVA de un factor\nanova_result &lt;- aov(crecimiento ~ fertilizante, data=datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfertilizante  1     36   36.00   3.247 0.0804 .\nResiduals    34    377   11.09                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAl nivel de significancia estadística de \\(0.05\\) podríamos decir que el factor fertilizante no es estadísticamente significativo y que ninguno de los dos factores influye en el crecimiento de las planteas.\nAhora bien, si tuvieras que elegir un método de riego y un fertilizando, ¿cuál recomendarías al cliente? ¿por qué?\n\n\n\n\n\n\n\n\n\nEjemplo. ANOVA de dos factores sin interacción\n\n\n\n\n\nSupongamos que estamos estudiando del tiempo de estudio y del tipo de dieta en el rendimiento académico de un grupo de estudiantes. El tiempo de estudio tiene \\(3\\) niveles (Alto, Medio y Bajo). El tipo de dieta tiene \\(3\\) niveles (Normal, Vegana y Vegetariana).\nEn primer lugar observamos los datos:\n\n# Datos\ndatos &lt;- data.frame(\n  Tiempo_estudio = factor(rep(c(\"1. Alto\", \"2. Medio\", \"3. Bajo\"), each = 9)),\n  Tipo_dieta = factor(rep(c(\"Vegetariana\", \"Normal\", \"Vegana\"), times = 9)),\n  Rendimiento = c(81, 71, 90, 75, 84, 81, 91, 100, 98, # Datos para Nivel1 de Factor1\n                60, 83, 70, 54, 65, 73, 82, 92, 73, # Datos para Nivel2 de Factor1\n                47, 63, 44, 61, 55, 52, 73, 63, 53) # Datos para Nivel3 de Factor1\n)\n\n# Visualizar\nlibrary(\"ggpubr\")\nggline(datos, x = \"Tiempo_estudio\", y = \"Rendimiento\", color = \"Tipo_dieta\",\n       add = c(\"mean_se\", \"dotplot\"),\n       palette = c(\"#00AFBB\", \"#E7B800\",\"#c94545\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nRealizamos el ANOVA de dos factores con interacción.\n\n# Realizar el ANOVA de dos factores con interacción\nanova_result &lt;- aov(Rendimiento ~ Tiempo_estudio * Tipo_dieta, data = datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n                          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nTiempo_estudio             2   3765  1882.3  17.410 6.2e-05 ***\nTipo_dieta                 2    169    84.6   0.782   0.472    \nTiempo_estudio:Tipo_dieta  4    465   116.1   1.074   0.398    \nResiduals                 18   1946   108.1                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa interacción entre los dos factores no es estadísticamente significativa a nivel \\(\\alpha=0.05\\) y por lo tanto eliminamos dicha fuente de variabilidad del modelo.\n\n# Realizar el ANOVA de dos factores sin interacción\nanova_result &lt;- aov(Rendimiento ~ Tiempo_estudio + Tipo_dieta, data = datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTiempo_estudio  2   3765  1882.3  17.178 3.21e-05 ***\nTipo_dieta      2    169    84.6   0.772    0.474    \nResiduals      22   2411   109.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPodemos observar que el tipo de dieta no es un factor significativo en el rendimiento académico. Por tanto, eliminamos dicho factor del modelo.\n\n# Realizar el ANOVA de un factor\nanova_result &lt;- aov(Rendimiento ~ Tiempo_estudio , data = datos)\n\n# Mostrar los resultados del ANOVA\nsummary(anova_result)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTiempo_estudio  2   3765  1882.3   17.51 2.04e-05 ***\nResiduals      24   2580   107.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nEn cambio, el tiempo de estudio sí es un factor determinante en el rendimiento académico. Su \\(p-valor\\) asociado es claramente inferior al nivel de significancia \\(0.05\\).\nPodemos visualizar el resultado:\n\n# Visualizar\nlibrary(\"ggpubr\")\nggline(datos, x = \"Tiempo_estudio\", y = \"Rendimiento\",\n       add = c(\"mean_se\", \"dotplot\"))\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "anova.html#comparaciones-múltiples",
    "href": "anova.html#comparaciones-múltiples",
    "title": "5  Análisis de varianza",
    "section": "5.3 Comparaciones múltiples",
    "text": "5.3 Comparaciones múltiples\nEn el último ejemplo del apartado anterior hemos determinado que un factor con \\(3\\) niveles era estadísticamente significativo. Es decir, podemos rechazar la siguiente hipótesis nula:\n\\[H_O: \\mu_1=\\mu_2\\ldots = \\mu_k\\] (con \\(k\\) igual al número de niveles en el factor). Podemos plantearnos la pregunta siguiente: ¿En qué niveles del factor se encuentran las principales diferencias? Es decir, qué hipotesis (una o varias) de las siguientes son rechazadas: \\[H_O: \\mu_1=\\mu_2\\] \\[H_O: \\mu_1=\\mu_3\\] \\[\\ldots\\] \\[H_O: \\mu_{k-1} = \\mu_k\\]\nEn un ejemplo con \\(k=3\\) niveles en el factor, es posible plantear \\(3\\) contrastes de igualdad de medias, como los estudiados en capítulos anteriores. En un ejemplo con \\(k\\) niveles en el factor, es posible plantear \\(k*(k-1)/2\\) posibles contrastes de igualda de medias. Sin embargo, si realizamos todos estos contrastes, aumenta la probabilidad de cometer errores de tipo I (rechazar incorrectamente la hipótesis nula). Este fenómeno se conoce como el problema de las pruebas múltiples.\nComo hemos venido viendo, cuando realizamos una sola prueba de hipótesis (por ejemplo, una prueba \\(t\\) de Student o un ANOVA), generalmente establecemos un nivel de significancia predeterminado, como \\(\\alpha = 0.05\\). Esto significa que estamos dispuestos a aceptar una probabilidad de error de tipo I del \\(5\\%\\), es decir, hay un \\(5\\%\\) de probabilidad de rechazar incorrectamente la hipótesis nula cuando es verdadera.\nSin embargo, cuando realizamos múltiples pruebas de hipótesis, la probabilidad acumulada de cometer al menos un error de tipo I aumenta significativamente con cada prueba adicional. Por ejemplo, si realizamos \\(10\\) pruebas de hipótesis independientes, cada una con un nivel de significancia de \\(\\alpha = 0.05\\), la probabilidad de cometer al menos un error de tipo I aumenta a más del \\(40\\%\\) (\\(1-(1-0.05)^{10}\\approx 0.401\\).\nExiste una solución, las comparaciones múltiples necesarias para controlar este aumento en el riesgo de error. Existen varios métodos para controlar el problema de las pruebas múltiples, como los ajustes de Bonferroni, Holm-Bonferroni, Holm, Hochberg, Benjamini-Hochberg (FDR), entre otros. Estos métodos controlan la tasa global de error de tipo I para todas las comparaciones realizadas, manteniendo un nivel de significancia general específico.\n\n5.3.1 Método de Bonferroni\nEl método de Bonferroni es una técnica comúnmente utilizada para corregir el problema de las comparaciones múltiples y controlar el riesgo de error de tipo I. Este método es relativamente simple y conservador, lo que lo hace popular en muchas aplicaciones estadísticas.\nLa idea principal detrás del método de Bonferroni es ajustar el nivel de significancia individual para cada prueba de hipótesis realizada. En lugar de utilizar un nivel de significancia estándar (por ejemplo, \\(\\alpha = 0.05\\)), dividimos el nivel de significancia global deseado (generalmente \\(0.05\\)) por el número total de pruebas realizadas (\\(m\\)): \\[\n\\alpha' = \\frac{\\alpha}{m}\n\\] Esta división produce un nivel de significancia más estricto (\\(\\alpha'\\)) para cada prueba individual, lo que ayuda a controlar el riesgo global de error de tipo I.\nUtilizamos el nivel de significancia individual ajustado para cada prueba de hipótesis. Si el \\(p-valor\\) de una prueba es menor que el nivel de significancia ajustado, rechazamos la hipótesis nula de la prueba.\nEl método de Bonferroni es fácil de entender e implementar, y proporciona un control conservador sobre el error de tipo I en comparaciones múltiples. No obstante puede ser un método demasiado conservador en situaciones donde se realizan muchas comparaciones, lo que puede resultar en una pérdida de potencia estadística. Además, no tiene en cuenta la correlación entre las pruebas realizadas.\n\n\n\n\n\n\nEjemplo. Comparaciones múltiples\n\n\n\n\n\nContinuamos con el ejemplo anterior anterior sobre la influencia del tiempo de estudio en el rendimiento académico. Hemos visto que existe una relación entre ambos factores. En otras palabras, hemos rechazado la siguiente hipótesis nula:\n\\[\nH_0: \\mu_{Alto}=\\mu_{Medio}=\\mu_{Bajo}\n\\] Pero, ¿dónde se encuentran las diferencias relevantes?. Calculamos las tres medias muestrales\n\nmean(datos[datos$Tiempo_estudio==\"1. Alto\",]$Rendimiento)\n\n[1] 85.66667\n\nmean(datos[datos$Tiempo_estudio==\"2. Medio\",]$Rendimiento)\n\n[1] 72.44444\n\nmean(datos[datos$Tiempo_estudio==\"3. Bajo\",]$Rendimiento)\n\n[1] 56.77778\n\n\nAplicamos el método de Bonferroni, y obtenemos:\n\npairwise.t.test(datos$Rendimiento, g=datos$Tiempo_estudio,p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  datos$Rendimiento and datos$Tiempo_estudio \n\n         1. Alto 2. Medio\n2. Medio 0.037   -       \n3. Bajo  1.3e-05 0.011   \n\nP value adjustment method: bonferroni \n\n\nDado que todos los \\(p-valores\\) ajustados son menores que \\(0.05\\), podemos rechazar las \\(3\\) hipótesis nulas. Es decir, rechazamos que el rendimiento sea el mismo para los diferentes niveles de tiempos de estudio.\nY en R podemos pintarlo como sigue (aquí empleamos otro método de corrección diferente al de Bonferroni, revísalo)\n\ncomparaciones_mult &lt;- TukeyHSD(anova_result)\nplot(comparaciones_mult)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "conclusiones.html#resumen-de-los-aprendizajes",
    "href": "conclusiones.html#resumen-de-los-aprendizajes",
    "title": "6  Conclusiones",
    "section": "6.1 Resumen de los aprendizajes",
    "text": "6.1 Resumen de los aprendizajes\n\nComprensión de poblaciones y muestras: Esperamos que hayas aprendido a distinguir entre población y muestra, comprendiendo la relevancia de los modelos probabilísticos y la importancia de asegurar un muestreo adecuado para obtener resultados representativos.\nEstimación de parámetros: Juntos hemos abordado diversas técnicas de estimación puntual y por intervalos, permitiendo a los estudiantes realizar inferencias sobre parámetros de la población con un nivel de confianza adecuado.\nContrastes de Hipótesis: A lo largo del curso debes haber desarrollado la habilidad de plantear y resolver contrastes de hipótesis, una herramienta fundamental para la toma de decisiones informadas basadas en datos.\nAnálisis de la Varianza (ANOVA): Se ha presentado, por primera vez en el Grado, el ANOVA como una técnica esencial para comparar múltiples grupos y entender las variaciones entre ellos, destacando su aplicación en diferentes contextos.\nContrastes no paramétricos: Para los casos donde las suposiciones paramétricas no se cumplen, se han introducido y aplicado métodos no paramétricos, ampliando el repertorio de herramientas disponibles para el trabajo futuro del científico e ingeniero de datos."
  },
  {
    "objectID": "conclusiones.html#reflexiones-finales",
    "href": "conclusiones.html#reflexiones-finales",
    "title": "6  Conclusiones",
    "section": "6.2 Reflexiones finales",
    "text": "6.2 Reflexiones finales\nLa inferencia estadística no solo es una herramienta académica, sino una metodología aplicable en múltiples campos profesionales. Desde la investigación científica hasta el análisis de mercados y la toma de decisiones en negocios, la capacidad de interpretar datos y extraer conclusiones válidas es una competencia esencial en la era de la información.\nAdemás, el enfoque en aplicaciones prácticas y el uso de R a lo largo de este libro ha proporcionado a los estudiantes una experiencia directa con herramientas de análisis de datos contemporáneas. Esta práctica no solo refuerza los conceptos teóricos, sino que también prepara a los estudiantes para enfrentar desafíos reales en su futura carrera profesional."
  },
  {
    "objectID": "conclusiones.html#mirando-hacia-adelante",
    "href": "conclusiones.html#mirando-hacia-adelante",
    "title": "6  Conclusiones",
    "section": "6.3 Mirando hacia adelante",
    "text": "6.3 Mirando hacia adelante\nCon el conocimiento y las habilidades adquiridas, los estudiantes están mejor equipados para explorar más a fondo el vasto campo de la ciencia de datos. La estadística y la inferencia estadística seguirán evolucionando con el avance de la tecnología y la disponibilidad de datos. Por lo tanto, es crucial que los futuros profesionales mantengan una mentalidad de aprendizaje continuo y estén abiertos a adoptar nuevas metodologías y herramientas.\nEn conclusión, esperamos que este libro haya proporcionado una comprensión profunda y práctica de la inferencia estadística, y que inspire a los estudiantes a aplicar estos conocimientos con confianza y creatividad en sus proyectos futuros. La capacidad de analizar datos de manera crítica y tomar decisiones basadas en evidencias es una habilidad poderosa y transformadora, que sin duda abrirá numerosas oportunidades en el ámbito profesional.\nOs recordamos que en próximos cursos os encontraréis con las asiganturas de Regresión, Aprendizaje Automático I y Aprendizaje Automático II, donde aplicaréis muchas de las técnicas y herramientas que hemos aprendido juntos.\n¡Buena suerte!"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Fox, John, and Sanford Weisberg. 2018. An r Companion to Applied\nRegression. Sage publications.\n\n\nHao, Jiangang, and Tin Kam Ho. 2019. “Machine Learning Made Easy:\nA Review of Scikit-Learn Package in Python Programming Language.”\nJournal of Educational and Behavioral Statistics 44 (3):\n348–61.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H\nFriedman. 2009. The Elements of Statistical Learning: Data Mining,\nInference, and Prediction. Vol. 2. Springer.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112.\nSpringer.\n\n\nKelleher, John D, Brian Mac Namee, and Aoife D’arcy. 2020.\nFundamentals of Machine Learning for Predictive Data Analytics:\nAlgorithms, Worked Examples, and Case Studies. MIT press.\n\n\nOsmani, Addy. 2012. Learning JavaScript Design Patterns: A\nJavaScript and jQuery Developer’s Guide. \" O’Reilly Media, Inc.\".\n\n\nOualline, Steve. 2003. Practical c++ Programming. \" O’Reilly\nMedia, Inc.\".\n\n\nTukey, John W et al. 1977. Exploratory Data Analysis. Vol. 2.\nReading, MA.\n\n\nWirth, Rüdiger, and Jochen Hipp. 2000. “CRISP-DM: Towards a\nStandard Process Model for Data Mining.” In Proceedings of\nthe 4th International Conference on the Practical Applications of\nKnowledge Discovery and Data Mining, 1:29–39. Manchester.",
    "crumbs": [
      "Bibliografía"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Inferencia Estadística",
    "section": "",
    "text": "Prefacio\nLa estadística es una disciplina fundamental en el mundo moderno, permitiendo a los profesionales extraer conocimientos valiosos a partir de datos. En un entorno donde la cantidad de datos disponibles crece exponencialmente, la capacidad de tomar decisiones informadas basadas en estos datos se vuelve crucial. Este libro, preparado para el Grado en Ciencia e Ingeniería de Datos, pretende proporcionar una comprensión profunda de los principios y métodos que subyacen en esta área esencial.\nLa inferencia estadística es una metodología poderosa y bien fundamentada matemáticamente que permite a los especialistas en datos hacer predicciones, estimaciones y decisiones basadas en información incompleta o incierta. Esta capacidad es vital para cualquier profesional de la ciencia de datos, ya que los conceptos de inferencia estadística forman el núcleo del análisis de datos y la toma de decisiones basada en datos.\nEste libro ha sido diseñado con el propósito de servir como una guía comprensible y completa para estudiantes de Ciencia e Ingeniería de Datos. A lo largo de sus capítulos, los alumnos serán introducidos a los conceptos clave de la inferencia estadística, desde las bases teóricas hasta las aplicaciones prácticas. Se cubrirán temas como la estimación de parámetros, pruebas de hipótesis, análisis de la varianza y más, siempre con un enfoque en la aplicación práctica y la interpretación de los resultados en contextos reales.\nLa estructura del libro está cuidadosamente planeada para facilitar el aprendizaje progresivo. Cada capítulo incluye ejemplos prácticos, ejercicios y aplicaciones en el mundo real, que no solo ilustran los conceptos teóricos, sino que también permiten a los estudiantes practicar y consolidar sus conocimientos. Además, se ha hecho un esfuerzo consciente para conectar los temas tratados con las herramientas y técnicas que los estudiantes encontrarán en sus futuras asignaturas y en su vida profesional.\nEl objetivo final de este libro es prepararos para enfrentar los desafíos del análisis y la ciencia de datos con confianza y competencia. La inferencia estadística es una habilidad indispensable para cualquier especialista en datos, y dominarla os abrirá innumerables puertas en el ámbito profesional.\nEsperamos que este libro sea una fuente valiosa de conocimiento y que os sirva de inspiración para profundizar en el fascinante campo de la inferencia estadística.\n¡Comenzamos!\n\n\n\n\n\n\nResultados de aprendizaje.\n\n\n\n\n\n\nEntender el concepto de población estadística en relación a los modelos probabilísticos.\nEntender el concepto de muestreo y distinguir si los datos bajo análisis proceden de un muestreo aleatorio simple.\nRealizar inferencias sobre parámetros de interés de la población, tanto puntualmente como por intervalos.\nPlantear y resolver contrastes de hipótesis para la toma de decisiones sobre características de la población bajo estudio.\nValorar si el modelo paramétrico asumido se ajusta adecuadamente a los datos.\n\n\n\n\n\n\n\n\n\n\nGrado en Ciencia e Ingeniería de Datos\n\n\n\nEste libro presenta el material de la asignatura de Inferencia Estadística del grado en Ciencia e Ingeniería de Datos de la Universidad Rey Juan Carlos. Os recordamos que en próximos cursos os encontraréis con las asiganturas de Regresión, Aprendizaje Automático I y Aprendizaje Automático II, donde aplicaréis muchas de las técnicas y herramientas que vamos a estudiar aquí.\n\n\n\n\n\n\n\n\nConocimientos previos\n\n\n\nEs conveniente haber superado con éxito las asignaturas de Cálculo, Herramientas Matemáticas para la Ciencia de Datos I y Probabilidad y Simulación, del grado en Ciencia e Ingeniería de Datos.\n\n\n\n\n\n\n\n\nSobre los autores\n\n\n\nCarmen Lancho Martín es graduada en Matemáticas y Estadística por la Universidad Complutense de Madrid (UCM), máster en Tratamiento Estadístico y Computacional de la Información por la UCM y la Universidad Politécnica de Madrid (UPM), doctora en Tecnologías de la Información y las Comunicaciones por la Universidad Rey Juan Carlos (URJC) y profesora del departamento de Informática y Estadística de la URJC. Miembro del grupo de investigación de alto rendimiento en Fundamentos y Aplicaciones de la Ciencia de Datos, DSLAB, de la URJC. Pertenece al grupo de innovación docente, DSLAB-TI.\nVíctor Aceña Gil es graduado en Matemáticas por la UNED, máster en Tratamiento Estadístico y Computacional de la Información por la UCM y la UPM, doctor en Tecnologías de la Información y las Comunicaciones por la URJC y profesor del departamento de Informática y Estadística de la URJC. Miembro del grupo de investigación de alto rendimiento en Fundamentos y Aplicaciones de la Ciencia de Datos, DSLAB, de la URJC. Pertenece al grupo de innovación docente, DSLAB-TI.\nIsaac Martín de Diego es diplomado en Estadística por la Universidad de Valladolid (UVA), licenciado en Ciencias y Técnicas Estadísticas por la Universidad Carlos III de Madrid (UC3M), doctor en Ingeniería Matemática por la UC3M, catedrático de Ciencias de la Computación e Inteligencia Artificial del departamento de Informática y Estadística de la URJC. Es fundador y coordinador del DSLAB y del DSLAB-TI.\n\n\nEsta obra está bajo una licencia de Creative Commons Atribuciónn-CompartirIgual 4.0 Internacional."
  },
  {
    "objectID": "intro.html#resúmenes-gráficos-y-numéricos-útiles-en-la-inferencia-estadística",
    "href": "intro.html#resúmenes-gráficos-y-numéricos-útiles-en-la-inferencia-estadística",
    "title": "1  Introducción",
    "section": "1.9 Resúmenes gráficos y numéricos útiles en la inferencia estadística",
    "text": "1.9 Resúmenes gráficos y numéricos útiles en la inferencia estadística\n\n1.9.1 Métodos numéricos\nTal y como hemos indicado anteriormente, los métodos numéricos de inferencia estadística son técnicas y procedimientos utilizados para analizar datos y hacer inferencias sobre una población a partir de una muestra. Estos métodos se apoyan en herramientas matemáticas y computacionales para estimar parámetros, evaluar hipótesis y tomar decisiones informadas. A continuación, se describen los principales métodos numéricos en la inferencia estadística. No tienes que aprenderlos ahora puesto que vamos a trabajar con estos métodos a lo largo de todo el curso. Los veremos con mayor detalle en los próximos capítulos.\n\n\n\n\n\n\nEstimación puntual\n\n\n\n\n\nLa estimación puntual implica el uso de un solo valor estadístico de la muestra para estimar un parámetro de la población.\n\nMedia Muestral (\\(\\bar{x}\\)): Utilizada para estimar la media poblacional (\\(\\mu\\)).\nProporción Muestral (\\(\\hat{p}\\)): Utilizada para estimar la proporción poblacional (\\(p\\)).\nVarianza Muestral (\\(s^2\\)): Utilizada para estimar la varianza poblacional (\\(\\sigma^2\\)).\n\n\n\n\n\n\n\n\n\n\nEstimación por intervalo\n\n\n\n\n\nLa estimación por intervalo proporciona un rango de valores dentro del cual se espera que se encuentre el parámetro poblacional con un cierto nivel de confianza.\n\nIntervalo de confianza para la media:\n\nPara muestras grandes (\\(n \\ge 30\\)): \\(\\bar{x} \\pm Z_{\\alpha/2} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right )\\)\n\nPara muestras pequeñas (\\(n &lt; 30\\)) y cuando la distribución es normal: \\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\left(\\frac{s}{\\sqrt{n}}\\right)\\)\n\nIntervalo de confianza para la proporción: \\(\\hat{p} \\pm Z_{\\alpha/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\)\nIntervalo de confianza para la varianza: \\(\\left( \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2, n-1}}, \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2, n-1}} \\right)\\)\n\n\n\n\n\n\n\n\n\n\nContraste de hipótesis\n\n\n\n\n\nEl contraste de hipótesis es un procedimiento para tomar decisiones sobre los parámetros poblacionales basándose en la evidencia proporcionada por los datos muestrales.\n\nFormulación de hipótesis:\n\nHipótesis nula (\\(H_0\\)): Es la afirmación que se desea probar o refutar.\nHipótesis alternativa (\\(H_1\\)): Es la afirmación que se acepta si se rechaza (\\(H_0\\)).\n\nPruebas para la media:\n\nPrueba Z: Utilizada para muestras grandes (\\(n \\ge 30\\)) o cuando se conoce la desviación estándar poblacional (\\(\\sigma\\)).\nPrueba t: Utilizada para muestras pequeñas (\\(n &lt; 30\\)) y cuando no se conoce (\\(\\sigma\\)).\n\nPruebas para la proporción:\n\nPrueba Z para proporciones: Utilizada para evaluar hipótesis sobre una proporción poblacional.\n\nPruebas para la Varianza:\n\nPrueba Chi-cuadrado: Utilizada para evaluar hipótesis sobre la varianza poblacional.\n\n\n\n\n\n\n\n\n\n\n\nMétodos de resampling\n\n\n\n\n\nLos métodos de resampling, como el bootstrap y el jackknife, son técnicas computacionales utilizadas para estimar la precisión de los estadísticos de la muestra.\n\nBootstrap: Consiste en tomar múltiples muestras con reemplazo de los datos originales y calcular el estadístico de interés para cada muestra. Esto permite construir una distribución empírica del estadístico y estimar intervalos de confianza.\nJackknife: Involucra excluir sistemáticamente cada observación de la muestra y recalcular el estadístico de interés. Esto proporciona una manera de estimar el sesgo y la varianza del estimador.\n\n\n\n\n\n\n\n\n\n\nMétodos bayesianos\n\n\n\n\n\nLa inferencia bayesiana utiliza la probabilidad subjetiva para actualizar la creencia sobre los parámetros poblacionales basándose en la evidencia muestral.\n\nTeorema de Bayes: \\(P( \\theta |x) = \\frac{P(x | \\theta)P( \\theta)}{P(x)}\\), donde \\(P(\\theta|x)\\) es la distribución a posteriori, \\(P(x|\\theta)\\) es la verosimilitud, \\(P(\\theta)\\) es la distribución a priori y \\(P(x)\\) es la probabilidad marginal de los datos.\nSimulación Monte Carlo Markov Chain (MCMC): Es un método numérico para aproximar distribuciones posteriores complejas.\n\n\n\n\n\n\n\n\n\n\nPruebas no paramétricas\n\n\n\n\n\nCuando no se cumplen los supuestos de normalidad, se utilizan pruebas no paramétricas que no requieren asumir una distribución específica.\n\nPrueba de Wilcoxon: Para comparar medianas entre dos muestras emparejadas.\nPrueba de Mann-Whitney: Para comparar medianas entre dos muestras independientes.\nPrueba de Kruskal-Wallis: Extensión de la prueba de Mann-Whitney para más de dos grupos.\nPrueba de Chi-cuadrado: Para pruebas de independencia y homogeneidad en tablas de contingencia.\n\n\n\n\n\n\n1.9.2 Métodos gráficos\nA lo largo del curso usaremos numerosos métodos gráficos para explorar los datos dentro del contexto de la inferencia estadística. Algunas de los métodos básicos son:\n\n\n\n\n\n\nHistogramas\n\n\n\n\n\nGráficos de barras que muestran la distribución de frecuencias de datos cuantitativos.\n\n\n\n\n\n\n\n\n\nGráficos de caja (boxplots)\n\n\n\n\n\nMuestran la mediana, los cuartiles y los posibles valores atípicos.\n\n\n\n\n\n\n\n\n\nGráficos de dispersión (scatterplots)\n\n\n\n\n\nUtilizados para observar la relación entre dos variables cuantitativas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "para.html",
    "href": "para.html",
    "title": "3  Estimación y contraste paramétrico",
    "section": "",
    "text": "3.1 Definición de estadístico\nUn estadístico es una medida calculada a partir de una muestra de datos que se utiliza para describir o resumir características de la muestra. En otras palabras, un estadístico es un valor numérico que resume o describe algún aspecto de los datos recolectados. Los estadísticos se utilizan ampliamente en análisis de datos, inferencia estadística y para hacer estimaciones sobre poblaciones más grandes basadas en la información obtenida de una muestra.\nSi te fijas bien, verás que en el Capítulo 2 ya hemos estado trabajando con estadísticos. Los estadísticos juegan un papel crucial en la inferencia estadística, donde se utilizan para hacer estimaciones o probar hipótesis sobre una población a partir de la información contenida en una muestra.\nEjemplos comunes de estadísticos incluyen:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimación y contraste paramétrico</span>"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "5  Análisis de varianza",
    "section": "",
    "text": "5.1 Modelo con un factor\nEl modelo de ANOVA de un factor se utiliza cuando se estudia el efecto de un solo factor (variable independiente) en una variable dependiente continua. Este modelo permite comparar las medias de varios grupos para determinar si existen diferencias significativas entre ellos.\nTenemos una variable aleatoria \\(Y\\) que toma valores reales y una variable cualitativa o factor \\(X\\) con \\(k\\) niveles \\(1,2,\\ldots,i,\\ldots,k\\). La variable \\(Y\\) toma valores \\(Y_{ij}, j=1,\\ldots,n_i\\) en el nivel \\(i\\) del factor \\(X\\), siendo \\(n_i\\) el número de observaciones en el nivel \\(i\\) del factor \\(X\\).\nTenemos los siguientes supuestos:\nEscribimos el modelo como sigue: \\[\n  Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\] Donde:\nLa Suma de las diferencias al cuadrado de cada dato respecto a la media general se calcula como sigue: \\[\n    \\text{SST} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_{..})^2        \n\\] donde \\(\\bar{Y}_{..}\\) es la media general de todas las observaciones.\nTeniendo en cuenta que: \\[\nY_{ij} - \\bar{Y}_{..}= Y_i  + \\epsilon_{ij} - \\bar{Y}_{..}\n\\] Podemos descomponer la suma de cuadrados, como sigue:\n\\[\n    \\text{SST} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_{..})^2=  \\sum_{i=1}^{k} n_i (\\bar{Y}_i - \\bar{Y}_{..})^2+\\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2      \n\\]\nDonde: La varianza entre grupos se calcula como la suma de las diferencias al cuadrado de las medias de los grupos respecto a la media general, ponderada por el tamaño de los grupos: \\[\n\\text{SSB} = \\sum_{i=1}^{k} n_i (\\bar{Y}_i - \\bar{Y}_{..})^2\n\\] donde \\(\\bar{Y}_i\\) es la media del grupo \\(i\\).\nAdemás, la varianza dentro de los grupos, se calcula como la suma de las diferencias al cuadrado de cada dato respecto a la media de su grupo se obtiene como: \\[\n\\text{SSW} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2\n\\] Esto es, se descompone la variabilidad total de los datos en dos componentes, SSB que refleja la diferencia de cada grupo respecto a la media global y SSW que refleja la variabilidad intrínseca dentro de cada grupo: \\[\n\\text{SST} = \\text{SSB} + \\text{SSW}\n\\]\nCálculo del Estadístico F: \\[\nF = \\frac{\\text{Varianza Entre Grupos (MSB)}}{\\text{Varianza Dentro de los Grupos (MSW)}}\n\\] Donde:\nEsto es: \\[\nF=\\frac{SSB/df_B}{SSW/df_W}\n\\] donde:\nUna vez se dispone de toda esta información, es común representarla en forma de tabla, en la llamada Tabla ANOVA:\nEl estadístico de prueba \\(F \\sim F_{df_B,df_W}\\) bajo la hipótesis nula de igualdad de medias.\nEl \\(p-valor\\) se obtiene a partir de la distribución \\(F\\), considerando los grados de libertad de los numeradores y denominadores. Esto es: \\[\np-valor=P(F_{df_b,df_W}&gt;F_{muestral})\n\\]\nComo en otros contrastes, si el \\(p-valor\\) p es menor que el nivel de significancia (\\(\\alpha\\)), se rechaza la hipótesis nula, concluyendo que al menos una de las medias de los grupos es diferente.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Análisis de varianza</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 Ciencia de datos\nLa última revolución asociada a la IA ha estado enmarcada por el crecimiento en el uso del Aprenzaje Automático dentro del contexto de la ciencia de datos (Kelleher, Mac Namee, y D’arcy 2020).\nPero, ¿qué áreas, métodos y técnicas están implicados en la ciencia de datos?. En primer lugar, presentemos los aspectos teórico y prácticos que sustentan un proyecto real de ciencia de datos. Para ellos recurrimos a la Figura Figura 1.1 (a) que representa el clásico diagrama de la ciencia de datos, como una disciplina en la intersección de tres aspectos fundamentales. Para saber más sobre estos aspectos, desplegad los paneles siguientes:\nLa Figura Figura 1.1 (b) basada en CRISP-DM: “Cross Industry Standard Process for Data Mining” (Wirth y Hipp 2000) presenta el ciclo de vida que todo proyecto de ciencia de datos debería seguir. El inicio del proyecto viene dado por la definición de los objetivos de la organización. A continuación, se recogen y gestionan los datos. Como siguiente paso, se desarrollan y evalúan algoritmos matemáticos sobre los datos. Los resultados de estos modelos se presentan a los expertos en el dominio de aplicación para su posterior integración dentro de la organización. Nótese que el proyecto puede tener varias iteraciones, volviendo a alguna de las etapas anteriores siempre que una etapa posterior así lo requiera. Para saber más detalles sobre estas etapas investigad los paneles siguientes:\nEn este libro vamos a tratar en profundidad la Inferencia Estadística. Pero, ¿qué relación tiene con la Estadística. Vamos a verlo.\nLa estadística es la ciencia que se encarga de recolectar, organizar, analizar e interpretar datos para tomar decisiones informadas. Su objetivo principal es comprender y describir la variabilidad inherente en los datos y utilizar esta comprensión para hacer predicciones y tomar decisiones bajo condiciones de incertidumbre. La estadística se divide en dos ramas principales:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "2  Análisis Exploratorio de Datos",
    "section": "",
    "text": "2.1 Preguntas\nTu objetivo principal durante el EDA es adquirir una comprensión profunda de los datos que se están analizando. La forma más sencilla de hacerlo es utilizar preguntas como herramientas para guiar la investigación. Cuando planteas una pregunta, ésta centra tu atención en una parte específica del conjunto de datos y te ayuda a decidir qué gráficos, modelos o transformaciones realizar.\nEDA es un proceso creativo y como tal, la clave para llevarlo a cabo consiste en el planteamiento de preguntas de calidad. ¿Qué preguntas son las correctas? La respuesta es que depende del conjunto de datos con el que se trabaje.\nAl inicio del análisis, puede resultar todo un desafío formular preguntas reveladoras, ya que aún no se conoce completamente la información contenida en el conjunto de datos. Si estás involucrado en un proceso de inferencia estadística, en muchas ocasiones, esas preguntas vendrán formuladas por un experto del dominio o por un superior con conocimientos estadísticos. Cada nueva pregunta que se plantee te llevará a explorar un nuevo aspecto de tus datos, aumentando así las posibilidades de hacer descubrimientos importantes.\nAlgunas de las preguntas que, generalmente, deberían de abordarse durante el EDA son:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Análisis Exploratorio de Datos</span>"
    ]
  }
]